INFO 03-04 15:18:22 api_server.py:528] vLLM API server version 0.5.2.dev1210+g6f078ea3c
INFO 03-04 15:18:22 api_server.py:529] args: Namespace(host=None, port=11004, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/opt/opt-1.3b/', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.45, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=96, max_logprobs=20, disable_log_stats=True, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', override_generation_config=[], disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False)
INFO 03-04 15:18:22 api_server.py:166] Multiprocessing frontend to use ipc:///scratch/tmp/16292126/66d97c2c-b95e-4053-8647-6587d0d86625 for IPC Path.
INFO 03-04 15:18:22 api_server.py:179] Started engine process with PID 41042
INFO 03-04 15:18:22 config.py:1673] Downcasting torch.float32 to torch.float16.
INFO 03-04 15:18:25 config.py:1673] Downcasting torch.float32 to torch.float16.
WARNING 03-04 15:18:26 arg_utils.py:1029] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
WARNING 03-04 15:18:28 arg_utils.py:1029] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 03-04 15:18:28 llm_engine.py:237] Initializing an LLM engine (v0.5.2.dev1210+g6f078ea3c) with config: model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/opt/opt-1.3b/', speculative_config=None, tokenizer='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/opt/opt-1.3b/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/opt/opt-1.3b/, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)
INFO 03-04 15:18:29 model_runner.py:1056] Starting to load model /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/opt/opt-1.3b/...
INFO 03-04 15:18:30 model_runner.py:1067] Loading model weights took 2.4509 GB
INFO 03-04 15:18:31 worker.py:262] Memory profiling results: total_gpu_memory=63.43GiB initial_memory_usage=6.24GiB peak_torch_memory=2.72GiB non_torch_memory=0.51GiB kv_cache_size=25.31GiB gpu_memory_utilization=0.45
INFO 03-04 15:18:31 gpu_executor.py:122] # GPU blocks: 8640, # CPU blocks: 1365
INFO 03-04 15:18:31 gpu_executor.py:126] Maximum concurrency for 2048 tokens per request: 67.50x
INFO 03-04 15:18:33 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-04 15:18:33 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-04 15:18:37 model_runner.py:1523] Graph capturing finished in 4 secs.
INFO 03-04 15:18:37 api_server.py:232] vLLM to use /scratch/tmp/16292126/tmpjqsiq34z as PROMETHEUS_MULTIPROC_DIR
WARNING 03-04 15:18:37 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.
INFO 03-04 15:18:37 launcher.py:19] Available routes are:
INFO 03-04 15:18:37 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD
INFO 03-04 15:18:37 launcher.py:27] Route: /docs, Methods: GET, HEAD
INFO 03-04 15:18:37 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 03-04 15:18:37 launcher.py:27] Route: /redoc, Methods: GET, HEAD
INFO 03-04 15:18:37 launcher.py:27] Route: /health, Methods: GET
INFO 03-04 15:18:37 launcher.py:27] Route: /tokenize, Methods: POST
INFO 03-04 15:18:37 launcher.py:27] Route: /detokenize, Methods: POST
INFO 03-04 15:18:37 launcher.py:27] Route: /v1/models, Methods: GET
INFO 03-04 15:18:37 launcher.py:27] Route: /version, Methods: GET
INFO 03-04 15:18:37 launcher.py:27] Route: /v1/chat/completions, Methods: POST
INFO 03-04 15:18:37 launcher.py:27] Route: /v1/completions, Methods: POST
INFO 03-04 15:18:37 launcher.py:27] Route: /v1/embeddings, Methods: POST
INFO:     127.0.0.1:40720 - "GET /metrics HTTP/1.1" 200 OK
INFO:     127.0.0.1:38740 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38764 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38768 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38780 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38794 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38804 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38820 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38834 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38850 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38862 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38870 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38876 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38890 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38892 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38908 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38922 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38930 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38946 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38948 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38980 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38986 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38998 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:39002 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:39010 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 03-04 15:18:58 launcher.py:57] Shutting down FastAPI HTTP server.
