Run profile with:
  engine_args = {'model': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'served_model_name': None, 'tokenizer': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'skip_tokenizer_init': False, 'tokenizer_mode': 'auto', 'trust_remote_code': False, 'download_dir': None, 'load_format': 'auto', 'config_format': <ConfigFormat.AUTO: 'auto'>, 'dtype': 'auto', 'kv_cache_dtype': 'auto', 'quantization_param_path': None, 'seed': 0, 'max_model_len': None, 'worker_use_ray': False, 'distributed_executor_backend': None, 'pipeline_parallel_size': 1, 'tensor_parallel_size': 1, 'max_parallel_loading_workers': None, 'block_size': 16, 'enable_prefix_caching': False, 'disable_sliding_window': False, 'use_v2_block_manager': False, 'swap_space': 4, 'cpu_offload_gb': 0, 'gpu_memory_utilization': 0.9, 'max_num_batched_tokens': 256000, 'max_num_seqs': 1024, 'max_logprobs': 20, 'disable_log_stats': False, 'revision': None, 'code_revision': None, 'rope_scaling': None, 'rope_theta': None, 'tokenizer_revision': None, 'quantization': None, 'enforce_eager': False, 'max_context_len_to_capture': None, 'max_seq_len_to_capture': 8192, 'disable_custom_all_reduce': False, 'tokenizer_pool_size': 0, 'tokenizer_pool_type': 'ray', 'tokenizer_pool_extra_config': None, 'limit_mm_per_prompt': None, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'enable_prompt_adapter': False, 'max_prompt_adapters': 1, 'max_prompt_adapter_token': 0, 'fully_sharded_loras': False, 'lora_extra_vocab_size': 256, 'long_lora_scaling_factors': None, 'lora_dtype': 'auto', 'max_cpu_loras': None, 'device': 'auto', 'num_scheduler_steps': 1, 'multi_step_stream_outputs': True, 'ray_workers_use_nsight': False, 'num_gpu_blocks_override': None, 'num_lookahead_slots': 0, 'model_loader_extra_config': None, 'ignore_patterns': [], 'preemption_mode': None, 'scheduler_delay_factor': 0.0, 'enable_chunked_prefill': None, 'guided_decoding_backend': 'outlines', 'speculative_model': None, 'speculative_model_quantization': None, 'speculative_draft_tensor_parallel_size': None, 'num_speculative_tokens': None, 'speculative_disable_mqa_scorer': False, 'speculative_max_model_len': None, 'speculative_disable_by_batch_size': None, 'ngram_prompt_lookup_max': None, 'ngram_prompt_lookup_min': None, 'spec_decoding_acceptance_method': 'rejection_sampler', 'typical_acceptance_sampler_posterior_threshold': None, 'typical_acceptance_sampler_posterior_alpha': None, 'qlora_adapter_name_or_path': None, 'disable_logprobs_during_spec_decoding': None, 'otlp_traces_endpoint': None, 'collect_detailed_traces': None, 'disable_async_output_proc': False, 'override_neuron_config': None, 'mm_processor_kwargs': None, 'scheduling_policy': 'fcfs'}
  prompt_len = 250
  output_len = 230
  batch_size = 64
  result_dir = benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_64
  save_chrome_traces_folder = chrome_traces
WARNING 11-25 13:50:35 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 11-25 13:50:35 llm_engine.py:237] Initializing an LLM engine (v0.5.2.dev1210+g6f078ea3c) with config: model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', speculative_config=None, tokenizer='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 11-25 13:50:36 model_runner.py:1056] Starting to load model /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b...
INFO 11-25 13:50:48 model_runner.py:1067] Loading model weights took 12.5523 GB
INFO 11-25 13:50:55 worker.py:262] Memory profiling results: total_gpu_memory=63.43GiB initial_memory_usage=13.15GiB peak_torch_memory=36.15GiB non_torch_memory=0.10GiB kv_cache_size=20.84GiB gpu_memory_utilization=0.90
INFO 11-25 13:50:56 gpu_executor.py:122] # GPU blocks: 2667, # CPU blocks: 512
INFO 11-25 13:50:56 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 10.42x
INFO 11-25 13:50:57 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-25 13:50:57 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-25 13:51:27 model_runner.py:1523] Graph capturing finished in 30 secs.
llm.llm_engine.model_config.max_model_len:  4096
Warm up run ...
Profile run ...
INFO 11-25 13:51:32 metrics.py:349] Avg prompt throughput: 6236.7 tokens/s, Avg generation throughput: 99.8 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.8%, CPU KV cache usage: 0.0%.
INFO 11-25 13:51:37 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 138.0 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 40.8%, CPU KV cache usage: 0.0%.
INFO 11-25 13:51:42 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 140.6 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.2%, CPU KV cache usage: 0.0%.
INFO 11-25 13:51:48 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 141.0 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.6%, CPU KV cache usage: 0.0%.
INFO 11-25 13:51:53 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 146.3 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.6%, CPU KV cache usage: 0.0%.
INFO 11-25 13:51:58 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 135.3 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 48.0%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:04 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 146.5 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 50.4%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:09 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 134.8 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 52.8%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:14 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 144.6 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 52.8%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:19 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 131.9 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 55.2%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:25 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 142.8 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 57.6%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:30 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 57.6%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:35 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 146.1 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 60.0%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:41 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 145.2 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 62.4%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:46 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 144.5 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 64.8%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:51 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 126.3 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 64.8%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:57 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 143.6 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 67.2%, CPU KV cache usage: 0.0%.
INFO 11-25 13:53:02 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 141.8 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 69.6%, CPU KV cache usage: 0.0%.
INFO 11-25 13:53:07 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 143.0 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 72.0%, CPU KV cache usage: 0.0%.
INFO 11-25 13:53:13 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.2 tokens/s, Running: 64 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 72.0%, CPU KV cache usage: 0.0%.
================================================================================
= Prefill Model Table (prompt_len=250, batch_size=64)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
LlamaForCausalLM                                             |     22903.92 |    396448.26 |        99.79 |                                                             
|- LlamaModel                                                |     22877.28 |    396448.26 |        99.79 |                                                             
|-- VocabParallelEmbedding(weight=float16[32000, 4096])      |        88.22 |       469.28 |         0.12 |                                                             
|--- void at::native::(anonymous namespace)::indexSelectL... |         0.00 |       469.28 |         0.12 | index_select(float16[32000, 4096], 0, int64[16000]) <- em...
|-- LlamaDecoderLayer                                        |      2945.08 |     12139.31 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        86.29 |       262.40 |         0.07 |                                                             
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c... |         0.00 |       262.40 |         0.07 | _C::rms_norm(float16[16000, 4096], float16[16000, 4096], ...
|--- LlamaAttention                                          |      2630.40 |      4562.91 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |       119.91 |      2232.50 |         0.56 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2231.70 |         0.56 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        75.00 |       447.81 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       447.81 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |      2263.78 |       822.17 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       384.35 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       437.82 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        95.32 |      1060.44 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1059.48 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        39.24 |       350.65 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       350.65 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       154.60 |      6963.35 |         1.75 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        46.56 |      4182.57 |         1.05 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4181.77 |         1.05 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        43.74 |       799.67 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       799.67 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        43.83 |      1981.11 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1980.15 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       731.10 |     12391.28 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.04 |       344.25 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       344.25 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       553.12 |      4593.25 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.70 |      2247.22 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2246.42 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        41.62 |       448.80 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       448.80 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       386.36 |       823.07 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       385.40 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       437.66 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        48.07 |      1074.17 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1073.40 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        20.67 |       349.73 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       349.73 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       110.55 |      7104.05 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.96 |      4226.02 |         1.06 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4225.00 |         1.06 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        26.95 |       798.36 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       798.36 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.94 |      2079.67 |         0.52 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2078.87 |         0.52 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       615.51 |     12128.05 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.75 |       341.79 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.79 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       456.91 |      4571.14 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.48 |      2217.97 |         0.56 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2217.20 |         0.56 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        32.94 |       447.42 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       447.42 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       310.24 |       823.13 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       384.80 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       438.33 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.62 |      1082.62 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1081.72 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        19.61 |       351.23 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.23 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       104.36 |      6863.89 |         1.73 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.88 |      4071.40 |         1.02 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4070.44 |         1.02 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.43 |       798.07 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       798.07 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.38 |      1994.42 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1993.62 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       633.75 |     12348.50 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.05 |       341.21 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.21 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       461.37 |      4569.44 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.66 |      2241.33 |         0.56 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2240.56 |         0.56 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        31.39 |       446.94 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       446.94 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       315.83 |       823.58 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       384.64 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       438.94 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        47.78 |      1057.59 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1056.60 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        21.26 |       352.13 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       352.13 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       118.74 |      7085.72 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.35 |      4200.61 |         1.06 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4199.81 |         1.06 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        27.62 |       801.12 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       801.12 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        35.39 |      2083.99 |         0.52 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2082.96 |         0.52 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       620.68 |     12267.38 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.88 |       343.55 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       343.55 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       456.54 |      4599.94 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.73 |      2251.35 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2250.55 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        32.69 |       450.33 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       450.33 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       310.67 |       821.91 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.40 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       435.52 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.43 |      1076.35 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.09 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1075.26 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        18.76 |       350.75 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       350.75 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       112.13 |      6973.14 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.24 |      4207.56 |         1.06 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4206.79 |         1.06 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.47 |       800.28 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       800.28 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.42 |      1965.30 |         0.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1964.31 |         0.49 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       609.35 |     12302.55 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.63 |       340.86 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       340.86 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       454.35 |      4615.81 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.27 |      2274.87 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2273.84 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        32.03 |       450.40 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       450.40 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       298.83 |       825.69 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       385.73 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       439.96 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        52.81 |      1064.86 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1064.06 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        19.20 |       352.70 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       352.70 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       104.01 |      6993.17 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.64 |      4209.00 |         1.06 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4208.04 |         1.06 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.74 |       802.81 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       802.81 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.32 |      1981.36 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1980.56 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       637.10 |     12216.66 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        20.66 |       342.53 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       342.53 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       471.49 |      4597.09 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.86 |      2241.36 |         0.56 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2240.37 |         0.56 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        32.61 |       451.81 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       451.81 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       322.08 |       825.08 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       387.04 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       438.04 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        46.45 |      1078.84 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1077.88 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        21.02 |       351.36 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.36 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       108.03 |      6925.68 |         1.74 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.84 |      4210.21 |         1.06 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4209.44 |         1.06 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        26.39 |       803.23 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.23 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.24 |      1912.24 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1911.28 |         0.48 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       614.73 |     12323.35 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.19 |       340.57 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       340.57 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       458.47 |      4585.44 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.62 |      2243.19 |         0.56 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2242.39 |         0.56 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        33.00 |       452.00 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.00 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       317.26 |       824.25 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       387.01 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       437.25 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.09 |      1066.01 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1065.24 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        19.23 |       350.40 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       350.40 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       103.16 |      7046.93 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.97 |      4293.48 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4292.48 |         1.08 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.13 |       802.04 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       802.04 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.70 |      1951.41 |         0.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1950.42 |         0.49 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       589.87 |     12272.85 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.86 |       341.50 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.50 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       441.19 |      4575.43 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.24 |      2238.80 |         0.56 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2238.00 |         0.56 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        31.66 |       452.03 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.03 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       299.63 |       826.07 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.46 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       439.61 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.15 |      1058.52 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1057.63 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        18.10 |       352.12 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       352.12 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |        99.71 |      7003.80 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        32.00 |      4209.45 |         1.06 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4208.52 |         1.06 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        23.98 |       802.30 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       802.30 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.36 |      1992.05 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1991.28 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       643.20 |     12391.51 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.30 |       341.69 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.69 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       481.62 |      4595.94 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        31.24 |      2260.72 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2259.73 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        30.02 |       451.93 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       451.93 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       334.19 |       823.61 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.30 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       437.31 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        53.39 |      1059.67 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1058.68 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        21.94 |       350.88 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       350.88 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       107.69 |      7103.00 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.94 |      4265.25 |         1.07 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4264.45 |         1.07 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        26.48 |       803.80 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.80 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.67 |      2033.94 |         0.51 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2033.14 |         0.51 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       613.94 |     12378.10 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.05 |       341.15 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.15 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       453.88 |      4655.27 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.48 |      2294.77 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2293.84 |         0.58 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        34.75 |       453.31 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       453.31 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       308.00 |       824.99 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       387.45 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       437.53 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.88 |      1082.20 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1081.14 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        18.53 |       351.87 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.87 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       108.44 |      7029.81 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.87 |      4314.82 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4314.05 |         1.09 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.50 |       803.71 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.71 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.48 |      1911.28 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.44 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1909.84 |         0.48 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       626.79 |     12521.91 |         3.15 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.67 |       341.63 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.63 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       473.47 |      4662.98 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.82 |      2322.70 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2321.94 |         0.58 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        32.47 |       453.21 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       453.21 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       309.56 |       825.53 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.46 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       439.07 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.61 |      1061.53 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1060.76 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        18.73 |       350.30 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       350.30 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       102.70 |      7167.00 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.18 |      4333.03 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4332.07 |         1.09 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        24.85 |       803.42 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.42 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.74 |      2030.55 |         0.51 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2029.75 |         0.51 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       630.17 |     12441.01 |         3.13 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.01 |       342.85 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       342.85 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       463.33 |      4642.91 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.40 |      2297.65 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2296.63 |         0.58 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        32.12 |       452.00 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.00 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       319.85 |       823.87 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       387.45 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       436.41 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.57 |      1069.40 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1068.47 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        18.73 |       350.65 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       350.65 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       116.00 |      7104.59 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.64 |      4330.40 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4329.41 |         1.09 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        28.34 |       802.17 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       802.17 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.90 |      1972.02 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1971.22 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       645.33 |     12310.87 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.00 |       341.76 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.76 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       447.05 |      4619.75 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.23 |      2281.81 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2281.04 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        30.77 |       452.41 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.41 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       309.48 |       822.81 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.56 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       436.25 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.36 |      1062.71 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1061.79 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        18.18 |       349.37 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       349.37 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       148.56 |      6999.99 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        75.87 |      4193.73 |         1.06 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4192.93 |         1.06 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.30 |       803.32 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.32 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        35.50 |      2002.93 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2001.97 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       609.77 |     12304.88 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.94 |       342.78 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       342.78 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       456.31 |      4641.67 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        31.01 |      2281.39 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2280.59 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        30.31 |       452.93 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.93 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       322.27 |       823.10 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.14 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       436.96 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.34 |      1084.25 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1083.19 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        19.92 |       350.62 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       350.62 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       102.02 |      6969.81 |         1.75 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.05 |      4183.49 |         1.05 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4182.73 |         1.05 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        24.85 |       803.77 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.77 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.50 |      1982.55 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1981.78 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       611.37 |     12494.86 |         3.15 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        14.73 |       342.78 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       342.78 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       450.26 |      4637.89 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.02 |      2297.71 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2296.72 |         0.58 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        30.22 |       452.12 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.12 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       309.81 |       822.75 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.33 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       436.41 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.13 |      1065.31 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1064.51 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        24.35 |       351.29 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.29 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       106.08 |      7162.90 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.67 |      4306.60 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4305.83 |         1.08 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.76 |       803.00 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.00 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.09 |      2053.30 |         0.52 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2052.53 |         0.52 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       610.29 |     12423.51 |         3.13 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        20.58 |       340.99 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       340.99 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       449.44 |      4653.28 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        35.10 |      2298.42 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2297.46 |         0.58 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        30.98 |       451.87 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       451.87 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       309.60 |       826.30 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       388.48 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       437.82 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.80 |      1076.70 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1075.77 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        19.19 |       353.25 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       353.25 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       104.94 |      7075.99 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.41 |      4306.37 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4305.57 |         1.08 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        26.31 |       803.36 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.36 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.71 |      1966.26 |         0.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1965.27 |         0.49 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       635.34 |     12462.32 |         3.14 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.46 |       341.89 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.89 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       466.10 |      4610.05 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.80 |      2252.72 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2251.95 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        30.45 |       452.22 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.22 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       317.67 |       823.93 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.75 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       437.18 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        53.52 |      1081.18 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1080.41 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        21.18 |       351.17 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.17 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       111.70 |      7159.22 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.21 |      4290.63 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4289.64 |         1.08 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        27.15 |       802.91 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       802.91 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.21 |      2065.68 |         0.52 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2064.91 |         0.52 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       620.64 |     12308.37 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.98 |       342.88 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       342.88 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       457.44 |      4625.41 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.55 |      2264.11 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2263.31 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        31.12 |       452.48 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.48 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       314.74 |       823.26 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       387.26 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       436.00 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.18 |      1085.56 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1084.66 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        20.54 |       353.44 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       353.44 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       108.91 |      6986.64 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.17 |      4300.20 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4299.24 |         1.08 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        26.39 |       803.93 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.93 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.94 |      1882.52 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1881.72 |         0.47 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       638.75 |     12504.82 |         3.15 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.77 |       341.60 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.60 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       481.15 |      4682.85 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.06 |      2334.99 |         0.59 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2334.19 |         0.59 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        32.27 |       453.37 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       453.37 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       340.36 |       824.03 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.56 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       437.47 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.80 |      1070.46 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1069.46 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        22.18 |       349.28 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       349.28 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       102.82 |      7131.09 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.19 |      4328.32 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4327.56 |         1.09 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        24.94 |       803.39 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.39 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.60 |      1999.38 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1998.42 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       630.20 |     12238.33 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.38 |       341.66 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.66 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       465.48 |      4608.29 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.89 |      2262.00 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2261.23 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        31.14 |       452.73 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.73 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       319.23 |       827.55 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.65 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       440.89 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        46.93 |      1066.01 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1065.02 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        20.39 |       351.87 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.87 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       106.86 |      6936.50 |         1.75 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.12 |      4253.22 |         1.07 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4252.45 |         1.07 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.89 |       802.88 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       802.88 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.95 |      1880.40 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1879.44 |         0.47 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       669.25 |     12391.63 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.78 |       341.76 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.76 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       476.03 |      4631.78 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.56 |      2277.27 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2276.31 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        33.55 |       451.68 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       451.68 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       321.37 |       826.39 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.91 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       439.49 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        47.53 |      1076.44 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1075.64 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        21.93 |       348.86 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       348.86 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       139.51 |      7069.24 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.11 |      4314.21 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4313.25 |         1.09 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.57 |       803.71 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.71 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        67.86 |      1951.32 |         0.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1950.52 |         0.49 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       654.55 |     12327.92 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.78 |       343.04 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       343.04 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       458.80 |      4634.53 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.41 |      2280.79 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2279.79 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        31.08 |       452.67 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.67 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       318.28 |       827.77 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       387.32 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       440.44 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.62 |      1073.31 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1072.41 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        17.72 |       352.48 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       352.48 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       146.80 |      6997.88 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.53 |      4269.83 |         1.07 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4269.06 |         1.07 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        24.54 |       804.03 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       804.03 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        34.09 |      1924.02 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1923.06 |         0.48 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       611.67 |     12369.36 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.87 |       341.76 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.76 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       448.21 |      4687.68 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.16 |      2332.78 |         0.59 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2332.02 |         0.59 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        33.40 |       453.21 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       453.21 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       308.48 |       825.12 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.53 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       438.59 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.30 |      1076.57 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1075.80 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        24.76 |       351.17 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.17 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       103.66 |      6988.76 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.41 |      4307.24 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4306.21 |         1.08 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        26.16 |       804.48 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       804.48 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.44 |      1877.04 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1876.02 |         0.47 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       625.59 |     12336.18 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.29 |       342.24 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       342.24 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       473.04 |      4643.91 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.31 |      2292.62 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2291.82 |         0.58 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        31.57 |       452.38 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.38 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       331.97 |       824.70 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.72 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       437.98 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.75 |      1074.20 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1073.27 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        18.65 |       351.55 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.55 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       102.39 |      6998.48 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.08 |      4258.79 |         1.07 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4257.83 |         1.07 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.36 |       804.00 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       804.00 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.73 |      1935.70 |         0.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1934.93 |         0.49 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       613.66 |     12464.88 |         3.14 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.25 |       341.15 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.15 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       451.51 |      4626.37 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.73 |      2276.05 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2275.06 |         0.57 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        30.16 |       453.15 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       453.15 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       312.72 |       825.43 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       387.04 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       438.40 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.94 |      1071.74 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1070.74 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        21.17 |       350.46 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       350.46 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       107.15 |      7146.90 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.83 |      4344.23 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4343.43 |         1.09 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.77 |       804.03 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       804.03 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        34.91 |      1998.64 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1997.84 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       613.23 |     12406.64 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.56 |       340.44 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       340.44 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       458.98 |      4656.51 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.37 |      2302.32 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2301.36 |         0.58 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        31.83 |       452.76 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.76 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       314.58 |       826.11 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.75 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       439.36 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        47.76 |      1075.32 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1074.30 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        18.73 |       353.34 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       353.34 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       101.82 |      7056.34 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.44 |      4258.47 |         1.07 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4257.67 |         1.07 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.81 |       803.13 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       803.13 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.00 |      1994.74 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1993.75 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       614.50 |     12433.91 |         3.13 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.85 |       342.88 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       342.88 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       454.69 |      4666.28 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.55 |      2313.36 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2312.59 |         0.58 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        31.46 |       453.34 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       453.34 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       314.05 |       825.60 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       386.01 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       439.58 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.31 |      1073.98 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1073.18 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        21.18 |       349.15 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       349.15 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       105.89 |      7075.60 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.89 |      4295.46 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4294.47 |         1.08 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        26.13 |       802.97 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       802.97 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.47 |      1977.17 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1976.40 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       606.57 |     12389.62 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.18 |       341.05 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.05 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       450.36 |      4634.18 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.96 |      2295.76 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2294.80 |         0.58 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        32.22 |       452.12 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.12 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       308.93 |       827.80 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       388.06 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       439.74 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.14 |      1058.49 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1057.53 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        19.61 |       351.74 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.74 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       104.81 |      7062.64 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.41 |      4282.50 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4281.57 |         1.08 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        25.02 |       802.46 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       802.46 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        34.50 |      1977.68 |         0.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1976.88 |         0.50 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       619.57 |     12393.17 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.33 |       341.37 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.37 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       466.83 |      4652.55 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        31.74 |      2297.75 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2296.95 |         0.58 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        32.49 |       452.25 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       452.25 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       322.22 |       827.99 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       387.52 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       440.48 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.82 |      1074.55 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1073.62 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        18.58 |       349.28 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       349.28 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       102.48 |      7049.97 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        32.55 |      4334.89 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4334.09 |         1.09 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        24.59 |       805.88 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       805.88 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.86 |      1909.20 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1908.21 |         0.48 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       672.06 |     12432.91 |         3.13 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.48 |       341.44 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       341.44 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       509.04 |      4702.72 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.68 |      2329.27 |         0.59 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2328.47 |         0.59 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        37.71 |       454.88 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       454.88 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       314.05 |       826.01 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       387.55 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       438.46 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        55.30 |      1092.57 |         0.28 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1091.51 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        21.54 |       351.74 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.74 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       109.63 |      7037.01 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.42 |      4322.21 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4321.45 |         1.09 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        26.17 |       805.47 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       805.47 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        34.99 |      1909.33 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1908.56 |         0.48 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- LlamaDecoderLayer                                        |       617.44 |     12518.77 |         3.15 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.10 |       343.45 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       343.45 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaAttention                                          |       456.13 |      4695.84 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.35 |      2339.28 |         0.59 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2338.26 |         0.59 | mm(float16[16000, 4096], float16[4096, 12288]) <- matmul(...
|---- RotaryEmbedding                                        |        32.05 |       454.56 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       454.56 |         0.11 | _C::rotary_embedding(int64[16000], float16[16000, 4096], ...
|---- Attention                                              |       311.21 |       825.02 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       387.36 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[16000, 32, ...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       437.66 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[16000, 32, 128], fl...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        48.64 |      1076.99 |         0.27 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1076.03 |         0.27 | mm(float16[16000, 4096], float16[4096, 4096]) <- matmul(f...
|--- RMSNorm(weight=float16[4096])                           |        21.28 |       351.13 |         0.09 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       351.13 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
|--- LlamaMLP                                                |       107.17 |      7128.34 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.41 |      4404.13 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      4403.33 |         1.11 | mm(float16[16000, 4096], float16[4096, 22016]) <- matmul(...
|---- SiluAndMul                                             |        27.75 |       804.86 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       804.86 |         0.20 | _C::silu_and_mul(float16[16000, 11008], float16[16000, 22...
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.65 |      1919.35 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1918.36 |         0.48 | mm(float16[16000, 11008], float16[11008, 4096]) <- matmul...
|-- RMSNorm(weight=float16[4096])                            |        16.00 |       343.58 |         0.09 |                                                             
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::... |         0.00 |       343.58 |         0.09 | _C::fused_add_rms_norm(float16[16000, 4096], float16[1600...
LogitsProcessor                                              |       151.60 |       210.62 |         0.05 |                                                             
|- void at::native::(anonymous namespace)::indexSelectLar... |         0.00 |         5.66 |         0.00 | index_select(float16[16000, 4096], 0, int64[64])            
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       204.96 |         0.05 | mm(float16[64, 4096], float16[4096, 32000]) <- matmul(flo...
Sampler                                                      |    375729.57 |       613.57 |         0.15 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.40 |         0.00 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.05 |         0.00 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.56 |         0.00 | copy_(int32[64], int32[64], True) <- _to_copy(int32[64], ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.14 |         0.00 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.14 |         0.00 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.05 |         0.00 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.43 |         0.00 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         7.87 |         0.00 | copy_(float32[64, 32000], float16[64, 32000], False) <- _...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |        11.68 |         0.00 | div_(float32[64, 32000], float16[64, 1])                    
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |        10.27 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         0.99 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.70 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |        17.31 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.76 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.70 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        57.47 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.82 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        54.21 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.79 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        53.86 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.82 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        53.38 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         0.77 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |        10.82 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.98 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.63 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        28.38 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |        35.97 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         5.70 |         0.00 | copy_(float32[64, 32000], float32[64, 32000], False) <- s...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         2.11 |         0.00 | copy_(int64[64], int32[64], False) <- _to_copy(int32[64],...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.73 |         0.00 | sub(int64[], int64[64], 1) <- rsub(int64[64], 32000, 1)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.69 |         0.00 | gather(float32[64, 32000], 1, int64[64, 1], False)          
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         8.90 |         0.00 | lt(float32[64, 32000], float32[64, 1])                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         4.61 |         0.00 | masked_fill_(float32[64, 32000], bool[64, 32000], -inf)     
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        12.26 |         0.00 | _softmax(float32[64, 32000], -1, False) <- softmax(float3...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        51.42 |         0.01 | cumsum(float32[64, 32000], -1, None)                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.73 |         0.00 | sub(int64[], float16[64, 1], 1) <- rsub(float16[64, 1], 1...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |        11.81 |         0.00 | le(float32[64, 32000], float16[64, 1])                      
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.98 |         0.00 | fill_(bool[64], bool[])                                     
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         7.78 |         0.00 | masked_fill_(float32[64, 32000], bool[64, 32000], -inf)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |        51.90 |         0.01 | scatter_(float32[64, 32000], -1, int64[64, 32000], float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        18.91 |         0.00 | _softmax(float32[64, 32000], -1, False) <- softmax(float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.85 |         0.00 | _log_softmax(float32[64, 32000], -1, False) <- log_softma...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         2.02 |         0.00 | copy_(int64[64], int32[64], False) <- _to_copy(int32[64],...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |        13.86 |         0.00 | index(float32[64, 32000], None)                             
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         7.07 |         0.00 | exponential_(float32[64, 32000], 1.0, None)                 
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         7.30 |         0.00 | div_(float32[64, 32000], float32[64, 32000])                
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        11.90 |         0.00 | argmax(float32[64, 32000], 1, False)                        
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.66 |         0.00 | copy_(int64[64, 1], int64[64, 1], False) <- _to_copy(int6...

================================================================================
= First Decode Step Model Table (prompt_len=250, batch_size=64)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
CUDAGraphRunner                                              |       586.00 |     13545.09 |        94.58 |                                                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.34 |         0.01 | copy_(int64[64], int64[64], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.31 |         0.01 | copy_(int64[64], int64[64], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.38 |         0.01 | copy_(int64[64], int64[64], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.38 |         0.01 | copy_(int32[64], int32[64], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.57 |         0.01 | copy_(int32[64, 256], int32[64, 256], True)                 
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.12 |         0.01 | fill_(int64[1], 0)                                          
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.25 |         0.01 | fill_(int64[1], 24)                                         
|- void at::native::(anonymous namespace)::indexSelectLar... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10:... |         0.00 |         4.51 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.94 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |       177.73 |         1.24 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.38 |         0.59 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.26 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        27.01 |         0.19 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.17 |         0.02 |                                                             
LogitsProcessor                                              |       266.36 |       198.11 |         1.38 |                                                             
|- void at::native::(anonymous namespace)::indexSelectLar... |         0.00 |         4.77 |         0.03 | index_select(float16[64, 4096], 0, int64[64])               
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       193.34 |         1.35 | mm(float16[64, 4096], float16[4096, 32000]) <- matmul(flo...
Sampler                                                      |     12096.36 |       578.14 |         4.04 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.08 |         0.01 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.14 |         0.01 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.08 |         0.01 | copy_(int32[64], int32[64], True) <- _to_copy(int32[64], ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.08 |         0.01 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.01 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.01 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.05 |         0.01 | copy_(float16[64], float16[64], True) <- _to_copy(float16...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         7.26 |         0.05 | copy_(float32[64, 32000], float16[64, 32000], False) <- _...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |        10.40 |         0.07 | div_(float32[64, 32000], float16[64, 1])                    
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |        10.21 |         0.07 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         0.70 |         0.00 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |        16.70 |         0.12 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.79 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.57 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        53.73 |         0.38 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.73 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        50.98 |         0.36 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.82 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        50.27 |         0.35 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.73 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        50.46 |         0.35 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         0.99 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |        10.88 |         0.08 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.70 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.01 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        26.30 |         0.18 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |        35.81 |         0.25 | sort(float32[64, 32000], False, -1, False) <- sort(float3...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         5.86 |         0.04 | copy_(float32[64, 32000], float32[64, 32000], False) <- s...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         2.05 |         0.01 | copy_(int64[64], int32[64], False) <- _to_copy(int32[64],...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.66 |         0.01 | sub(int64[], int64[64], 1) <- rsub(int64[64], 32000, 1)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.75 |         0.02 | gather(float32[64, 32000], 1, int64[64, 1], False)          
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         8.19 |         0.06 | lt(float32[64, 32000], float32[64, 1])                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         5.25 |         0.04 | masked_fill_(float32[64, 32000], bool[64, 32000], -inf)     
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        11.46 |         0.08 | _softmax(float32[64, 32000], -1, False) <- softmax(float3...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        46.72 |         0.33 | cumsum(float32[64, 32000], -1, None)                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.70 |         0.01 | sub(int64[], float16[64, 1], 1) <- rsub(float16[64, 1], 1...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |        10.69 |         0.07 | le(float32[64, 32000], float16[64, 1])                      
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.79 |         0.01 | fill_(bool[64], bool[])                                     
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         7.71 |         0.05 | masked_fill_(float32[64, 32000], bool[64, 32000], -inf)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |        48.96 |         0.34 | scatter_(float32[64, 32000], -1, int64[64, 32000], float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        17.22 |         0.12 | _softmax(float32[64, 32000], -1, False) <- softmax(float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.46 |         0.07 | _log_softmax(float32[64, 32000], -1, False) <- log_softma...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.98 |         0.01 | copy_(int64[64], int32[64], False) <- _to_copy(int32[64],...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |        13.41 |         0.09 | index(float32[64, 32000], None)                             
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         6.46 |         0.05 | exponential_(float32[64, 32000], 1.0, None)                 
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         6.82 |         0.05 | div_(float32[64, 32000], float32[64, 32000])                
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        10.78 |         0.08 | argmax(float32[64, 32000], 1, False)                        
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.30 |         0.02 | copy_(int64[64, 1], int64[64, 1], False) <- _to_copy(int6...

================================================================================
= Prefill Summary Table (prompt_len=250, batch_size=64)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
LlamaForCausalLM                                                                 |    396448.26 |        99.79 |            1.00
|- LlamaModel                                                                    |    396448.26 |        99.79 |            1.00
|-- VocabParallelEmbedding(weight=float16[32000, 4096])                          |       469.28 |         0.12 |            1.00
|--- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half,... |       469.28 |         0.12 |            1.00
|-- LlamaDecoderLayer                                                            |    395635.40 |        99.59 |           32.00
|--- RMSNorm(weight=float16[4096])                                               |     22098.87 |         5.56 |           64.00
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10... |       262.40 |         0.07 |            1.00
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void... |     21836.47 |         5.50 |           63.00
|--- LlamaAttention                                                              |    148139.10 |        37.29 |           32.00
|---- QKVParallelLinear(weight=float16[12288, 4096])                             |     72974.84 |        18.37 |           32.00
|----- Memset (Device)                                                           |        27.74 |         0.01 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |     72947.10 |        18.36 |           32.00
|---- RotaryEmbedding                                                            |     14461.12 |         3.64 |           32.00
|----- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::... |     14461.12 |         3.64 |           32.00
|---- Attention                                                                  |     26394.62 |         6.64 |           32.00
|----- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned sho... |     12371.99 |         3.11 |           32.00
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128, 128, 64, 4, false, ... |     14022.63 |         3.53 |           32.00
|---- RowParallelLinear(weight=float16[4096, 4096])                              |     34308.52 |         8.64 |           32.00
|----- Memset (Device)                                                           |        29.34 |         0.01 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |     34279.18 |         8.63 |           32.00
|--- LlamaMLP                                                                    |    225397.43 |        56.74 |           32.00
|---- MergedColumnParallelLinear(weight=float16[22016, 4096])                    |    136608.32 |        34.39 |           32.00
|----- Memset (Device)                                                           |        27.74 |         0.01 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |    136580.58 |        34.38 |           32.00
|---- SiluAndMul                                                                 |     25692.54 |         6.47 |           32.00
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel... |     25692.54 |         6.47 |           32.00
|---- RowParallelLinear(weight=float16[4096, 11008])                             |     63096.56 |        15.88 |           32.00
|----- Memset (Device)                                                           |        28.58 |         0.01 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x256x64_warpgroupsize... |     63067.99 |        15.88 |           32.00
|-- RMSNorm(weight=float16[4096])                                                |       343.58 |         0.09 |            1.00
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>... |       343.58 |         0.09 |            1.00
LogitsProcessor                                                                  |       210.62 |         0.05 |            1.00
|- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half, l... |         5.66 |         0.00 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x64_warpgroupsize1x1x1... |       204.96 |         0.05 |            1.00
Sampler                                                                          |       613.57 |         0.15 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        15.78 |         0.00 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         7.87 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |        11.68 |         0.00 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |        10.27 |         0.00 |            1.00
|- Memset (Device)                                                               |        13.66 |         0.00 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |        17.31 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.76 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |       218.91 |         0.06 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |        10.82 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.98 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        28.38 |         0.01 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |        35.97 |         0.01 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         5.70 |         0.00 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.13 |         0.00 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.73 |         0.00 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.69 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         8.90 |         0.00 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |        12.38 |         0.00 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        31.17 |         0.01 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        51.42 |         0.01 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.73 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |        11.81 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.98 |         0.00 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |        51.90 |         0.01 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        10.85 |         0.00 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |        13.86 |         0.00 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         7.07 |         0.00 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         7.30 |         0.00 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        11.90 |         0.00 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.66 |         0.00 |            1.00

================================================================================
= First Decode Step Summary Table (prompt_len=250, batch_size=64)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
CUDAGraphRunner                                                                  |     13545.09 |        94.58 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         6.98 |         0.05 |            5.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<... |         2.37 |         0.02 |            2.00
|- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half, l... |         4.29 |         0.03 |            1.00
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10::H... |         4.51 |         0.03 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |      5400.58 |        37.71 |           64.00
|- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::Half... |       149.50 |         1.04 |           32.00
|- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned short, ... |       125.92 |         0.88 |           32.00
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<128, 64, 128, 4, fal... |      5687.30 |        39.71 |           32.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64_warpgroupsize1x1x1_... |      1728.45 |        12.07 |           64.00
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>::... |       202.75 |         1.42 |           64.00
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel<c10... |       232.45 |         1.62 |           32.00
LogitsProcessor                                                                  |       198.11 |         1.38 |            1.00
|- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half, l... |         4.77 |         0.03 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x64_warpgroupsize1x1x1... |       193.34 |         1.35 |            1.00
Sampler                                                                          |       578.14 |         4.04 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.46 |         0.10 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         7.26 |         0.05 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |        10.40 |         0.07 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |        10.21 |         0.07 |            1.00
|- Memset (Device)                                                               |        12.96 |         0.09 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |        16.70 |         0.12 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.79 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |       205.44 |         1.43 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |        10.88 |         0.08 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.70 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        26.30 |         0.18 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |        35.81 |         0.25 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         5.86 |         0.04 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.03 |         0.03 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.66 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.75 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         8.19 |         0.06 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |        12.96 |         0.09 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        28.67 |         0.20 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        46.72 |         0.33 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.70 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |        10.69 |         0.07 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.79 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |        48.96 |         0.34 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        10.46 |         0.07 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |        13.41 |         0.09 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         6.46 |         0.05 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         6.82 |         0.05 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        10.78 |         0.08 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.30 |         0.02 |            1.00
Traces saved as prefill.json and decode_1.json, etc. in folder benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_64/chrome_traces
