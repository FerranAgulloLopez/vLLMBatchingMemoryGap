Run profile with:
  engine_args = {'model': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'served_model_name': None, 'tokenizer': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'skip_tokenizer_init': False, 'tokenizer_mode': 'auto', 'trust_remote_code': False, 'download_dir': None, 'load_format': 'auto', 'config_format': <ConfigFormat.AUTO: 'auto'>, 'dtype': 'auto', 'kv_cache_dtype': 'auto', 'quantization_param_path': None, 'seed': 0, 'max_model_len': None, 'worker_use_ray': False, 'distributed_executor_backend': None, 'pipeline_parallel_size': 1, 'tensor_parallel_size': 1, 'max_parallel_loading_workers': None, 'block_size': 16, 'enable_prefix_caching': False, 'disable_sliding_window': False, 'use_v2_block_manager': False, 'swap_space': 4, 'cpu_offload_gb': 0, 'gpu_memory_utilization': 0.9, 'max_num_batched_tokens': 256000, 'max_num_seqs': 1024, 'max_logprobs': 20, 'disable_log_stats': False, 'revision': None, 'code_revision': None, 'rope_scaling': None, 'rope_theta': None, 'tokenizer_revision': None, 'quantization': None, 'enforce_eager': False, 'max_context_len_to_capture': None, 'max_seq_len_to_capture': 8192, 'disable_custom_all_reduce': False, 'tokenizer_pool_size': 0, 'tokenizer_pool_type': 'ray', 'tokenizer_pool_extra_config': None, 'limit_mm_per_prompt': None, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'enable_prompt_adapter': False, 'max_prompt_adapters': 1, 'max_prompt_adapter_token': 0, 'fully_sharded_loras': False, 'lora_extra_vocab_size': 256, 'long_lora_scaling_factors': None, 'lora_dtype': 'auto', 'max_cpu_loras': None, 'device': 'auto', 'num_scheduler_steps': 1, 'multi_step_stream_outputs': True, 'ray_workers_use_nsight': False, 'num_gpu_blocks_override': None, 'num_lookahead_slots': 0, 'model_loader_extra_config': None, 'ignore_patterns': [], 'preemption_mode': None, 'scheduler_delay_factor': 0.0, 'enable_chunked_prefill': None, 'guided_decoding_backend': 'outlines', 'speculative_model': None, 'speculative_model_quantization': None, 'speculative_draft_tensor_parallel_size': None, 'num_speculative_tokens': None, 'speculative_disable_mqa_scorer': False, 'speculative_max_model_len': None, 'speculative_disable_by_batch_size': None, 'ngram_prompt_lookup_max': None, 'ngram_prompt_lookup_min': None, 'spec_decoding_acceptance_method': 'rejection_sampler', 'typical_acceptance_sampler_posterior_threshold': None, 'typical_acceptance_sampler_posterior_alpha': None, 'qlora_adapter_name_or_path': None, 'disable_logprobs_during_spec_decoding': None, 'otlp_traces_endpoint': None, 'collect_detailed_traces': None, 'disable_async_output_proc': False, 'override_neuron_config': None, 'mm_processor_kwargs': None, 'scheduling_policy': 'fcfs'}
  prompt_len = 250
  output_len = 230
  batch_size = 4
  result_dir = benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_4
  save_chrome_traces_folder = chrome_traces
WARNING 11-22 16:42:35 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 11-22 16:42:35 llm_engine.py:237] Initializing an LLM engine (v0.5.2.dev1210+g6f078ea3c) with config: model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', speculative_config=None, tokenizer='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 11-22 16:42:36 model_runner.py:1056] Starting to load model /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b...
INFO 11-22 16:42:54 model_runner.py:1067] Loading model weights took 12.5523 GB
INFO 11-22 16:43:02 worker.py:262] Memory profiling results: total_gpu_memory=63.43GiB initial_memory_usage=13.15GiB peak_torch_memory=36.15GiB non_torch_memory=0.10GiB kv_cache_size=20.84GiB gpu_memory_utilization=0.90
INFO 11-22 16:43:02 gpu_executor.py:122] # GPU blocks: 2667, # CPU blocks: 512
INFO 11-22 16:43:02 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 10.42x
INFO 11-22 16:43:03 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 16:43:03 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 16:43:34 model_runner.py:1523] Graph capturing finished in 31 secs.
llm.llm_engine.model_config.max_model_len:  4096
Warm up run ...
Profile run ...
INFO 11-22 16:43:39 metrics.py:349] Avg prompt throughput: 391.4 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:44 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:49 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 32.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:54 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.
INFO 11-22 16:44:00 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%.
================================================================================
= Prefill Model Table (prompt_len=250, batch_size=4)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
LlamaForCausalLM                                             |     22933.74 |     25761.88 |        98.37 |                                                             
|- LlamaModel                                                |     22910.96 |     25761.88 |        98.37 |                                                             
|-- VocabParallelEmbedding(weight=float16[32000, 4096])      |        69.21 |        29.98 |         0.11 |                                                             
|--- void at::native::(anonymous namespace)::indexSelectL... |         0.00 |        29.98 |         0.11 | index_select(float16[32000, 4096], 0, int64[1000]) <- emb...
|-- LlamaDecoderLayer                                        |      2623.69 |       805.79 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        68.70 |        13.79 |         0.05 |                                                             
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c... |         0.00 |        13.79 |         0.05 | _C::rms_norm(float16[1000, 4096], float16[1000, 4096], fl...
|--- LlamaAttention                                          |      2335.29 |       302.53 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |       106.43 |       160.80 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       160.03 |         0.61 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        58.80 |        26.11 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.11 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |      2020.12 |        55.04 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.20 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        31.84 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        86.81 |        60.58 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        60.58 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        39.29 |        11.97 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.97 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       153.09 |       477.50 |         1.82 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        56.31 |       292.54 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         0.86 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       290.91 |         1.11 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        41.71 |        51.97 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        51.97 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.06 |       132.99 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       132.99 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       719.70 |       805.09 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.71 |        11.46 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.46 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       541.19 |       302.11 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.09 |       158.11 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       157.34 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        40.44 |        25.92 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        25.92 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       362.68 |        55.52 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.04 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.48 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        58.47 |        62.56 |         0.24 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        62.56 |         0.24 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.52 |        12.19 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.19 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       115.88 |       479.33 |         1.83 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.98 |       293.31 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.25 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         0.86 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       291.20 |         1.11 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        28.47 |        52.03 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.03 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.18 |       133.98 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       133.98 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       659.95 |       803.84 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.59 |        11.71 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.71 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       480.59 |       299.07 |         1.14 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.33 |       157.82 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       157.06 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        33.49 |        26.14 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.14 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       334.20 |        55.90 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.74 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.16 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.04 |        59.20 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        59.20 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.76 |        11.90 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.90 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       113.64 |       481.15 |         1.84 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.10 |       294.88 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       293.09 |         1.12 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        27.40 |        52.45 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.45 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.99 |       133.82 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       133.82 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       651.79 |       800.00 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.80 |        11.55 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.55 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       486.48 |       301.76 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.28 |       160.70 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       159.74 |         0.61 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        32.39 |        26.50 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.50 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       334.23 |        55.65 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.49 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.16 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        48.68 |        58.91 |         0.22 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        58.91 |         0.22 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.44 |        12.19 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.19 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       114.57 |       474.49 |         1.81 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        45.81 |       288.48 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.18 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       286.50 |         1.09 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.94 |        52.32 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.32 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.69 |       133.69 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       133.69 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       658.55 |       803.42 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.01 |        12.06 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.06 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       489.91 |       297.66 |         1.14 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.04 |       157.76 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       156.99 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        32.25 |        25.98 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        25.98 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       336.78 |        55.78 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.49 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.29 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        52.99 |        58.14 |         0.22 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        58.14 |         0.22 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.95 |        11.71 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.71 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       114.34 |       481.98 |         1.84 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.06 |       296.96 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.18 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       294.81 |         1.13 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.58 |        51.71 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        51.71 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        34.32 |       133.31 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       133.31 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       697.26 |       795.87 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.83 |        11.46 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.46 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       533.02 |       300.32 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.61 |       157.44 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       156.67 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        33.38 |        26.53 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.53 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       337.56 |        55.84 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.36 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.48 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        79.71 |        60.51 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        60.51 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.79 |        11.90 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.90 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       111.82 |       472.19 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        40.80 |       285.34 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.25 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       283.20 |         1.08 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.79 |        51.81 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        51.81 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.51 |       135.04 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       135.04 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       629.78 |       809.72 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.22 |        11.55 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.55 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       464.33 |       301.12 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.08 |       157.50 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       156.57 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.32 |        26.37 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.37 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       314.90 |        55.71 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.42 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.29 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.33 |        61.54 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        61.54 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.79 |        12.29 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.29 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       113.53 |       484.77 |         1.85 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.68 |       298.14 |         1.14 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.22 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       295.90 |         1.13 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.14 |        52.29 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.29 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.69 |       134.34 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       134.34 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       644.09 |       796.96 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.25 |        11.30 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.30 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       483.25 |       301.15 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.03 |       157.47 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       156.51 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        35.27 |        26.50 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.50 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       330.88 |        55.81 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.49 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.32 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        48.80 |        61.38 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        61.38 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.42 |        12.35 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.35 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       109.61 |       472.16 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        40.22 |       284.54 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.28 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       282.30 |         1.08 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.38 |        52.32 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.32 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.99 |       135.30 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       135.30 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       657.65 |       807.04 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.94 |        11.97 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.97 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       486.20 |       301.02 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.90 |       159.33 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       158.53 |         0.61 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.25 |        25.92 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        25.92 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       341.70 |        55.78 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.58 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.19 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.70 |        60.00 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        60.00 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.76 |        11.62 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.62 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       117.92 |       482.43 |         1.84 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.82 |       295.97 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.34 |         0.01 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       293.66 |         1.12 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.45 |        52.48 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.48 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.03 |       133.98 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       133.98 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       647.21 |       799.33 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.98 |        11.17 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.17 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       481.76 |       302.21 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.19 |       159.04 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       158.08 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        32.08 |        26.43 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.43 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       338.26 |        56.00 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.26 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.74 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.38 |        60.74 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        60.74 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.80 |        11.78 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.78 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       113.36 |       474.17 |         1.81 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.39 |       287.30 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.28 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       285.25 |         1.09 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.45 |        51.94 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        51.94 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.58 |       134.94 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       134.94 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       615.44 |       807.81 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.87 |        11.71 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.71 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       449.93 |       300.00 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.75 |       157.82 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       157.06 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        33.18 |        26.66 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.66 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       311.38 |        55.68 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.30 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.38 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.87 |        59.84 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        59.84 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        24.67 |        11.74 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.74 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       110.43 |       484.35 |         1.85 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.50 |       298.02 |         1.14 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.31 |         0.01 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       295.81 |         1.13 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.38 |        52.35 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.35 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.35 |       133.98 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       133.98 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       636.19 |       811.71 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.61 |        11.36 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.36 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       470.75 |       301.41 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.73 |       158.98 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       158.18 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.05 |        25.92 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        25.92 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       307.22 |        55.17 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.36 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        31.81 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.88 |        61.34 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        61.34 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.39 |        12.16 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.16 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       112.91 |       486.78 |         1.86 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.37 |       297.70 |         1.14 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.22 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       295.55 |         1.13 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.25 |        52.90 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.90 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.27 |       136.19 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       136.19 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       639.42 |       793.21 |         3.03 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.29 |        11.94 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.94 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       470.43 |       296.90 |         1.13 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.10 |       157.18 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       156.26 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.34 |        26.02 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.02 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       327.05 |        56.03 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.49 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.54 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.16 |        57.66 |         0.22 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        57.66 |         0.22 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.04 |        11.65 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.65 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       116.26 |       472.74 |         1.81 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        45.46 |       289.57 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.22 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       287.45 |         1.10 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.50 |        51.68 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        51.68 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.77 |       131.49 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       131.49 |         0.50 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       681.33 |       809.18 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.65 |        11.30 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.30 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       467.99 |       300.51 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.03 |       157.60 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       156.80 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        32.15 |        26.02 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.02 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       326.89 |        54.46 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        22.95 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        31.52 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.97 |        62.43 |         0.24 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        62.43 |         0.24 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.62 |        11.94 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.94 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       157.60 |       485.44 |         1.85 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        87.21 |       298.46 |         1.14 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.25 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       296.42 |         1.13 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        28.35 |        51.97 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        51.97 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.19 |       135.01 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       135.01 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       610.96 |       795.97 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.71 |        11.94 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.94 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       456.92 |       302.85 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.37 |       158.78 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       157.86 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        30.98 |        26.34 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.34 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       320.60 |        55.78 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.55 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.22 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.52 |        61.95 |         0.24 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        61.95 |         0.24 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        18.74 |        11.94 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.94 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       104.17 |       469.25 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.49 |       282.85 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.31 |         0.01 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       280.57 |         1.07 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.46 |        52.99 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.99 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.74 |       133.41 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       133.41 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       631.34 |       797.86 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.07 |        11.33 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.33 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       464.41 |       301.50 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.11 |       159.65 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       158.69 |         0.61 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        30.32 |        25.66 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        25.66 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       318.66 |        55.94 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.39 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.54 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.04 |        60.26 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        60.26 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.33 |        12.00 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.00 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       112.96 |       473.02 |         1.81 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.20 |       284.51 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.31 |         0.01 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       282.24 |         1.08 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.66 |        52.26 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.26 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.79 |       136.26 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       136.26 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       613.28 |       803.90 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.89 |        11.81 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.81 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       452.11 |       299.71 |         1.14 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.34 |       158.30 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       157.34 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        32.44 |        26.40 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.40 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       306.30 |        55.58 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.36 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.22 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.53 |        59.42 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        59.42 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.76 |        11.87 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.87 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       109.29 |       480.51 |         1.83 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.08 |       294.75 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.34 |         0.01 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       292.45 |         1.12 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        28.40 |        52.06 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.06 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.25 |       133.69 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       133.69 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       607.01 |       794.88 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.50 |        11.42 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.42 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       445.25 |       300.77 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.00 |       157.66 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       156.90 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.66 |        26.14 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.14 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       308.62 |        55.49 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.58 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        31.90 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.28 |        61.47 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        61.47 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.92 |        12.32 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.32 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       107.56 |       470.37 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        38.55 |       283.58 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.25 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       281.41 |         1.07 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        27.53 |        52.03 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.03 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.04 |       134.75 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       134.75 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       611.53 |       809.60 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.39 |        11.81 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.81 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       452.77 |       297.70 |         1.14 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.54 |       157.50 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       156.70 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.73 |        26.56 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.56 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       313.03 |        55.33 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.30 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.03 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.00 |        58.30 |         0.22 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        58.30 |         0.22 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.02 |        11.74 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.74 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       107.57 |       488.35 |         1.86 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.50 |       299.49 |         1.14 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.31 |         0.01 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       297.22 |         1.13 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        24.64 |        52.35 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.35 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.73 |       136.51 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       136.51 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       599.43 |       812.12 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.20 |        11.36 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.36 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       443.88 |       304.57 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.65 |       158.11 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       157.34 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        34.62 |        26.21 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.21 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       303.68 |        56.48 |         0.22 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.97 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.51 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.25 |        63.78 |         0.24 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        63.78 |         0.24 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        18.66 |        12.00 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.00 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       105.84 |       484.19 |         1.85 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.89 |       295.01 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.31 |         0.01 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       292.74 |         1.12 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        24.65 |        52.13 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.13 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.36 |       137.06 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       137.06 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       654.66 |       795.33 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.27 |        11.87 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.87 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       495.35 |       298.18 |         1.14 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.57 |       157.73 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       156.96 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.34 |        25.92 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        25.92 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       354.96 |        55.58 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.07 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.51 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.34 |        58.94 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        58.94 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        18.88 |        11.39 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.39 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       109.99 |       473.89 |         1.81 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.63 |       288.26 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.15 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       286.14 |         1.09 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.35 |        52.83 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.83 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.00 |       132.80 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       132.80 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       668.69 |       808.73 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        25.15 |        11.26 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.26 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       459.47 |       301.47 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.74 |       159.10 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       158.14 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        30.67 |        25.98 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        25.98 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       310.30 |        55.74 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.55 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.19 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        46.65 |        60.64 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        60.64 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.67 |        12.26 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.26 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       146.84 |       483.74 |         1.85 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.20 |       294.97 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.31 |         0.01 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       292.89 |         1.12 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.44 |        52.06 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.06 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        69.56 |       136.70 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       136.70 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       674.80 |       811.81 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.04 |        12.06 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.06 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       466.23 |       301.44 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        31.60 |       158.98 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       158.21 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        33.90 |        26.02 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.02 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       319.18 |        55.23 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.10 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.13 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        47.72 |        61.22 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        61.22 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.77 |        11.97 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.97 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       156.18 |       486.33 |         1.86 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.18 |       299.58 |         1.14 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.25 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       297.41 |         1.14 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        27.05 |        52.22 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.22 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.98 |       134.53 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       134.53 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       615.07 |       807.13 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.88 |        11.46 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.46 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       455.82 |       302.40 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.58 |       159.07 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       158.27 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        33.17 |        26.27 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.27 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       307.02 |        55.77 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.49 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.29 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.65 |        61.28 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        61.28 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.11 |        11.81 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.81 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       107.67 |       481.47 |         1.84 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.66 |       293.60 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.28 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       291.42 |         1.11 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.80 |        52.16 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.16 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.70 |       135.71 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       135.71 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       643.72 |       806.53 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.57 |        12.06 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.06 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       462.23 |       301.60 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.46 |       159.78 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       158.81 |         0.61 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.50 |        26.43 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.43 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       320.30 |        55.97 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.42 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.54 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.63 |        59.42 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        59.42 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        28.26 |        11.81 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.81 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       121.57 |       481.06 |         1.84 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        50.93 |       296.86 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.22 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       294.66 |         1.13 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        27.21 |        52.67 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.67 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.32 |       131.52 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       131.52 |         0.50 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       620.15 |       809.28 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.38 |        11.33 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.33 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       458.13 |       303.94 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.95 |       158.43 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       157.66 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        32.31 |        26.50 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.50 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       313.99 |        55.78 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.46 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.32 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.95 |        63.23 |         0.24 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        63.23 |         0.24 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.36 |        11.46 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.46 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       108.44 |       482.56 |         1.84 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.05 |       295.39 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.22 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       293.38 |         1.12 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.02 |        52.38 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.38 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.43 |       134.78 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       134.78 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       614.79 |       811.23 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.98 |        11.78 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.78 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       459.20 |       301.50 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.50 |       158.30 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       157.38 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.60 |        26.30 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.30 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       312.76 |        55.55 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.49 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.06 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.89 |        61.34 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        61.34 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        18.54 |        12.03 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.03 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       105.56 |       485.92 |         1.86 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.48 |       299.81 |         1.14 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.28 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       297.54 |         1.14 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        24.66 |        52.03 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.03 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.17 |       134.08 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       134.08 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       622.97 |       800.51 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.85 |        11.55 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.55 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       451.76 |       301.12 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.98 |       160.51 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       159.55 |         0.61 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        30.61 |        25.92 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        25.92 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       308.64 |        55.33 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.36 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        31.97 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.50 |        59.36 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        59.36 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.87 |        12.06 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.06 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       115.98 |       475.78 |         1.82 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.89 |       288.06 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.28 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       285.82 |         1.09 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.16 |        52.77 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.77 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.17 |       134.94 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       134.94 |         0.52 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       606.57 |       796.80 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.03 |        12.10 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.10 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       448.42 |       299.58 |         1.14 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.36 |       158.56 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       157.76 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        30.95 |        26.02 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.02 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       309.30 |        55.81 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.39 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.42 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.54 |        59.20 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        59.20 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.41 |        11.97 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.97 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       107.51 |       473.15 |         1.81 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        38.93 |       288.10 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.18 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       285.95 |         1.09 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.84 |        52.42 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.42 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.14 |       132.64 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       132.64 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       629.50 |       796.73 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.36 |        11.46 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.46 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       470.64 |       300.16 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        35.01 |       157.22 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       156.26 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.89 |        26.27 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.27 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       318.05 |        55.26 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.01 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.26 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.53 |        61.41 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        61.41 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        18.45 |        12.32 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.32 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       109.86 |       472.80 |         1.81 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.07 |       288.57 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.28 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       286.53 |         1.09 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.98 |        51.46 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        51.46 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.10 |       132.77 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       132.77 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       666.59 |       805.15 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.34 |        11.62 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.62 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       502.95 |       301.34 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.32 |       159.10 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       158.34 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.71 |        26.11 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        26.11 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       317.02 |        55.04 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.33 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        31.71 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        50.11 |        61.09 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        61.09 |         0.23 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.55 |        11.94 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.94 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       110.57 |       480.26 |         1.83 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        40.92 |       296.77 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.31 |         0.01 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       294.50 |         1.12 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.48 |        51.87 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        51.87 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.28 |       131.62 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       131.62 |         0.50 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       641.19 |       807.62 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.62 |        11.14 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        11.14 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaAttention                                          |       474.74 |       303.62 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        35.54 |       158.78 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       158.01 |         0.60 | mm(float16[1000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        34.68 |        25.89 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        25.89 |         0.10 | _C::rotary_embedding(int64[1000], float16[1000, 4096], fl...
|---- Attention                                              |       324.71 |        55.81 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        23.39 |         0.09 | _C_cache_ops::reshape_and_cache_flash(float16[1000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        32.42 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[1000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.07 |        63.14 |         0.24 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        63.14 |         0.24 | mm(float16[1000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.53 |        12.00 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        12.00 |         0.05 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
|--- LlamaMLP                                                |       114.50 |       480.86 |         1.84 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.12 |       294.98 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- Memset (Device)                                       |         0.00 |         1.25 |         0.00 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       292.93 |         1.12 | mm(float16[1000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.00 |        52.00 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        52.00 |         0.20 | _C::silu_and_mul(float16[1000, 11008], float16[1000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.08 |       133.89 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       133.89 |         0.51 | mm(float16[1000, 11008], float16[11008, 4096]) <- matmul(...
|-- RMSNorm(weight=float16[4096])                            |        16.47 |        11.74 |         0.04 |                                                             
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::... |         0.00 |        11.74 |         0.04 | _C::fused_add_rms_norm(float16[1000, 4096], float16[1000,...
LogitsProcessor                                              |       110.51 |       192.70 |         0.74 |                                                             
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         4.03 |         0.02 | index_select(float16[1000, 4096], 0, int64[4])              
|- Memset (Device)                                           |         0.00 |         0.77 |         0.00 | mm(float16[4, 4096], float16[4096, 32000]) <- matmul(floa...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       187.90 |         0.72 | mm(float16[4, 4096], float16[4096, 32000]) <- matmul(floa...
Sampler                                                      |      5980.79 |       233.50 |         0.89 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.34 |         0.01 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.01 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.30 |         0.01 | copy_(int32[4], int32[4], True) <- _to_copy(int32[4], 3, ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.01 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.01 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.01 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.95 |         0.01 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         4.13 |         0.02 | copy_(float32[4, 32000], float16[4, 32000], False) <- _to...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.32 |         0.02 | div_(float32[4, 32000], float16[4, 1])                      
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |         2.02 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.06 |         0.00 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         3.71 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.76 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        10.34 |         0.04 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        10.18 |         0.04 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         9.70 |         0.04 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         9.73 |         0.04 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.77 |         0.00 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.28 |         0.00 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         2.78 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.44 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         7.87 |         0.03 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |         3.58 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.86 |         0.01 | copy_(float32[4, 32000], float32[4, 32000], False) <- sor...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.70 |         0.01 | copy_(int64[4], int32[4], False) <- _to_copy(int32[4], 4,...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.31 |         0.01 | sub(int64[], int64[4], 1) <- rsub(int64[4], 32000, 1)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.56 |         0.01 | gather(float32[4, 32000], 1, int64[4, 1], False)            
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.17 |         0.01 | lt(float32[4, 32000], float32[4, 1])                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.73 |         0.01 | masked_fill_(float32[4, 32000], bool[4, 32000], -inf)       
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.69 |         0.04 | _softmax(float32[4, 32000], -1, False) <- softmax(float32...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        46.62 |         0.18 | cumsum(float32[4, 32000], -1, None)                         
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.66 |         0.01 | sub(int64[], float16[4, 1], 1) <- rsub(float16[4, 1], 1, 1) 
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.38 |         0.02 | le(float32[4, 32000], float16[4, 1])                        
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.60 |         0.01 | fill_(bool[4], bool[])                                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.73 |         0.01 | masked_fill_(float32[4, 32000], bool[4, 32000], -inf)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         5.31 |         0.02 | scatter_(float32[4, 32000], -1, int64[4, 32000], float32[...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        16.26 |         0.06 | _softmax(float32[4, 32000], -1, False) <- softmax(float32...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |         9.05 |         0.03 | _log_softmax(float32[4, 32000], -1, False) <- log_softmax...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.95 |         0.01 | copy_(int64[4], int32[4], False) <- _to_copy(int32[4], 4,...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         4.61 |         0.02 | index(float32[4, 32000], None)                              
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         2.98 |         0.01 | exponential_(float32[4, 32000], 1.0, None)                  
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.43 |         0.01 | div_(float32[4, 32000], float32[4, 32000])                  
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        11.14 |         0.04 | argmax(float32[4, 32000], 1, False)                         
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.62 |         0.01 | copy_(int64[4, 1], int64[4, 1], False) <- _to_copy(int64[...

================================================================================
= First Decode Step Model Table (prompt_len=250, batch_size=4)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
CUDAGraphRunner                                              |       636.50 |     12313.54 |        96.75 |                                                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.41 |         0.01 | copy_(int64[4], int64[4], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.34 |         0.01 | copy_(int64[4], int64[4], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.41 |         0.01 | copy_(int64[4], int64[4], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.15 |         0.01 | copy_(int32[4], int32[4], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.44 |         0.01 | copy_(int32[4, 256], int32[4, 256], True)                   
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.31 |         0.01 | fill_(int64[1], 0)                                          
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.12 |         0.01 | fill_(int64[1], 12)                                         
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         4.00 |         0.03 |                                                             
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10:... |         0.00 |         4.22 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.78 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        15.81 |         0.12 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.55 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        76.96 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       124.03 |         0.97 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.91 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.19 |         0.50 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.95 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
LogitsProcessor                                              |       233.27 |       181.79 |         1.43 |                                                             
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         3.81 |         0.03 | index_select(float16[4, 4096], 0, int64[4])                 
|- Memset (Device)                                           |         0.00 |         0.96 |         0.01 | mm(float16[4, 4096], float16[4096, 32000]) <- matmul(floa...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       177.02 |         1.39 | mm(float16[4, 4096], float16[4096, 32000]) <- matmul(floa...
Sampler                                                      |      9800.40 |       231.55 |         1.82 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.02 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.05 |         0.02 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.02 | copy_(int32[4], int32[4], True) <- _to_copy(int32[4], 3, ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.02 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.02 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.02 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.11 |         0.02 | copy_(float16[4], float16[4], True) <- _to_copy(float16[4...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         4.06 |         0.03 | copy_(float32[4, 32000], float16[4, 32000], False) <- _to...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.29 |         0.03 | div_(float32[4, 32000], float16[4, 1])                      
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |         2.05 |         0.02 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.63 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         3.71 |         0.03 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.66 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        10.24 |         0.08 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         9.86 |         0.08 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         9.63 |         0.08 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         9.95 |         0.08 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.80 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.28 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         2.78 |         0.02 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.60 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         7.84 |         0.06 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |         3.55 |         0.03 | sort(float32[4, 32000], False, -1, False) <- sort(float32...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.73 |         0.01 | copy_(float32[4, 32000], float32[4, 32000], False) <- sor...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.79 |         0.01 | copy_(int64[4], int32[4], False) <- _to_copy(int32[4], 4,...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.31 |         0.01 | sub(int64[], int64[4], 1) <- rsub(int64[4], 32000, 1)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.27 |         0.02 | gather(float32[4, 32000], 1, int64[4, 1], False)            
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.17 |         0.02 | lt(float32[4, 32000], float32[4, 1])                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.67 |         0.01 | masked_fill_(float32[4, 32000], bool[4, 32000], -inf)       
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.62 |         0.08 | _softmax(float32[4, 32000], -1, False) <- softmax(float32...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        46.59 |         0.37 | cumsum(float32[4, 32000], -1, None)                         
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.63 |         0.01 | sub(int64[], float16[4, 1], 1) <- rsub(float16[4, 1], 1, 1) 
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.45 |         0.03 | le(float32[4, 32000], float16[4, 1])                        
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.57 |         0.01 | fill_(bool[4], bool[])                                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.82 |         0.01 | masked_fill_(float32[4, 32000], bool[4, 32000], -inf)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         5.31 |         0.04 | scatter_(float32[4, 32000], -1, int64[4, 32000], float32[...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        16.16 |         0.13 | _softmax(float32[4, 32000], -1, False) <- softmax(float32...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |         8.99 |         0.07 | _log_softmax(float32[4, 32000], -1, False) <- log_softmax...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.82 |         0.01 | copy_(int64[4], int32[4], False) <- _to_copy(int32[4], 4,...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         4.61 |         0.04 | index(float32[4, 32000], None)                              
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         3.01 |         0.02 | exponential_(float32[4, 32000], 1.0, None)                  
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.40 |         0.02 | div_(float32[4, 32000], float32[4, 32000])                  
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        11.10 |         0.09 | argmax(float32[4, 32000], 1, False)                         
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.24 |         0.02 | copy_(int64[4, 1], int64[4, 1], False) <- _to_copy(int64[...

================================================================================
= Prefill Summary Table (prompt_len=250, batch_size=4)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
LlamaForCausalLM                                                                 |     25761.88 |        98.37 |            1.00
|- LlamaModel                                                                    |     25761.88 |        98.37 |            1.00
|-- VocabParallelEmbedding(weight=float16[32000, 4096])                          |        29.98 |         0.11 |            1.00
|--- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half,... |        29.98 |         0.11 |            1.00
|-- LlamaDecoderLayer                                                            |     25720.15 |        98.21 |           32.00
|--- RMSNorm(weight=float16[4096])                                               |       755.93 |         2.89 |           64.00
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10... |        13.79 |         0.05 |            1.00
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void... |       742.14 |         2.83 |           63.00
|--- LlamaAttention                                                              |      9631.22 |        36.78 |           32.00
|---- QKVParallelLinear(weight=float16[12288, 4096])                             |      5073.14 |        19.37 |           32.00
|----- Memset (Device)                                                           |        27.14 |         0.10 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |      5046.01 |        19.27 |           32.00
|---- RotaryEmbedding                                                            |       837.95 |         3.20 |           32.00
|----- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::... |       837.95 |         3.20 |           32.00
|---- Attention                                                                  |      1779.84 |         6.80 |           32.00
|----- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned sho... |       748.38 |         2.86 |           32.00
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128, 128, 64, 4, false, ... |      1031.45 |         3.94 |           32.00
|---- RowParallelLinear(weight=float16[4096, 4096])                              |      1940.29 |         7.41 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x256x64_warpgroupsize... |      1940.29 |         7.41 |           32.00
|--- LlamaMLP                                                                    |     15333.00 |        58.55 |           32.00
|---- MergedColumnParallelLinear(weight=float16[22016, 4096])                    |      9366.35 |        35.77 |           32.00
|----- Memset (Device)                                                           |        68.52 |         0.26 |           64.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |      9297.84 |        35.50 |           32.00
|---- SiluAndMul                                                                 |      1670.91 |         6.38 |           32.00
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel... |      1670.91 |         6.38 |           32.00
|---- RowParallelLinear(weight=float16[4096, 11008])                             |      4295.74 |        16.40 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x256x64_warpgroupsize... |      4295.74 |        16.40 |           32.00
|-- RMSNorm(weight=float16[4096])                                                |        11.74 |         0.04 |            1.00
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>... |        11.74 |         0.04 |            1.00
LogitsProcessor                                                                  |       192.70 |         0.74 |            1.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         4.03 |         0.02 |            1.00
|- Memset (Device)                                                               |         0.77 |         0.00 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |       187.90 |         0.72 |            1.00
Sampler                                                                          |       233.50 |         0.89 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.59 |         0.06 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.13 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.32 |         0.02 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |         2.02 |         0.01 |            1.00
|- Memset (Device)                                                               |        12.00 |         0.05 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         3.71 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.76 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        39.94 |         0.15 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         2.78 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.44 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |         7.87 |         0.03 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |         3.58 |         0.01 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         1.86 |         0.01 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.65 |         0.01 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.31 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.56 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         3.17 |         0.01 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         3.46 |         0.01 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        26.94 |         0.10 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        46.62 |         0.18 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.66 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.38 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.60 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         5.31 |         0.02 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |         9.05 |         0.03 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         4.61 |         0.02 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         2.98 |         0.01 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         2.43 |         0.01 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        11.14 |         0.04 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.62 |         0.01 |            1.00

================================================================================
= First Decode Step Summary Table (prompt_len=250, batch_size=4)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
CUDAGraphRunner                                                                  |     12313.54 |        96.75 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         6.75 |         0.05 |            5.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<... |         2.43 |         0.02 |            2.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         4.00 |         0.03 |            1.00
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10::H... |         4.22 |         0.03 |            1.00
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x... |      4925.44 |        38.70 |           64.00
|- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::Half... |       137.22 |         1.08 |           32.00
|- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned short, ... |       120.83 |         0.95 |           32.00
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<128, 64, 128, 4, fal... |       505.82 |         3.97 |           32.00
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel_traits<128, 64, 128... |       113.66 |         0.89 |           32.00
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>::... |       186.37 |         1.46 |           64.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |      3969.02 |        31.19 |           32.00
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel<c10... |       221.15 |         1.74 |           32.00
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f16_128x64_64x6_tn_ali... |      2054.14 |        16.14 |           32.00
|- void splitKreduce_kernel<32, 16, int, float, __half, float, __half, true, ... |        62.46 |         0.49 |           32.00
LogitsProcessor                                                                  |       181.79 |         1.43 |            1.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         3.81 |         0.03 |            1.00
|- Memset (Device)                                                               |         0.96 |         0.01 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |       177.02 |         1.39 |            1.00
Sampler                                                                          |       231.55 |         1.82 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.21 |         0.11 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.06 |         0.03 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.29 |         0.03 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |         2.05 |         0.02 |            1.00
|- Memset (Device)                                                               |        11.84 |         0.09 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         3.71 |         0.03 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.66 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        39.68 |         0.31 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         2.78 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.60 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |         7.84 |         0.06 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |         3.55 |         0.03 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         1.73 |         0.01 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.62 |         0.03 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.31 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.27 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         3.17 |         0.02 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         3.49 |         0.03 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        26.78 |         0.21 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        46.59 |         0.37 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.63 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.45 |         0.03 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.57 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         5.31 |         0.04 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |         8.99 |         0.07 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         4.61 |         0.04 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         3.01 |         0.02 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         2.40 |         0.02 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        11.10 |         0.09 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.24 |         0.02 |            1.00
Traces saved as prefill.json and decode_1.json, etc. in folder benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_4/chrome_traces
