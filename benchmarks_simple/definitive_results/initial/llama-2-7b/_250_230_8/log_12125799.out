Run profile with:
  engine_args = {'model': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'served_model_name': None, 'tokenizer': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'skip_tokenizer_init': False, 'tokenizer_mode': 'auto', 'trust_remote_code': False, 'download_dir': None, 'load_format': 'auto', 'config_format': <ConfigFormat.AUTO: 'auto'>, 'dtype': 'auto', 'kv_cache_dtype': 'auto', 'quantization_param_path': None, 'seed': 0, 'max_model_len': None, 'worker_use_ray': False, 'distributed_executor_backend': None, 'pipeline_parallel_size': 1, 'tensor_parallel_size': 1, 'max_parallel_loading_workers': None, 'block_size': 16, 'enable_prefix_caching': False, 'disable_sliding_window': False, 'use_v2_block_manager': False, 'swap_space': 4, 'cpu_offload_gb': 0, 'gpu_memory_utilization': 0.9, 'max_num_batched_tokens': 256000, 'max_num_seqs': 1024, 'max_logprobs': 20, 'disable_log_stats': False, 'revision': None, 'code_revision': None, 'rope_scaling': None, 'rope_theta': None, 'tokenizer_revision': None, 'quantization': None, 'enforce_eager': False, 'max_context_len_to_capture': None, 'max_seq_len_to_capture': 8192, 'disable_custom_all_reduce': False, 'tokenizer_pool_size': 0, 'tokenizer_pool_type': 'ray', 'tokenizer_pool_extra_config': None, 'limit_mm_per_prompt': None, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'enable_prompt_adapter': False, 'max_prompt_adapters': 1, 'max_prompt_adapter_token': 0, 'fully_sharded_loras': False, 'lora_extra_vocab_size': 256, 'long_lora_scaling_factors': None, 'lora_dtype': 'auto', 'max_cpu_loras': None, 'device': 'auto', 'num_scheduler_steps': 1, 'multi_step_stream_outputs': True, 'ray_workers_use_nsight': False, 'num_gpu_blocks_override': None, 'num_lookahead_slots': 0, 'model_loader_extra_config': None, 'ignore_patterns': [], 'preemption_mode': None, 'scheduler_delay_factor': 0.0, 'enable_chunked_prefill': None, 'guided_decoding_backend': 'outlines', 'speculative_model': None, 'speculative_model_quantization': None, 'speculative_draft_tensor_parallel_size': None, 'num_speculative_tokens': None, 'speculative_disable_mqa_scorer': False, 'speculative_max_model_len': None, 'speculative_disable_by_batch_size': None, 'ngram_prompt_lookup_max': None, 'ngram_prompt_lookup_min': None, 'spec_decoding_acceptance_method': 'rejection_sampler', 'typical_acceptance_sampler_posterior_threshold': None, 'typical_acceptance_sampler_posterior_alpha': None, 'qlora_adapter_name_or_path': None, 'disable_logprobs_during_spec_decoding': None, 'otlp_traces_endpoint': None, 'collect_detailed_traces': None, 'disable_async_output_proc': False, 'override_neuron_config': None, 'mm_processor_kwargs': None, 'scheduling_policy': 'fcfs'}
  prompt_len = 250
  output_len = 230
  batch_size = 8
  result_dir = benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_8
  save_chrome_traces_folder = chrome_traces
WARNING 11-22 16:42:37 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 11-22 16:42:37 llm_engine.py:237] Initializing an LLM engine (v0.5.2.dev1210+g6f078ea3c) with config: model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', speculative_config=None, tokenizer='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 11-22 16:42:38 model_runner.py:1056] Starting to load model /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b...
INFO 11-22 16:42:52 model_runner.py:1067] Loading model weights took 12.5523 GB
INFO 11-22 16:42:59 worker.py:262] Memory profiling results: total_gpu_memory=63.43GiB initial_memory_usage=13.15GiB peak_torch_memory=36.15GiB non_torch_memory=0.10GiB kv_cache_size=20.84GiB gpu_memory_utilization=0.90
INFO 11-22 16:43:00 gpu_executor.py:122] # GPU blocks: 2667, # CPU blocks: 512
INFO 11-22 16:43:00 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 10.42x
INFO 11-22 16:43:01 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 16:43:01 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 16:43:33 model_runner.py:1523] Graph capturing finished in 32 secs.
llm.llm_engine.model_config.max_model_len:  4096
Warm up run ...
Profile run ...
INFO 11-22 16:43:38 metrics.py:349] Avg prompt throughput: 785.3 tokens/s, Avg generation throughput: 42.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:43 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:48 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:53 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 53.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.2%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:58 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%.
INFO 11-22 16:44:03 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%.
================================================================================
= Prefill Model Table (prompt_len=250, batch_size=8)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
LlamaForCausalLM                                             |     26603.56 |     50275.78 |        99.12 |                                                             
|- LlamaModel                                                |     26577.56 |     50275.78 |        99.12 |                                                             
|-- VocabParallelEmbedding(weight=float16[32000, 4096])      |        82.40 |        60.19 |         0.12 |                                                             
|--- void at::native::(anonymous namespace)::indexSelectL... |         0.00 |        60.19 |         0.12 | index_select(float16[32000, 4096], 0, int64[2000]) <- emb...
|-- LlamaDecoderLayer                                        |      3034.55 |      1567.19 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        78.45 |        30.94 |         0.06 |                                                             
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c... |         0.00 |        30.94 |         0.06 | _C::rms_norm(float16[2000, 4096], float16[2000, 4096], fl...
|--- LlamaAttention                                          |      2702.55 |       593.18 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |       112.64 |       314.30 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       313.53 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        71.05 |        54.05 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        54.05 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |      2349.65 |       117.86 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        52.96 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.90 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        97.01 |       106.98 |         0.21 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       106.98 |         0.21 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        42.36 |        33.63 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.63 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       176.85 |       909.43 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        61.09 |       549.76 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       548.96 |         1.08 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        48.84 |       101.66 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.66 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        44.15 |       258.01 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       258.01 |         0.51 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       840.06 |      1575.45 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        23.04 |        35.30 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        35.30 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       639.00 |       589.73 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        45.13 |       314.11 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       313.31 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        47.61 |        54.14 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        54.14 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       447.15 |       117.98 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.18 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.80 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        53.20 |       103.49 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.49 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        24.74 |        34.53 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.53 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       129.18 |       915.90 |         1.81 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        45.24 |       557.82 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       556.83 |         1.10 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.54 |       101.22 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.22 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.21 |       256.86 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       256.86 |         0.51 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       762.15 |      1567.22 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.02 |        34.34 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.34 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       564.11 |       587.71 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        40.80 |       312.77 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       312.00 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        43.33 |        53.15 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.15 |         0.10 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       388.53 |       117.82 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.18 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.64 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        49.97 |       103.97 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.97 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        24.03 |        34.62 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.62 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       126.71 |       910.56 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        44.44 |       552.29 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       551.33 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.15 |       102.02 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       102.02 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.71 |       256.25 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       256.25 |         0.51 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       735.83 |      1571.29 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        24.74 |        33.95 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.95 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       528.12 |       587.74 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.70 |       312.61 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       311.65 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.24 |        53.66 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.66 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       362.88 |       117.28 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.28 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.00 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        50.45 |       104.19 |         0.21 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       104.19 |         0.21 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        24.65 |        33.63 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.63 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       139.60 |       915.96 |         1.81 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        50.76 |       557.09 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       556.06 |         1.10 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.75 |       101.44 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.44 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        42.00 |       257.44 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       257.44 |         0.51 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       945.23 |      1568.02 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        21.09 |        33.76 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.76 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       732.09 |       592.99 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.82 |       318.05 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       317.28 |         0.63 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        42.46 |        53.41 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.41 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       538.10 |       118.37 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.41 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.96 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        68.27 |       103.17 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.17 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        30.97 |        33.44 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.44 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       140.79 |       907.84 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        51.08 |       554.40 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       553.63 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        35.77 |       100.89 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       100.89 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.03 |       252.54 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       252.54 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       851.51 |      1569.30 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        26.71 |        33.31 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.31 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       651.65 |       589.79 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        41.63 |       312.38 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       311.45 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        42.26 |        54.11 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        54.11 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       421.86 |       119.07 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.41 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        65.66 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        86.19 |       104.22 |         0.21 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       104.22 |         0.21 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.39 |        33.09 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.09 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       130.22 |       913.12 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        45.01 |       557.34 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       556.38 |         1.10 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.11 |       101.47 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.47 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.22 |       254.30 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       254.30 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       741.03 |      1563.03 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.93 |        34.53 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.53 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       553.80 |       587.07 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        43.11 |       312.41 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       311.61 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        39.67 |        53.34 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.34 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       382.03 |       118.50 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.05 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        65.44 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        46.90 |       102.82 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       102.82 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.61 |        33.76 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.76 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       129.64 |       907.68 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.65 |       550.91 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       549.98 |         1.08 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.20 |       101.47 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.47 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        41.84 |       255.29 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       255.29 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       713.64 |      1571.35 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.05 |        33.54 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.54 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       530.83 |       588.19 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        40.91 |       311.74 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       310.94 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        39.84 |        53.98 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.98 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       368.48 |       118.08 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        52.96 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        65.12 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        46.02 |       104.38 |         0.21 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       104.38 |         0.21 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.39 |        34.17 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.17 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       125.90 |       915.45 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.70 |       555.90 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       554.94 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.52 |       101.57 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.57 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        41.19 |       257.98 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       257.98 |         0.51 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       738.11 |      1566.39 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.85 |        34.53 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.53 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       547.77 |       591.84 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.30 |       316.35 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       315.55 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        40.57 |        53.89 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.89 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       383.35 |       118.24 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.38 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.86 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        47.27 |       103.36 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.36 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.47 |        33.34 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.34 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       131.91 |       906.68 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.80 |       551.29 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       550.53 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        37.04 |       102.05 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       102.05 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.87 |       253.34 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       253.34 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       728.55 |      1571.61 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.14 |        33.63 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.63 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       549.35 |       592.00 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.14 |       315.58 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       314.78 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        40.60 |        54.30 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        54.30 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       384.38 |       118.46 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.15 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        65.31 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        46.13 |       103.65 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.65 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.24 |        33.50 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.50 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       123.94 |       912.48 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.16 |       555.64 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       554.72 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.59 |       101.44 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.44 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.52 |       255.39 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       255.39 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       712.36 |      1567.67 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        17.61 |        34.30 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.30 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       524.02 |       587.07 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.19 |       312.06 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       311.29 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        41.72 |        53.09 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.09 |         0.10 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       356.97 |       118.59 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.22 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        65.38 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        50.19 |       103.33 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.33 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        24.14 |        34.56 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.56 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       127.94 |       911.74 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        44.13 |       554.56 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       553.60 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.55 |       101.76 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.76 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.60 |       255.42 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       255.42 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       744.55 |      1568.79 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.65 |        33.92 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.92 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       563.88 |       589.28 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        40.46 |       312.57 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       311.81 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        40.15 |        53.44 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.44 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       374.22 |       119.87 |         0.24 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.28 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        66.59 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        47.63 |       103.39 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.39 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.20 |        34.72 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.72 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       121.46 |       910.88 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.19 |       554.85 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       554.08 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.51 |       101.44 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.44 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        35.32 |       254.59 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       254.59 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       696.02 |      1565.31 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.64 |        33.86 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.86 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       512.21 |       589.21 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.45 |       313.31 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       312.32 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.42 |        53.15 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.15 |         0.10 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       355.31 |       118.82 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.15 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        65.66 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.05 |       103.94 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.94 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.16 |        33.79 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.79 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       126.72 |       908.44 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        46.96 |       553.40 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       552.64 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.09 |       101.02 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.02 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.84 |       254.01 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       254.01 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       791.25 |      1571.00 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        23.06 |        33.98 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.98 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       553.88 |       588.25 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.58 |       311.13 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       310.37 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        40.19 |        53.28 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.28 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       384.24 |       120.45 |         0.24 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.12 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        67.33 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        53.19 |       103.39 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.39 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.12 |        33.63 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.63 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       170.24 |       915.13 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        85.15 |       558.27 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       557.50 |         1.10 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.38 |       101.34 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.34 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.20 |       255.52 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       255.52 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       729.61 |      1568.09 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.36 |        34.72 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.72 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       544.42 |       587.80 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.23 |       311.93 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       310.94 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.80 |        53.57 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.57 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       378.55 |       118.18 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.28 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.90 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        47.95 |       104.13 |         0.21 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       104.13 |         0.21 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.82 |        34.62 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.62 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       125.32 |       910.94 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.78 |       553.92 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       552.99 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.10 |       101.50 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.50 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.07 |       255.52 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       255.52 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       705.02 |      1563.29 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.03 |        34.27 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.27 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       521.18 |       585.92 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        42.19 |       309.95 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       309.18 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.20 |        53.89 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.89 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       357.89 |       118.11 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.34 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.77 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.74 |       103.97 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.97 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.56 |        33.83 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.83 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       126.49 |       909.28 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.48 |       554.30 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       553.53 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        33.12 |       101.89 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.89 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.23 |       253.09 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       253.09 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       722.69 |      1563.89 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.13 |        34.62 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.62 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       538.25 |       587.29 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.70 |       310.88 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       309.89 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.72 |        54.24 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        54.24 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       376.82 |       118.53 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.18 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        65.34 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        46.36 |       103.65 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.65 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.10 |        34.14 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.14 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       123.28 |       907.83 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.88 |       552.54 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       551.49 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.32 |       101.66 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.66 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.75 |       253.63 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       253.63 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       710.73 |      1565.85 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        22.98 |        34.21 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.21 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       529.30 |       587.61 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.74 |       312.86 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       312.09 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.69 |        53.54 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.54 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       370.21 |       118.17 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.31 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.86 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.56 |       103.04 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.04 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.57 |        33.79 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.79 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       120.55 |       910.24 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.28 |       555.26 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       554.46 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.48 |       101.54 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.54 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.81 |       253.44 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       253.44 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       729.67 |      1568.18 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.21 |        34.21 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.21 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       540.99 |       593.05 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.98 |       318.21 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       317.25 |         0.63 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        35.26 |        53.06 |         0.10 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.06 |         0.10 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       376.09 |       118.18 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.12 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        65.06 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        52.57 |       103.61 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.61 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.53 |        34.40 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.40 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       129.06 |       906.52 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        44.91 |       552.45 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       551.68 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.18 |       101.54 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.54 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.26 |       252.54 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       252.54 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       754.26 |      1568.44 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.57 |        34.34 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.34 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       564.50 |       587.36 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.78 |       312.00 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       311.23 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.53 |        53.98 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.98 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       393.38 |       117.44 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.25 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.19 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        54.81 |       103.94 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.94 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.69 |        33.47 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.47 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       130.47 |       913.27 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.44 |       558.85 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       558.08 |         1.10 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        35.28 |       101.73 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.73 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.58 |       252.70 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       252.70 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       725.00 |      1565.08 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.71 |        34.62 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.62 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       541.66 |       588.06 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.40 |       312.19 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       311.23 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.48 |        53.89 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.89 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       381.62 |       117.69 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.15 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.54 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        46.01 |       104.29 |         0.21 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       104.29 |         0.21 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.18 |        34.30 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.30 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       125.74 |       908.09 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        46.53 |       551.36 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       550.40 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.75 |       101.70 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.70 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.08 |       255.04 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       255.04 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       754.72 |      1572.31 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.21 |        34.11 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.11 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       525.23 |       590.59 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        43.39 |       314.85 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       314.08 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.33 |        53.73 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.73 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       355.07 |       117.86 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.22 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.64 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.94 |       104.16 |         0.21 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       104.16 |         0.21 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        30.57 |        34.40 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.40 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       163.84 |       913.21 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.23 |       556.83 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       556.06 |         1.10 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.64 |       101.38 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.38 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        76.26 |       255.01 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       255.01 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       779.51 |      1574.17 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.24 |        34.27 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.27 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       547.94 |       595.39 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.31 |       319.61 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       318.69 |         0.63 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.07 |        53.73 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.73 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       381.61 |       118.62 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.85 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.77 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        49.22 |       103.42 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.42 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.60 |        33.73 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.73 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       173.59 |       910.78 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.41 |       553.53 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       552.77 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.80 |       101.82 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.82 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.97 |       255.42 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       255.42 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       696.25 |      1565.66 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        17.59 |        34.14 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.14 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       520.60 |       587.17 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.79 |       311.84 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       311.04 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.57 |        53.50 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.50 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       357.82 |       118.59 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.25 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        65.34 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        50.74 |       103.23 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.23 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.16 |        34.14 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.14 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       121.42 |       910.20 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.55 |       554.08 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       553.28 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.88 |       101.28 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.28 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.68 |       254.85 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       254.85 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       717.86 |      1564.31 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        17.18 |        33.25 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.25 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       529.29 |       590.11 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.16 |       313.92 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       312.96 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        35.34 |        53.95 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.95 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       367.50 |       118.59 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.28 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        65.31 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        51.66 |       103.65 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.65 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        24.13 |        34.02 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.02 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       129.07 |       906.94 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        44.24 |       551.23 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       550.27 |         1.08 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.69 |       101.73 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.73 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.10 |       253.98 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       253.98 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       714.32 |      1573.24 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.89 |        34.40 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.40 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       528.54 |       591.90 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        44.71 |       315.36 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       314.40 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        44.29 |        53.86 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.86 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       353.06 |       119.39 |         0.24 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.22 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        66.17 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        47.14 |       103.30 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.30 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.55 |        33.82 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.82 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       127.11 |       913.12 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.85 |       558.49 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       557.69 |         1.10 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.20 |       101.70 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.70 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.70 |       252.93 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       252.93 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       695.38 |      1561.66 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        22.18 |        34.11 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.11 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       511.58 |       585.73 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.08 |       311.74 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       310.94 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.78 |        53.73 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.73 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       352.92 |       117.89 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.18 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.70 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.76 |       102.37 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       102.37 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.34 |        33.76 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.76 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       122.72 |       908.06 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.58 |       552.48 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       551.68 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.50 |       101.47 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.47 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        35.39 |       254.11 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       254.11 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       710.82 |      1566.01 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        17.92 |        33.34 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.34 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       517.02 |       588.25 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.45 |       311.49 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.12 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       310.37 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.30 |        53.70 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.70 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       354.07 |       117.95 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.22 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.74 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        50.32 |       105.12 |         0.21 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       105.12 |         0.21 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        24.16 |        33.89 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.89 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       133.32 |       910.52 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.38 |       555.07 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       554.11 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        36.71 |       101.47 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.47 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.03 |       253.98 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       253.98 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       703.95 |      1572.25 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.06 |        34.37 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.37 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       521.03 |       593.82 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.24 |       318.27 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       317.47 |         0.63 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.66 |        53.47 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.47 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       359.68 |       117.73 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.41 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.32 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.66 |       104.35 |         0.21 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       104.35 |         0.21 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.11 |        34.34 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.34 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       123.62 |       909.72 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.16 |       555.64 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       554.88 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.43 |       101.47 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.47 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.85 |       252.61 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       252.61 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       696.00 |      1570.11 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        17.93 |        33.54 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.54 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       519.84 |       589.57 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.98 |       313.38 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       312.42 |         0.62 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.13 |        53.41 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.41 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       347.83 |       118.24 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.34 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.90 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.37 |       104.54 |         0.21 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       104.54 |         0.21 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.19 |        33.79 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.79 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       120.77 |       913.21 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.15 |       555.93 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       555.13 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.37 |       101.47 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.47 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        35.46 |       255.81 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       255.81 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       784.95 |      1562.91 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.59 |        33.44 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.44 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       587.76 |       586.59 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.47 |       310.66 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       309.86 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        35.47 |        54.30 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        54.30 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       377.17 |       118.02 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        53.34 |         0.11 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.67 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        56.37 |       103.61 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.61 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.17 |        34.18 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.18 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       135.13 |       908.70 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        45.67 |       552.35 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       551.58 |         1.09 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.63 |       101.79 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       101.79 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        42.18 |       254.56 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       254.56 |         0.50 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       726.69 |      1572.38 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.03 |        33.98 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        33.98 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaAttention                                          |       536.34 |       586.75 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.97 |       311.23 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       310.27 |         0.61 | mm(float16[2000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.89 |        53.86 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        53.86 |         0.11 | _C::rotary_embedding(int64[2000], float16[2000, 4096], fl...
|---- Attention                                              |       370.35 |       117.89 |         0.23 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |        52.99 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[2000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        64.90 |         0.13 | vllm_flash_attn_c::varlen_fwd(float16[2000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        47.21 |       103.78 |         0.20 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       103.78 |         0.20 | mm(float16[2000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.39 |        34.14 |         0.07 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        34.14 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
|--- LlamaMLP                                                |       130.74 |       917.50 |         1.81 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.22 |       559.77 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       558.78 |         1.10 | mm(float16[2000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.83 |       100.93 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       100.93 |         0.20 | _C::silu_and_mul(float16[2000, 11008], float16[2000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        41.42 |       256.80 |         0.51 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x... |         0.00 |       256.80 |         0.51 | mm(float16[2000, 11008], float16[11008, 4096]) <- matmul(...
|-- RMSNorm(weight=float16[4096])                            |        18.81 |        34.14 |         0.07 |                                                             
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::... |         0.00 |        34.14 |         0.07 | _C::fused_add_rms_norm(float16[2000, 4096], float16[2000,...
LogitsProcessor                                              |       132.30 |       199.68 |         0.39 |                                                             
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         6.18 |         0.01 | index_select(float16[2000, 4096], 0, int64[8])              
|- Memset (Device)                                           |         0.00 |         0.99 |         0.00 | mm(float16[8, 4096], float16[4096, 32000]) <- matmul(floa...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       192.51 |         0.38 | mm(float16[8, 4096], float16[4096, 32000]) <- matmul(floa...
Sampler                                                      |     26889.79 |       247.16 |         0.49 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.34 |         0.00 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.00 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.30 |         0.00 | copy_(int32[8], int32[8], True) <- _to_copy(int32[8], 3, ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.00 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.11 |         0.00 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.00 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.00 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         4.19 |         0.01 | copy_(float32[8, 32000], float16[8, 32000], False) <- _to...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.45 |         0.01 | div_(float32[8, 32000], float16[8, 1])                      
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |         2.53 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.12 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.63 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         4.51 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.70 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        11.74 |         0.02 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        11.01 |         0.02 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.31 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        10.62 |         0.02 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        11.07 |         0.02 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.02 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.41 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         2.94 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.73 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.00 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         8.93 |         0.02 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |         5.12 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         2.14 |         0.00 | copy_(float32[8, 32000], float32[8, 32000], False) <- sor...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.86 |         0.00 | copy_(int64[8], int32[8], False) <- _to_copy(int32[8], 4,...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.54 |         0.00 | sub(int64[], int64[8], 1) <- rsub(int64[8], 32000, 1)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.46 |         0.00 | gather(float32[8, 32000], 1, int64[8, 1], False)            
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.30 |         0.01 | lt(float32[8, 32000], float32[8, 1])                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.89 |         0.00 | masked_fill_(float32[8, 32000], bool[8, 32000], -inf)       
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        11.07 |         0.02 | _softmax(float32[8, 32000], -1, False) <- softmax(float32...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        44.67 |         0.09 | cumsum(float32[8, 32000], -1, None)                         
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.50 |         0.00 | sub(int64[], float16[8, 1], 1) <- rsub(float16[8, 1], 1, 1) 
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.51 |         0.01 | le(float32[8, 32000], float16[8, 1])                        
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.86 |         0.00 | fill_(bool[8], bool[])                                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.11 |         0.00 | masked_fill_(float32[8, 32000], bool[8, 32000], -inf)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         7.58 |         0.01 | scatter_(float32[8, 32000], -1, int64[8, 32000], float32[...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        16.42 |         0.03 | _softmax(float32[8, 32000], -1, False) <- softmax(float32...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |         8.86 |         0.02 | _log_softmax(float32[8, 32000], -1, False) <- log_softmax...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.82 |         0.00 | copy_(int64[8], int32[8], False) <- _to_copy(int32[8], 4,...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         4.83 |         0.01 | index(float32[8, 32000], None)                              
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         4.45 |         0.01 | exponential_(float32[8, 32000], 1.0, None)                  
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.72 |         0.01 | div_(float32[8, 32000], float32[8, 32000])                  
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        11.07 |         0.02 | argmax(float32[8, 32000], 1, False)                         
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.69 |         0.01 | copy_(int64[8, 1], int64[8, 1], False) <- _to_copy(int64[...

================================================================================
= First Decode Step Model Table (prompt_len=250, batch_size=8)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
CUDAGraphRunner                                              |       709.97 |     12897.28 |        96.77 |                                                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.34 |         0.01 | copy_(int64[8], int64[8], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.31 |         0.01 | copy_(int64[8], int64[8], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.38 |         0.01 | copy_(int64[8], int64[8], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.41 |         0.01 | copy_(int32[8], int32[8], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.31 |         0.01 | copy_(int32[8, 256], int32[8, 256], True)                   
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.12 |         0.01 | fill_(int64[1], 0)                                          
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.25 |         0.01 | fill_(int64[1], 12)                                         
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         7.07 |         0.05 |                                                             
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10:... |         0.00 |         4.38 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.54 |         0.03 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.46 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        26.75 |         0.20 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        82.72 |         0.62 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       122.59 |         0.92 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.94 |         0.05 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        64.64 |         0.49 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.89 |         0.01 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         3.07 |         0.02 |                                                             
LogitsProcessor                                              |       271.15 |       185.02 |         1.39 |                                                             
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         5.60 |         0.04 | index_select(float16[8, 4096], 0, int64[8])                 
|- Memset (Device)                                           |         0.00 |         0.77 |         0.01 | mm(float16[8, 4096], float16[4096, 32000]) <- matmul(floa...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       178.66 |         1.34 | mm(float16[8, 4096], float16[4096, 32000]) <- matmul(floa...
Sampler                                                      |      9522.17 |       245.37 |         1.84 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.11 |         0.02 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.01 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.02 | copy_(int32[8], int32[8], True) <- _to_copy(int32[8], 3, ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.14 |         0.02 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.11 |         0.02 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.02 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.11 |         0.02 | copy_(float16[8], float16[8], True) <- _to_copy(float16[8...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         4.06 |         0.03 | copy_(float32[8, 32000], float16[8, 32000], False) <- _to...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.42 |         0.03 | div_(float32[8, 32000], float16[8, 1])                      
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |         2.46 |         0.02 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.77 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         4.32 |         0.03 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         2.11 |         0.02 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        11.49 |         0.09 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        10.72 |         0.08 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.34 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        11.14 |         0.08 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        10.50 |         0.08 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.99 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.57 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         3.04 |         0.02 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.57 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         9.22 |         0.07 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |         5.12 |         0.04 | sort(float32[8, 32000], False, -1, False) <- sort(float32...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         2.05 |         0.02 | copy_(float32[8, 32000], float32[8, 32000], False) <- sor...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.76 |         0.01 | copy_(int64[8], int32[8], False) <- _to_copy(int32[8], 4,...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.47 |         0.01 | sub(int64[], int64[8], 1) <- rsub(int64[8], 32000, 1)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.56 |         0.02 | gather(float32[8, 32000], 1, int64[8, 1], False)            
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.33 |         0.02 | lt(float32[8, 32000], float32[8, 1])                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.89 |         0.01 | masked_fill_(float32[8, 32000], bool[8, 32000], -inf)       
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        11.10 |         0.08 | _softmax(float32[8, 32000], -1, False) <- softmax(float32...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        44.61 |         0.33 | cumsum(float32[8, 32000], -1, None)                         
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.54 |         0.01 | sub(int64[], float16[8, 1], 1) <- rsub(float16[8, 1], 1, 1) 
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.64 |         0.03 | le(float32[8, 32000], float16[8, 1])                        
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.82 |         0.01 | fill_(bool[8], bool[])                                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.92 |         0.01 | masked_fill_(float32[8, 32000], bool[8, 32000], -inf)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         7.55 |         0.06 | scatter_(float32[8, 32000], -1, int64[8, 32000], float32[...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        16.03 |         0.12 | _softmax(float32[8, 32000], -1, False) <- softmax(float32...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |         8.74 |         0.07 | _log_softmax(float32[8, 32000], -1, False) <- log_softmax...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.85 |         0.01 | copy_(int64[8], int32[8], False) <- _to_copy(int32[8], 4,...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         4.83 |         0.04 | index(float32[8, 32000], None)                              
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         4.45 |         0.03 | exponential_(float32[8, 32000], 1.0, None)                  
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.72 |         0.02 | div_(float32[8, 32000], float32[8, 32000])                  
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        11.65 |         0.09 | argmax(float32[8, 32000], 1, False)                         
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.14 |         0.02 | copy_(int64[8, 1], int64[8, 1], False) <- _to_copy(int64[...

================================================================================
= Prefill Summary Table (prompt_len=250, batch_size=8)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
LlamaForCausalLM                                                                 |     50275.78 |        99.12 |            1.00
|- LlamaModel                                                                    |     50275.78 |        99.12 |            1.00
|-- VocabParallelEmbedding(weight=float16[32000, 4096])                          |        60.19 |         0.12 |            1.00
|--- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half,... |        60.19 |         0.12 |            1.00
|-- LlamaDecoderLayer                                                            |     50181.45 |        98.93 |           32.00
|--- RMSNorm(weight=float16[4096])                                               |      2175.04 |         4.29 |           64.00
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10... |        30.94 |         0.06 |            1.00
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void... |      2144.09 |         4.23 |           63.00
|--- LlamaAttention                                                              |     18856.99 |        37.18 |           32.00
|---- QKVParallelLinear(weight=float16[12288, 4096])                             |     10029.74 |        19.77 |           32.00
|----- Memset (Device)                                                           |        27.55 |         0.05 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |     10002.19 |        19.72 |           32.00
|---- RotaryEmbedding                                                            |      1718.39 |         3.39 |           32.00
|----- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::... |      1718.39 |         3.39 |           32.00
|---- Attention                                                                  |      3786.44 |         7.46 |           32.00
|----- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned sho... |      1703.67 |         3.36 |           32.00
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128, 128, 64, 4, false, ... |      2082.77 |         4.11 |           32.00
|---- RowParallelLinear(weight=float16[4096, 4096])                              |      3322.42 |         6.55 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x128x64_warpgroupsize... |      3322.42 |         6.55 |           32.00
|--- LlamaMLP                                                                    |     29149.42 |        57.47 |           32.00
|---- MergedColumnParallelLinear(weight=float16[22016, 4096])                    |     17747.59 |        34.99 |           32.00
|----- Memset (Device)                                                           |        27.59 |         0.05 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |     17720.01 |        34.94 |           32.00
|---- SiluAndMul                                                                 |      3248.85 |         6.41 |           32.00
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel... |      3248.85 |         6.41 |           32.00
|---- RowParallelLinear(weight=float16[4096, 11008])                             |      8152.98 |        16.07 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize256x128x64_warpgroupsize... |      8152.98 |        16.07 |           32.00
|-- RMSNorm(weight=float16[4096])                                                |        34.14 |         0.07 |            1.00
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>... |        34.14 |         0.07 |            1.00
LogitsProcessor                                                                  |       199.68 |         0.39 |            1.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         6.18 |         0.01 |            1.00
|- Memset (Device)                                                               |         0.99 |         0.00 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |       192.51 |         0.38 |            1.00
Sampler                                                                          |       247.16 |         0.49 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.72 |         0.03 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.19 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.45 |         0.01 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |         2.53 |         0.00 |            1.00
|- Memset (Device)                                                               |        12.54 |         0.02 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         4.51 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.70 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        44.45 |         0.09 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         2.94 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.73 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |         8.93 |         0.02 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |         5.12 |         0.01 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         2.14 |         0.00 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.68 |         0.01 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.54 |         0.00 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.46 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         3.30 |         0.01 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         4.00 |         0.01 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        27.49 |         0.05 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        44.67 |         0.09 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.50 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.51 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.86 |         0.00 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         7.58 |         0.01 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |         8.86 |         0.02 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         4.83 |         0.01 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         4.45 |         0.01 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         2.72 |         0.01 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        11.07 |         0.02 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.69 |         0.01 |            1.00

================================================================================
= First Decode Step Summary Table (prompt_len=250, batch_size=8)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
CUDAGraphRunner                                                                  |     12897.28 |        96.77 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         6.75 |         0.05 |            5.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<... |         2.37 |         0.02 |            2.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         7.07 |         0.05 |            1.00
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10::H... |         4.38 |         0.03 |            1.00
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x... |      5294.02 |        39.72 |           64.00
|- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::Half... |       145.44 |         1.09 |           32.00
|- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned short, ... |       110.59 |         0.83 |           32.00
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<128, 64, 128, 4, fal... |       856.03 |         6.42 |           32.00
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>::... |       196.61 |         1.48 |           64.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |      3922.94 |        29.43 |           32.00
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel<c10... |       222.18 |         1.67 |           32.00
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f16_128x64_64x6_tn_ali... |      2068.48 |        15.52 |           32.00
|- void splitKreduce_kernel<32, 16, int, float, __half, float, __half, true, ... |        60.42 |         0.45 |           32.00
LogitsProcessor                                                                  |       185.02 |         1.39 |            1.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         5.60 |         0.04 |            1.00
|- Memset (Device)                                                               |         0.77 |         0.01 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |       178.66 |         1.34 |            1.00
Sampler                                                                          |       245.37 |         1.84 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.50 |         0.11 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.06 |         0.03 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.42 |         0.03 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |         2.46 |         0.02 |            1.00
|- Memset (Device)                                                               |        12.06 |         0.09 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         4.32 |         0.03 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         2.11 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        43.84 |         0.33 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         3.04 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.57 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |         9.22 |         0.07 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |         5.12 |         0.04 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         2.05 |         0.02 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.62 |         0.03 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.47 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.56 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         3.33 |         0.02 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         3.81 |         0.03 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        27.14 |         0.20 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        44.61 |         0.33 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.54 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.64 |         0.03 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.82 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         7.55 |         0.06 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |         8.74 |         0.07 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         4.83 |         0.04 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         4.45 |         0.03 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         2.72 |         0.02 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        11.65 |         0.09 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.14 |         0.02 |            1.00
Traces saved as prefill.json and decode_1.json, etc. in folder benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_8/chrome_traces
