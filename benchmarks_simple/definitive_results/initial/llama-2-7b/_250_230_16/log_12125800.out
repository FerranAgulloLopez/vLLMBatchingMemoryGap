Run profile with:
  engine_args = {'model': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'served_model_name': None, 'tokenizer': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'skip_tokenizer_init': False, 'tokenizer_mode': 'auto', 'trust_remote_code': False, 'download_dir': None, 'load_format': 'auto', 'config_format': <ConfigFormat.AUTO: 'auto'>, 'dtype': 'auto', 'kv_cache_dtype': 'auto', 'quantization_param_path': None, 'seed': 0, 'max_model_len': None, 'worker_use_ray': False, 'distributed_executor_backend': None, 'pipeline_parallel_size': 1, 'tensor_parallel_size': 1, 'max_parallel_loading_workers': None, 'block_size': 16, 'enable_prefix_caching': False, 'disable_sliding_window': False, 'use_v2_block_manager': False, 'swap_space': 4, 'cpu_offload_gb': 0, 'gpu_memory_utilization': 0.9, 'max_num_batched_tokens': 256000, 'max_num_seqs': 1024, 'max_logprobs': 20, 'disable_log_stats': False, 'revision': None, 'code_revision': None, 'rope_scaling': None, 'rope_theta': None, 'tokenizer_revision': None, 'quantization': None, 'enforce_eager': False, 'max_context_len_to_capture': None, 'max_seq_len_to_capture': 8192, 'disable_custom_all_reduce': False, 'tokenizer_pool_size': 0, 'tokenizer_pool_type': 'ray', 'tokenizer_pool_extra_config': None, 'limit_mm_per_prompt': None, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'enable_prompt_adapter': False, 'max_prompt_adapters': 1, 'max_prompt_adapter_token': 0, 'fully_sharded_loras': False, 'lora_extra_vocab_size': 256, 'long_lora_scaling_factors': None, 'lora_dtype': 'auto', 'max_cpu_loras': None, 'device': 'auto', 'num_scheduler_steps': 1, 'multi_step_stream_outputs': True, 'ray_workers_use_nsight': False, 'num_gpu_blocks_override': None, 'num_lookahead_slots': 0, 'model_loader_extra_config': None, 'ignore_patterns': [], 'preemption_mode': None, 'scheduler_delay_factor': 0.0, 'enable_chunked_prefill': None, 'guided_decoding_backend': 'outlines', 'speculative_model': None, 'speculative_model_quantization': None, 'speculative_draft_tensor_parallel_size': None, 'num_speculative_tokens': None, 'speculative_disable_mqa_scorer': False, 'speculative_max_model_len': None, 'speculative_disable_by_batch_size': None, 'ngram_prompt_lookup_max': None, 'ngram_prompt_lookup_min': None, 'spec_decoding_acceptance_method': 'rejection_sampler', 'typical_acceptance_sampler_posterior_threshold': None, 'typical_acceptance_sampler_posterior_alpha': None, 'qlora_adapter_name_or_path': None, 'disable_logprobs_during_spec_decoding': None, 'otlp_traces_endpoint': None, 'collect_detailed_traces': None, 'disable_async_output_proc': False, 'override_neuron_config': None, 'mm_processor_kwargs': None, 'scheduling_policy': 'fcfs'}
  prompt_len = 250
  output_len = 230
  batch_size = 16
  result_dir = benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_16
  save_chrome_traces_folder = chrome_traces
WARNING 11-22 16:42:45 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 11-22 16:42:45 llm_engine.py:237] Initializing an LLM engine (v0.5.2.dev1210+g6f078ea3c) with config: model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', speculative_config=None, tokenizer='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 11-22 16:42:46 model_runner.py:1056] Starting to load model /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b...
INFO 11-22 16:42:52 model_runner.py:1067] Loading model weights took 12.5523 GB
INFO 11-22 16:43:00 worker.py:262] Memory profiling results: total_gpu_memory=63.43GiB initial_memory_usage=13.15GiB peak_torch_memory=36.15GiB non_torch_memory=0.10GiB kv_cache_size=20.84GiB gpu_memory_utilization=0.90
INFO 11-22 16:43:00 gpu_executor.py:122] # GPU blocks: 2667, # CPU blocks: 512
INFO 11-22 16:43:00 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 10.42x
INFO 11-22 16:43:01 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 16:43:01 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 16:43:31 model_runner.py:1523] Graph capturing finished in 30 secs.
llm.llm_engine.model_config.max_model_len:  4096
Warm up run ...
Profile run ...
INFO 11-22 16:43:36 metrics.py:349] Avg prompt throughput: 1571.2 tokens/s, Avg generation throughput: 69.1 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:41 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.5 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:46 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:51 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.6 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.2%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:57 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.4%, CPU KV cache usage: 0.0%.
INFO 11-22 16:44:02 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.6%, CPU KV cache usage: 0.0%.
INFO 11-22 16:44:07 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.2%, CPU KV cache usage: 0.0%.
INFO 11-22 16:44:12 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 17.4%, CPU KV cache usage: 0.0%.
================================================================================
= Prefill Model Table (prompt_len=250, batch_size=16)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
LlamaForCausalLM                                             |     24573.19 |    103379.29 |        99.50 |                                                             
|- LlamaModel                                                |     24550.33 |    103379.29 |        99.50 |                                                             
|-- VocabParallelEmbedding(weight=float16[32000, 4096])      |        71.32 |       117.47 |         0.11 |                                                             
|--- void at::native::(anonymous namespace)::indexSelectL... |         0.00 |       117.47 |         0.11 | index_select(float16[32000, 4096], 0, int64[4000]) <- emb...
|-- LlamaDecoderLayer                                        |      2639.03 |      3202.30 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        73.64 |        69.70 |         0.07 |                                                             
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c... |         0.00 |        69.70 |         0.07 | _C::rms_norm(float16[4000, 4096], float16[4000, 4096], fl...
|--- LlamaAttention                                          |      2339.08 |      1212.99 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |       105.44 |       648.13 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       647.36 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        59.54 |       111.01 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.01 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |      2018.74 |       216.77 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.80 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       115.97 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        91.89 |       237.09 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       236.29 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        38.29 |        81.28 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.28 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       155.51 |      1838.33 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        47.43 |      1144.54 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1143.77 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        42.41 |       201.18 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.18 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        44.76 |       492.61 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       491.84 |         0.47 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       741.37 |      3217.27 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        21.05 |        81.57 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.57 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       563.86 |      1206.08 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.75 |       647.52 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       646.72 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        42.11 |       110.62 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.62 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       390.74 |       218.56 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.61 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.95 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        51.11 |       229.38 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       228.48 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.65 |        80.74 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.74 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       113.15 |      1848.89 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.83 |      1148.80 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1148.03 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.84 |       201.53 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.53 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        35.92 |       498.56 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       497.60 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       650.79 |      3223.07 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        17.12 |        82.11 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.11 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       473.45 |      1220.89 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.69 |       653.82 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       653.05 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        33.22 |       110.37 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.37 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       325.09 |       218.21 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.54 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.66 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.58 |       238.50 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       237.47 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.45 |        80.58 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.58 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       115.95 |      1839.48 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.57 |      1145.31 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1144.38 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        33.91 |       203.30 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       203.30 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        34.78 |       490.88 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       490.11 |         0.47 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       649.32 |      3215.19 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        17.48 |        81.34 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.34 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       482.21 |      1209.76 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        41.68 |       649.38 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       648.41 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        34.25 |       110.97 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.97 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       328.00 |       218.72 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.02 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.70 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.68 |       230.69 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       229.89 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.56 |        81.41 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.41 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       113.70 |      1842.68 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.47 |      1143.61 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1142.81 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.92 |       201.31 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.31 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.92 |       497.76 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       496.99 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       632.04 |      3225.08 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.58 |        82.66 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.66 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       475.04 |      1222.49 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.74 |       653.76 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       652.83 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.26 |       110.75 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.75 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       333.20 |       219.01 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.67 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       118.34 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.94 |       238.98 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       238.02 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.15 |        80.80 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.80 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       107.22 |      1839.13 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.25 |      1147.01 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1146.21 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.91 |       201.60 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.60 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.81 |       490.53 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       489.60 |         0.47 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       701.60 |      3215.71 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.36 |        81.89 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.89 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       526.05 |      1207.10 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.38 |       647.39 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       646.62 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        30.59 |       110.91 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.91 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       327.14 |       218.56 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.35 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       118.21 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        84.42 |       230.24 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       229.47 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.01 |        81.22 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.22 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       119.84 |      1845.50 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.60 |      1144.16 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1143.20 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        27.79 |       201.41 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.41 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        34.79 |       499.93 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       499.17 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       640.14 |      3212.60 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        20.09 |        82.72 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.72 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       476.78 |      1218.46 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.31 |       652.03 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       651.26 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.13 |       109.86 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       109.86 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       321.20 |       217.86 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.51 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.34 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.61 |       238.72 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       237.82 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.47 |        81.09 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.09 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       106.23 |      1830.33 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.89 |      1137.18 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1136.29 |         1.09 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.27 |       201.53 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.53 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.81 |       491.61 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       490.85 |         0.47 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       614.71 |      3212.38 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.43 |        82.27 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.27 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       459.34 |      1207.33 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        35.42 |       646.75 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       645.79 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.56 |       111.58 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.58 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       311.82 |       217.98 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.58 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.41 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.51 |       231.01 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.11 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.52 |        81.22 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.22 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       103.56 |      1841.56 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.38 |      1147.45 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1146.69 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.00 |       201.25 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.25 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.26 |       492.86 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       491.94 |         0.47 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       651.39 |      3202.01 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.02 |        81.79 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.79 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       483.36 |      1206.97 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.25 |       650.17 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       649.38 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.71 |       110.50 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.50 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       337.86 |       217.69 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.86 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       116.83 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        48.73 |       228.61 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       227.58 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.95 |        80.61 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.61 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       113.43 |      1832.64 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.49 |      1140.93 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1140.16 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.86 |       201.25 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.25 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.92 |       490.46 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       489.50 |         0.47 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       653.75 |      3222.23 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        17.25 |        82.53 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.53 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       489.49 |      1209.79 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        35.20 |       650.01 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       649.05 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.40 |       110.59 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.59 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       340.93 |       218.69 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.12 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.57 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        48.94 |       230.50 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       229.69 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        20.21 |        81.70 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.70 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       107.69 |      1848.22 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.29 |      1150.37 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1149.34 |         1.11 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.30 |       201.47 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.47 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        34.05 |       496.38 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       495.58 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       745.64 |      3235.29 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.39 |        81.73 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.73 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       544.21 |      1219.65 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.71 |       652.35 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       651.39 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.63 |       111.20 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.20 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       373.32 |       218.43 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.90 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.54 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        54.56 |       237.66 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       236.74 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.51 |        81.89 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.89 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       124.20 |      1852.03 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.20 |      1148.73 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1147.97 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.12 |       201.38 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.38 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.75 |       501.92 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       500.96 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       748.26 |      3210.27 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.98 |        81.70 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.70 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       559.84 |      1209.53 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.98 |       648.83 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       648.06 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        39.10 |       110.94 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.94 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       370.47 |       218.59 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.28 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.31 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        57.22 |       231.17 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.40 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        28.09 |        80.29 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.29 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       122.89 |      1838.75 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.33 |      1144.54 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1143.77 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.46 |       201.76 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.76 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.31 |       492.45 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       491.68 |         0.47 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       666.59 |      3226.62 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.51 |        82.46 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.46 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       481.07 |      1219.07 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.04 |       652.32 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       651.55 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.73 |       111.04 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.04 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       335.74 |       216.70 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.35 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       116.35 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        48.49 |       239.01 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       238.08 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.76 |        80.86 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.86 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       131.25 |      1844.22 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        45.10 |      1150.53 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1149.57 |         1.11 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.05 |       201.60 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.60 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        44.01 |       492.10 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       491.33 |         0.47 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       808.80 |      3211.93 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        20.65 |        81.98 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.98 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       576.65 |      1197.41 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.92 |       639.17 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       638.24 |         0.61 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        40.86 |       110.66 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.66 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       394.79 |       216.74 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.13 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       116.61 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        65.89 |       230.85 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       229.89 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        26.45 |        81.31 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.31 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       167.67 |      1851.23 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        81.69 |      1148.93 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1148.16 |         1.11 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.01 |       201.95 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.95 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.52 |       500.35 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       499.33 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       738.80 |      3230.01 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        21.69 |        82.02 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.02 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       552.97 |      1220.77 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.16 |       651.84 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       651.07 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        39.97 |       110.21 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.21 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       382.46 |       219.87 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.83 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       119.04 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        53.37 |       238.85 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       237.79 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.68 |        80.80 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.80 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       122.79 |      1846.43 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        40.51 |      1149.02 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1148.22 |         1.11 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.21 |       202.05 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.05 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.52 |       495.36 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       494.40 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       731.18 |      3210.27 |         3.09 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.23 |        82.21 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.21 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       545.87 |      1206.85 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        41.15 |       647.71 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       646.94 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.41 |       110.72 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.72 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       374.59 |       218.43 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.83 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.60 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        56.40 |       229.98 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       229.22 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.10 |        81.25 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.25 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       121.50 |      1839.97 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.84 |      1143.68 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1142.75 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.11 |       201.60 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.60 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.67 |       494.69 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       493.89 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       741.57 |      3228.35 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.01 |        83.17 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        83.17 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       547.83 |      1220.48 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.92 |       654.50 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       653.50 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.35 |       110.72 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.72 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       381.25 |       218.27 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.61 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.66 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        55.70 |       236.99 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       236.09 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.03 |        80.67 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.67 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       130.52 |      1844.03 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.76 |      1148.70 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1147.81 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.66 |       201.76 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       201.76 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.01 |       493.57 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       492.77 |         0.47 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       740.59 |      3218.55 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.57 |        82.08 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.08 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       553.65 |      1205.47 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        44.67 |       646.37 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       645.60 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.24 |       110.88 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       110.88 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       380.00 |       216.93 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       100.32 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       116.61 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        51.86 |       231.29 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.40 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.50 |        80.38 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.38 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       123.73 |      1850.62 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        40.12 |      1149.47 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1148.67 |         1.11 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.95 |       202.94 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.94 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.95 |       498.21 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       497.21 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       714.40 |      3235.29 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.77 |        82.21 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.21 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       535.59 |      1223.39 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.73 |       654.62 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       653.86 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.86 |       111.97 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.97 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       373.08 |       219.14 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.18 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.95 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        50.88 |       237.66 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.86 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       236.80 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.89 |        81.60 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.60 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       120.36 |      1848.09 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.15 |      1148.83 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1147.93 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.23 |       202.94 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.94 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.54 |       496.32 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       495.52 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       740.55 |      3220.76 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.32 |        81.95 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.95 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       543.16 |      1210.85 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.35 |       645.85 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       644.86 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.14 |       111.49 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.49 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       377.92 |       219.52 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.06 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       118.46 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        54.87 |       233.98 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       233.22 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        30.22 |        81.31 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.31 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       130.74 |      1846.65 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.23 |      1147.33 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1146.53 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        35.38 |       202.78 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.78 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        40.44 |       496.54 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       495.74 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       746.60 |      3233.95 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.19 |        82.69 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.69 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       560.27 |      1225.92 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.27 |       652.96 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       652.00 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        40.51 |       111.58 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.58 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       390.41 |       219.84 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.34 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       118.50 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        53.25 |       241.54 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       240.48 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.31 |        81.22 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.22 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       126.65 |      1844.12 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.57 |      1144.89 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1144.09 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.22 |       202.59 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.59 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        43.92 |       496.64 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       495.58 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       763.40 |      3229.59 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.75 |        82.59 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.59 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       538.72 |      1214.43 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.51 |       650.14 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       649.34 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.45 |       112.29 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       112.29 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       364.06 |       220.26 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.09 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       119.17 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        51.02 |       231.74 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.97 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        24.93 |        81.22 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.22 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       164.49 |      1851.36 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.72 |      1151.26 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1150.33 |         1.11 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.43 |       202.53 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.53 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        82.83 |       497.57 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       496.77 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       782.09 |      3239.26 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.43 |        82.14 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.14 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       543.50 |      1227.23 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.58 |       656.06 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       655.29 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.37 |       111.97 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.97 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       370.26 |       219.65 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.02 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       118.62 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        62.31 |       239.55 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.86 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       238.69 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.84 |        81.18 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.18 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       175.90 |      1848.70 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.31 |      1147.77 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1146.81 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.62 |       203.10 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       203.10 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        41.29 |       497.82 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       497.02 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       712.23 |      3227.35 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        20.26 |        82.50 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.50 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       529.59 |      1212.86 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.73 |       650.17 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.12 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       649.05 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        39.30 |       111.91 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.91 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       356.76 |       219.36 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.28 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       118.08 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        57.34 |       231.42 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.50 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.81 |        81.44 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.44 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       120.80 |      1850.56 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.82 |      1148.51 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1147.71 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.95 |       202.37 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.37 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.02 |       499.68 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       498.75 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       703.90 |      3236.22 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.54 |        82.34 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.34 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       523.31 |      1226.69 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.91 |       654.98 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       654.21 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.53 |       111.87 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.87 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       361.39 |       220.90 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.28 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       119.62 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        50.56 |       238.94 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       237.92 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.20 |        81.31 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.31 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       121.94 |      1845.88 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.01 |      1145.31 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1144.51 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.12 |       203.29 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       203.29 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.92 |       497.28 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       496.32 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       662.93 |      3232.60 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.74 |        81.73 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.73 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       500.05 |      1212.41 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.99 |       650.91 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       649.89 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.94 |       111.65 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.65 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       336.19 |       218.85 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.28 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       117.57 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.04 |       231.01 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.21 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.86 |        81.34 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.34 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       107.78 |      1857.12 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.38 |      1153.37 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1152.45 |         1.11 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.70 |       202.85 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.85 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        34.58 |       500.90 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       500.10 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       619.17 |      3234.27 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.19 |        81.25 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.25 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       463.01 |      1225.12 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.42 |       654.62 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       653.66 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.85 |       111.39 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.39 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       318.94 |       220.19 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.09 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       119.10 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.95 |       238.91 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       237.98 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.43 |        81.09 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.09 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       105.45 |      1846.81 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        33.73 |      1148.25 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1147.48 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        28.03 |       202.53 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.53 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.26 |       496.03 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       495.07 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       600.41 |      3218.11 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.02 |        82.37 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.37 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       447.93 |      1210.65 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.87 |       646.88 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       646.08 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        30.91 |       111.78 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.78 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       311.04 |       220.32 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.12 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       119.20 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.72 |       231.68 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.88 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        18.58 |        81.63 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.63 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       104.67 |      1843.45 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.99 |      1143.84 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1143.07 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.39 |       202.37 |         0.19 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.37 |         0.19 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.27 |       497.25 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       496.45 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       637.71 |      3242.87 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.44 |        81.89 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.89 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       474.42 |      1227.42 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        31.61 |       655.52 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       654.72 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        30.44 |       111.97 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.97 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       331.20 |       220.61 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.41 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       119.20 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        48.82 |       239.33 |         0.23 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       238.40 |         0.23 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.11 |        80.67 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.67 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       111.05 |      1852.89 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        38.11 |      1152.16 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1151.20 |         1.11 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.80 |       204.61 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       204.61 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.43 |       496.13 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       495.36 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       628.55 |      3233.82 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        21.14 |        82.18 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        82.18 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       461.08 |      1215.81 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.55 |       649.54 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       648.48 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.45 |       112.25 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       112.25 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       305.96 |       220.70 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.18 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       119.52 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.07 |       233.31 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       232.35 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.79 |        81.70 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.70 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       106.27 |      1854.14 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.89 |      1148.16 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1147.36 |         1.10 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        25.75 |       202.66 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.66 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.24 |       503.33 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       502.37 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       646.84 |      3237.05 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.07 |        83.01 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        83.01 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       487.96 |      1222.17 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        31.43 |       657.25 |         0.63 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       656.48 |         0.63 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        31.62 |       112.09 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       112.09 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       305.36 |       220.77 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.12 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       119.65 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        49.02 |       232.06 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       231.01 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        19.50 |        81.44 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        81.44 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       108.41 |      1850.43 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.10 |      1150.85 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1150.08 |         1.11 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.29 |       202.94 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       202.94 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.34 |       496.64 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.38 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       495.26 |         0.48 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       642.32 |      3238.84 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.89 |        83.20 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        83.20 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaAttention                                          |       487.45 |      1210.27 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.13 |       646.88 |         0.62 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       646.08 |         0.62 | mm(float16[4000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        32.03 |       111.46 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       111.46 |         0.11 | _C::rotary_embedding(int64[4000], float16[4000, 4096], fl...
|---- Attention                                              |       345.65 |       219.58 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       101.06 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[4000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       118.53 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[4000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.57 |       232.35 |         0.22 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       231.58 |         0.22 | mm(float16[4000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        18.48 |        80.70 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |        80.70 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
|--- LlamaMLP                                                |       105.71 |      1864.67 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.37 |      1155.77 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1154.81 |         1.11 | mm(float16[4000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        26.14 |       203.74 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       203.74 |         0.20 | _C::silu_and_mul(float16[4000, 11008], float16[4000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.44 |       505.15 |         0.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       504.35 |         0.49 | mm(float16[4000, 11008], float16[11008, 4096]) <- matmul(...
|-- RMSNorm(weight=float16[4096])                            |        16.18 |        82.72 |         0.08 |                                                             
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::... |         0.00 |        82.72 |         0.08 | _C::fused_add_rms_norm(float16[4000, 4096], float16[4000,...
LogitsProcessor                                              |       114.63 |       207.23 |         0.20 |                                                             
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |        12.13 |         0.01 | index_select(float16[4000, 4096], 0, int64[16])             
|- Memset (Device)                                           |         0.00 |         0.80 |         0.00 | mm(float16[16, 4096], float16[4096, 32000]) <- matmul(flo...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       194.30 |         0.19 | mm(float16[16, 4096], float16[4096, 32000]) <- matmul(flo...
Sampler                                                      |     81352.86 |       315.39 |         0.30 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.37 |         0.00 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.00 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.40 |         0.00 | copy_(int32[16], int32[16], True) <- _to_copy(int32[16], ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.00 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.05 |         0.00 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.00 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.05 |         0.00 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         4.96 |         0.00 | copy_(float32[16, 32000], float16[16, 32000], False) <- _...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         5.18 |         0.00 | div_(float32[16, 32000], float16[16, 1])                    
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |         3.78 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         0.96 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.73 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         6.59 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.86 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.66 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        19.71 |         0.02 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.66 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        18.11 |         0.02 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.63 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        18.40 |         0.02 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.67 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        17.95 |         0.02 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         0.77 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.34 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         3.62 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.86 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.70 |         0.00 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        12.67 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |         8.38 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         2.98 |         0.00 | copy_(float32[16, 32000], float32[16, 32000], False) <- s...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         2.02 |         0.00 | copy_(int64[16], int32[16], False) <- _to_copy(int32[16],...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.60 |         0.00 | sub(int64[], int64[16], 1) <- rsub(int64[16], 32000, 1)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.37 |         0.00 | gather(float32[16, 32000], 1, int64[16, 1], False)          
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.19 |         0.00 | lt(float32[16, 32000], float32[16, 1])                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.27 |         0.00 | masked_fill_(float32[16, 32000], bool[16, 32000], -inf)     
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        12.26 |         0.01 | _softmax(float32[16, 32000], -1, False) <- softmax(float3...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        51.62 |         0.05 | cumsum(float32[16, 32000], -1, None)                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.63 |         0.00 | sub(int64[], float16[16, 1], 1) <- rsub(float16[16, 1], 1...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         5.25 |         0.01 | le(float32[16, 32000], float16[16, 1])                      
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.98 |         0.00 | fill_(bool[16], bool[])                                     
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.56 |         0.00 | masked_fill_(float32[16, 32000], bool[16, 32000], -inf)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |        13.06 |         0.01 | scatter_(float32[16, 32000], -1, int64[16, 32000], float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        18.82 |         0.02 | _softmax(float32[16, 32000], -1, False) <- softmax(float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.11 |         0.01 | _log_softmax(float32[16, 32000], -1, False) <- log_softma...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         2.02 |         0.00 | copy_(int64[16], int32[16], False) <- _to_copy(int32[16],...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         5.66 |         0.01 | index(float32[16, 32000], None)                             
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         5.22 |         0.01 | exponential_(float32[16, 32000], 1.0, None)                 
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         3.62 |         0.00 | div_(float32[16, 32000], float32[16, 32000])                
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        12.38 |         0.01 | argmax(float32[16, 32000], 1, False)                        
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.69 |         0.00 | copy_(int64[16, 1], int64[16, 1], False) <- _to_copy(int6...

================================================================================
= First Decode Step Model Table (prompt_len=250, batch_size=16)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
CUDAGraphRunner                                              |       649.04 |     11941.63 |        96.17 |                                                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.31 |         0.01 | copy_(int64[16], int64[16], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.31 |         0.01 | copy_(int64[16], int64[16], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.44 |         0.01 | copy_(int64[16], int64[16], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.38 |         0.01 | copy_(int32[16], int32[16], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.50 |         0.01 | copy_(int32[16, 256], int32[16, 256], True)                 
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.12 |         0.01 | fill_(int64[1], 0)                                          
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.28 |         0.01 | fill_(int64[1], 12)                                         
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |        11.71 |         0.09 |                                                             
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10:... |         0.00 |         4.29 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        84.58 |         0.68 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.61 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.65 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        49.25 |         0.40 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        25.54 |         0.21 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |       122.78 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.30 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        66.11 |         0.53 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         2.02 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.02 |                                                             
LogitsProcessor                                              |       246.81 |       192.00 |         1.55 |                                                             
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         9.44 |         0.08 | index_select(float16[16, 4096], 0, int64[16])               
|- Memset (Device)                                           |         0.00 |         0.80 |         0.01 | mm(float16[16, 4096], float16[4096, 32000]) <- matmul(flo...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       181.76 |         1.46 | mm(float16[16, 4096], float16[4096, 32000]) <- matmul(flo...
Sampler                                                      |     10282.25 |       283.33 |         2.28 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.02 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.02 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.18 |         0.02 | copy_(int32[16], int32[16], True) <- _to_copy(int32[16], ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.14 |         0.02 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.14 |         0.02 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.02 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.08 |         0.02 | copy_(float16[16], float16[16], True) <- _to_copy(float16...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         4.32 |         0.03 | copy_(float32[16, 32000], float16[16, 32000], False) <- _...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.61 |         0.04 | div_(float32[16, 32000], float16[16, 1])                    
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |         3.33 |         0.03 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         6.14 |         0.05 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.92 |         0.02 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        17.38 |         0.14 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        16.35 |         0.13 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        15.74 |         0.13 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        15.78 |         0.13 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         0.99 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.31 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         3.30 |         0.03 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.44 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        11.62 |         0.09 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |         8.22 |         0.07 | sort(float32[16, 32000], False, -1, False) <- sort(float3...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         2.78 |         0.02 | copy_(float32[16, 32000], float32[16, 32000], False) <- s...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.70 |         0.01 | copy_(int64[16], int32[16], False) <- _to_copy(int32[16],...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.44 |         0.01 | sub(int64[], int64[16], 1) <- rsub(int64[16], 32000, 1)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.56 |         0.02 | gather(float32[16, 32000], 1, int64[16, 1], False)          
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.78 |         0.03 | lt(float32[16, 32000], float32[16, 1])                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.14 |         0.02 | masked_fill_(float32[16, 32000], bool[16, 32000], -inf)     
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.94 |         0.09 | _softmax(float32[16, 32000], -1, False) <- softmax(float3...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        44.51 |         0.36 | cumsum(float32[16, 32000], -1, None)                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.54 |         0.01 | sub(int64[], float16[16, 1], 1) <- rsub(float16[16, 1], 1...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.64 |         0.04 | le(float32[16, 32000], float16[16, 1])                      
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.79 |         0.01 | fill_(bool[16], bool[])                                     
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.27 |         0.02 | masked_fill_(float32[16, 32000], bool[16, 32000], -inf)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |        12.70 |         0.10 | scatter_(float32[16, 32000], -1, int64[16, 32000], float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        16.42 |         0.13 | _softmax(float32[16, 32000], -1, False) <- softmax(float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |         9.25 |         0.07 | _log_softmax(float32[16, 32000], -1, False) <- log_softma...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.92 |         0.02 | copy_(int64[16], int32[16], False) <- _to_copy(int32[16],...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         5.25 |         0.04 | index(float32[16, 32000], None)                             
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         4.54 |         0.04 | exponential_(float32[16, 32000], 1.0, None)                 
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         3.26 |         0.03 | div_(float32[16, 32000], float32[16, 32000])                
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        11.01 |         0.09 | argmax(float32[16, 32000], 1, False)                        
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.24 |         0.02 | copy_(int64[16, 1], int64[16, 1], False) <- _to_copy(int6...

================================================================================
= Prefill Summary Table (prompt_len=250, batch_size=16)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
LlamaForCausalLM                                                                 |    103379.29 |        99.50 |            1.00
|- LlamaModel                                                                    |    103379.29 |        99.50 |            1.00
|-- VocabParallelEmbedding(weight=float16[32000, 4096])                          |       117.47 |         0.11 |            1.00
|--- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half,... |       117.47 |         0.11 |            1.00
|-- LlamaDecoderLayer                                                            |    103179.10 |        99.30 |           32.00
|--- RMSNorm(weight=float16[4096])                                               |      5213.88 |         5.02 |           64.00
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10... |        69.70 |         0.07 |            1.00
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void... |      5144.18 |         4.95 |           63.00
|--- LlamaAttention                                                              |     38886.29 |        37.43 |           32.00
|---- QKVParallelLinear(weight=float16[12288, 4096])                             |     20818.42 |        20.04 |           32.00
|----- Memset (Device)                                                           |        27.58 |         0.03 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |     20790.84 |        20.01 |           32.00
|---- RotaryEmbedding                                                            |      3559.19 |         3.43 |           32.00
|----- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::... |      3559.19 |         3.43 |           32.00
|---- Attention                                                                  |      7005.68 |         6.74 |           32.00
|----- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned sho... |      3228.83 |         3.11 |           32.00
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128, 128, 64, 4, false, ... |      3776.86 |         3.64 |           32.00
|---- RowParallelLinear(weight=float16[4096, 4096])                              |      7503.00 |         7.22 |           32.00
|----- Memset (Device)                                                           |        28.58 |         0.03 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |      7474.42 |         7.19 |           32.00
|--- LlamaMLP                                                                    |     59078.93 |        56.86 |           32.00
|---- MergedColumnParallelLinear(weight=float16[22016, 4096])                    |     36719.26 |        35.34 |           32.00
|----- Memset (Device)                                                           |        27.10 |         0.03 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |     36692.16 |        35.31 |           32.00
|---- SiluAndMul                                                                 |      6472.18 |         6.23 |           32.00
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel... |      6472.18 |         6.23 |           32.00
|---- RowParallelLinear(weight=float16[4096, 11008])                             |     15887.49 |        15.29 |           32.00
|----- Memset (Device)                                                           |        28.10 |         0.03 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x256x64_warpgroupsize... |     15859.39 |        15.26 |           32.00
|-- RMSNorm(weight=float16[4096])                                                |        82.72 |         0.08 |            1.00
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>... |        82.72 |         0.08 |            1.00
LogitsProcessor                                                                  |       207.23 |         0.20 |            1.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |        12.13 |         0.01 |            1.00
|- Memset (Device)                                                               |         0.80 |         0.00 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |       194.30 |         0.19 |            1.00
Sampler                                                                          |       315.39 |         0.30 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.91 |         0.01 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.96 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         5.18 |         0.00 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |         3.78 |         0.00 |            1.00
|- Memset (Device)                                                               |        13.12 |         0.01 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         6.59 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.86 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        74.17 |         0.07 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         3.62 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.86 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        12.67 |         0.01 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |         8.38 |         0.01 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         2.98 |         0.00 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.03 |         0.00 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.60 |         0.00 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.37 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         4.19 |         0.00 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         4.83 |         0.00 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        31.07 |         0.03 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        51.62 |         0.05 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.63 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         5.25 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.98 |         0.00 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |        13.06 |         0.01 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        10.11 |         0.01 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         5.66 |         0.01 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         5.22 |         0.01 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         3.62 |         0.00 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        12.38 |         0.01 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.69 |         0.00 |            1.00

================================================================================
= First Decode Step Summary Table (prompt_len=250, batch_size=16)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
CUDAGraphRunner                                                                  |     11941.63 |        96.17 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         6.94 |         0.06 |            5.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<... |         2.40 |         0.02 |            2.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |        11.71 |         0.09 |            1.00
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10::H... |         4.29 |         0.03 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |      2706.43 |        21.80 |           32.00
|- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::Half... |       147.46 |         1.19 |           32.00
|- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned short, ... |       116.77 |         0.94 |           32.00
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<128, 64, 128, 4, fal... |      1575.90 |        12.69 |           32.00
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x... |       817.15 |         6.58 |           32.00
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>::... |       186.37 |         1.50 |           64.00
|- Memset (Device)                                                               |        23.55 |         0.19 |           32.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64_warpgroupsize1x1x1_... |      3929.09 |        31.64 |           32.00
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel<c10... |       233.47 |         1.88 |           32.00
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f16_128x64_64x6_tn_ali... |      2115.58 |        17.04 |           32.00
|- void splitKreduce_kernel<32, 16, int, float, __half, float, __half, true, ... |        64.51 |         0.52 |           32.00
LogitsProcessor                                                                  |       192.00 |         1.55 |            1.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         9.44 |         0.08 |            1.00
|- Memset (Device)                                                               |         0.80 |         0.01 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |       181.76 |         1.46 |            1.00
Sampler                                                                          |       283.33 |         2.28 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.53 |         0.12 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.32 |         0.03 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.61 |         0.04 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |         3.33 |         0.03 |            1.00
|- Memset (Device)                                                               |        11.97 |         0.10 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         6.14 |         0.05 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.92 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        65.25 |         0.53 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         3.30 |         0.03 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.44 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        11.62 |         0.09 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |         8.22 |         0.07 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         2.78 |         0.02 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.62 |         0.03 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.44 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.56 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         3.78 |         0.03 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         4.42 |         0.04 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        27.36 |         0.22 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        44.51 |         0.36 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.54 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.64 |         0.04 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.79 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |        12.70 |         0.10 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |         9.25 |         0.07 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         5.25 |         0.04 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         4.54 |         0.04 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         3.26 |         0.03 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        11.01 |         0.09 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.24 |         0.02 |            1.00
Traces saved as prefill.json and decode_1.json, etc. in folder benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_16/chrome_traces
