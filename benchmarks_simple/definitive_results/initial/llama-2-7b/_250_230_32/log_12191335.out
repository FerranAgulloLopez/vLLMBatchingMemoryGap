Run profile with:
  engine_args = {'model': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'served_model_name': None, 'tokenizer': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'skip_tokenizer_init': False, 'tokenizer_mode': 'auto', 'trust_remote_code': False, 'download_dir': None, 'load_format': 'auto', 'config_format': <ConfigFormat.AUTO: 'auto'>, 'dtype': 'auto', 'kv_cache_dtype': 'auto', 'quantization_param_path': None, 'seed': 0, 'max_model_len': None, 'worker_use_ray': False, 'distributed_executor_backend': None, 'pipeline_parallel_size': 1, 'tensor_parallel_size': 1, 'max_parallel_loading_workers': None, 'block_size': 16, 'enable_prefix_caching': False, 'disable_sliding_window': False, 'use_v2_block_manager': False, 'swap_space': 4, 'cpu_offload_gb': 0, 'gpu_memory_utilization': 0.9, 'max_num_batched_tokens': 256000, 'max_num_seqs': 1024, 'max_logprobs': 20, 'disable_log_stats': False, 'revision': None, 'code_revision': None, 'rope_scaling': None, 'rope_theta': None, 'tokenizer_revision': None, 'quantization': None, 'enforce_eager': False, 'max_context_len_to_capture': None, 'max_seq_len_to_capture': 8192, 'disable_custom_all_reduce': False, 'tokenizer_pool_size': 0, 'tokenizer_pool_type': 'ray', 'tokenizer_pool_extra_config': None, 'limit_mm_per_prompt': None, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'enable_prompt_adapter': False, 'max_prompt_adapters': 1, 'max_prompt_adapter_token': 0, 'fully_sharded_loras': False, 'lora_extra_vocab_size': 256, 'long_lora_scaling_factors': None, 'lora_dtype': 'auto', 'max_cpu_loras': None, 'device': 'auto', 'num_scheduler_steps': 1, 'multi_step_stream_outputs': True, 'ray_workers_use_nsight': False, 'num_gpu_blocks_override': None, 'num_lookahead_slots': 0, 'model_loader_extra_config': None, 'ignore_patterns': [], 'preemption_mode': None, 'scheduler_delay_factor': 0.0, 'enable_chunked_prefill': None, 'guided_decoding_backend': 'outlines', 'speculative_model': None, 'speculative_model_quantization': None, 'speculative_draft_tensor_parallel_size': None, 'num_speculative_tokens': None, 'speculative_disable_mqa_scorer': False, 'speculative_max_model_len': None, 'speculative_disable_by_batch_size': None, 'ngram_prompt_lookup_max': None, 'ngram_prompt_lookup_min': None, 'spec_decoding_acceptance_method': 'rejection_sampler', 'typical_acceptance_sampler_posterior_threshold': None, 'typical_acceptance_sampler_posterior_alpha': None, 'qlora_adapter_name_or_path': None, 'disable_logprobs_during_spec_decoding': None, 'otlp_traces_endpoint': None, 'collect_detailed_traces': None, 'disable_async_output_proc': False, 'override_neuron_config': None, 'mm_processor_kwargs': None, 'scheduling_policy': 'fcfs'}
  prompt_len = 250
  output_len = 230
  batch_size = 32
  result_dir = benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_32
  save_chrome_traces_folder = chrome_traces
WARNING 11-25 13:50:36 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 11-25 13:50:36 llm_engine.py:237] Initializing an LLM engine (v0.5.2.dev1210+g6f078ea3c) with config: model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', speculative_config=None, tokenizer='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 11-25 13:50:37 model_runner.py:1056] Starting to load model /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b...
INFO 11-25 13:50:43 model_runner.py:1067] Loading model weights took 12.5523 GB
INFO 11-25 13:50:50 worker.py:262] Memory profiling results: total_gpu_memory=63.43GiB initial_memory_usage=13.15GiB peak_torch_memory=36.15GiB non_torch_memory=0.10GiB kv_cache_size=20.84GiB gpu_memory_utilization=0.90
INFO 11-25 13:50:50 gpu_executor.py:122] # GPU blocks: 2667, # CPU blocks: 512
INFO 11-25 13:50:50 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 10.42x
INFO 11-25 13:50:52 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-25 13:50:52 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-25 13:51:23 model_runner.py:1523] Graph capturing finished in 32 secs.
llm.llm_engine.model_config.max_model_len:  4096
Warm up run ...
Profile run ...
INFO 11-25 13:51:29 metrics.py:349] Avg prompt throughput: 3041.9 tokens/s, Avg generation throughput: 79.1 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.4%, CPU KV cache usage: 0.0%.
INFO 11-25 13:51:34 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.2 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.6%, CPU KV cache usage: 0.0%.
INFO 11-25 13:51:39 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.9 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 22.8%, CPU KV cache usage: 0.0%.
INFO 11-25 13:51:44 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.0%, CPU KV cache usage: 0.0%.
INFO 11-25 13:51:49 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.2 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 25.2%, CPU KV cache usage: 0.0%.
INFO 11-25 13:51:55 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.4%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:00 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.9 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.6%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:05 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.8%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:10 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.7 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.0%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:15 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.8 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.2%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:20 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.4 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.4%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:26 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 34.8%, CPU KV cache usage: 0.0%.
INFO 11-25 13:52:31 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.9 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.0%, CPU KV cache usage: 0.0%.
================================================================================
= Prefill Model Table (prompt_len=250, batch_size=32)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
LlamaForCausalLM                                             |     27045.81 |    201826.69 |        99.70 |                                                             
|- LlamaModel                                                |     27019.28 |    201826.69 |        99.70 |                                                             
|-- VocabParallelEmbedding(weight=float16[32000, 4096])      |        91.45 |       236.25 |         0.12 |                                                             
|--- void at::native::(anonymous namespace)::indexSelectL... |         0.00 |       236.25 |         0.12 | index_select(float16[32000, 4096], 0, int64[8000]) <- emb...
|-- LlamaDecoderLayer                                        |      2999.91 |      6148.07 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        80.20 |       135.14 |         0.07 |                                                             
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c... |         0.00 |       135.14 |         0.07 | _C::rms_norm(float16[8000, 4096], float16[8000, 4096], fl...
|--- LlamaAttention                                          |      2667.73 |      2318.04 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |       111.75 |      1166.97 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1166.20 |         0.58 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        67.52 |       223.10 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       223.10 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |      2314.06 |       417.53 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       195.78 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       221.76 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |       101.50 |       510.43 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       509.66 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        43.85 |       168.83 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.83 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       173.14 |      3526.07 |         1.74 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        55.58 |      2178.14 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2177.37 |         1.08 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        46.48 |       400.61 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       400.61 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        48.70 |       947.32 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       946.56 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       809.69 |      6203.31 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        23.32 |       168.48 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.48 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       609.29 |      2340.73 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        41.72 |      1171.68 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1170.72 |         0.58 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        45.02 |       223.23 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       223.23 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       417.37 |       418.94 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.16 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       222.78 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        57.97 |       526.88 |         0.26 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       525.89 |         0.26 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.16 |       169.53 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       169.53 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       129.34 |      3524.56 |         1.74 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.31 |      2177.37 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2176.44 |         1.08 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.79 |       400.57 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       400.57 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.48 |       946.62 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       945.85 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       739.01 |      6191.85 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.03 |       169.02 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       169.02 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       548.98 |      2325.56 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.42 |      1171.26 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1170.24 |         0.58 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.36 |       223.58 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       223.58 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       369.18 |       417.63 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       195.84 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       221.79 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        54.68 |       513.09 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       512.16 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.75 |       167.33 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.33 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       124.75 |      3529.94 |         1.74 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.43 |      2168.51 |         1.07 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2167.74 |         1.07 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.29 |       400.13 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       400.13 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.73 |       961.31 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       960.32 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       733.71 |      6230.54 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.91 |       169.57 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       169.57 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       537.73 |      2320.34 |         1.15 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.22 |      1167.80 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1167.00 |         0.58 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.76 |       223.29 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       223.29 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       362.77 |       416.89 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       195.94 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       220.96 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        56.50 |       512.35 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.12 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       511.23 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.36 |       167.07 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.07 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       132.63 |      3573.55 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        45.53 |      2174.81 |         1.07 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2174.04 |         1.07 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.14 |       400.00 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       400.00 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.60 |       998.75 |         0.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       997.79 |         0.49 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       749.37 |      6173.32 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        29.25 |       168.90 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.90 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       549.05 |      2317.24 |         1.14 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.64 |      1161.85 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1161.05 |         0.57 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.21 |       222.59 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       222.59 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       379.30 |       416.89 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       195.65 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       221.25 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        51.36 |       515.90 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       515.13 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.21 |       167.68 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.68 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       125.68 |      3519.51 |         1.74 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.31 |      2175.86 |         1.07 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2174.91 |         1.07 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.05 |       400.45 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       400.45 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.03 |       943.20 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       942.43 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       762.59 |      6185.45 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.09 |       167.94 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.94 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       576.70 |      2314.62 |         1.14 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.08 |      1161.92 |         0.57 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1160.99 |         0.57 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.09 |       223.62 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       223.62 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       364.17 |       416.73 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       195.71 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       221.02 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        93.56 |       512.35 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       511.42 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.06 |       166.72 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       166.72 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       124.56 |      3536.18 |         1.75 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.12 |      2180.89 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2179.96 |         1.08 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.58 |       400.35 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       400.35 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.56 |       954.94 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       954.17 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       771.48 |      6183.53 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        17.93 |       168.29 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.29 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       559.44 |      2310.46 |         1.14 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        43.30 |      1169.66 |         0.58 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1168.89 |         0.58 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        40.89 |       223.58 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       223.58 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       375.61 |       414.88 |         0.20 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       195.17 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       219.71 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        58.97 |       502.33 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       501.41 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.47 |       169.41 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       169.41 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       149.17 |      3535.38 |         1.75 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        45.07 |      2182.33 |         1.08 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2181.56 |         1.08 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.78 |       402.85 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       402.85 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.97 |       950.20 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.05 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       949.15 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       733.75 |      6296.65 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        20.27 |       168.64 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.64 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       539.73 |      2363.74 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.07 |      1211.48 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1210.72 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.87 |       225.82 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       225.82 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       369.34 |       419.23 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.45 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       222.78 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        51.97 |       507.20 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       506.27 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.86 |       167.01 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.01 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       132.35 |      3597.27 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.62 |      2246.30 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2245.34 |         1.11 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.95 |       402.81 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       402.81 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        46.40 |       948.16 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       947.39 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       715.53 |      6205.00 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.05 |       168.48 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.48 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       528.96 |      2354.01 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.46 |      1198.65 |         0.59 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1197.69 |         0.59 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.61 |       225.57 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       225.57 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       361.51 |       420.13 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.57 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       223.55 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        50.98 |       509.66 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       508.89 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        21.69 |       167.84 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.84 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       124.44 |      3514.67 |         1.74 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.00 |      2166.04 |         1.07 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2165.24 |         1.07 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.20 |       402.30 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       402.30 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.92 |       946.33 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       945.56 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       761.10 |      6282.19 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.13 |       169.41 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       169.41 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       569.30 |      2356.38 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        36.78 |      1200.89 |         0.59 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1199.90 |         0.59 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.30 |       225.82 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       225.82 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       393.33 |       419.23 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       197.06 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       222.18 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        59.34 |       510.43 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       509.41 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        24.59 |       167.33 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.33 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       130.15 |      3589.08 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        45.93 |      2239.00 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2238.20 |         1.11 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.72 |       402.88 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       402.88 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.30 |       947.20 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       946.24 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       725.43 |      6280.36 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.94 |       167.94 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.94 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       540.33 |      2353.37 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.89 |      1206.68 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1205.92 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.22 |       226.27 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.27 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       369.91 |       417.63 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.09 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       221.54 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        54.24 |       502.78 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       501.98 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.93 |       167.07 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.07 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       124.36 |      3591.99 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.42 |      2232.54 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2231.58 |         1.10 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.10 |       403.04 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.04 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.68 |       956.41 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       955.61 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       717.08 |      6266.15 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.72 |       168.96 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.96 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       529.11 |      2373.43 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.96 |      1207.58 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1206.81 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.01 |       225.66 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       225.66 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       352.86 |       422.14 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       197.18 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       224.96 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        51.57 |       518.05 |         0.26 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       517.12 |         0.26 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.81 |       168.03 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.03 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       125.50 |      3555.73 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.75 |      2205.98 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2205.05 |         1.09 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.50 |       402.81 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       402.81 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.40 |       946.94 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       946.17 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       753.56 |      6326.83 |         3.13 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.05 |       168.06 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.06 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       564.37 |      2368.92 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.53 |      1210.36 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1209.40 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.46 |       225.57 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       225.57 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       393.83 |       418.62 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.64 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       221.98 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        54.00 |       514.37 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       513.47 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.10 |       167.52 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.52 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       127.62 |      3622.32 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        44.95 |      2248.38 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2247.58 |         1.11 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.48 |       402.97 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       402.97 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.13 |       970.97 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       970.01 |         0.48 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       790.71 |      6296.68 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.33 |       170.59 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       170.59 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       548.46 |      2367.13 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.59 |      1212.06 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1211.29 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.00 |       224.67 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       224.67 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       379.09 |       419.97 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.54 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       223.42 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        55.25 |       510.43 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       509.44 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        35.79 |       167.52 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.52 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       169.08 |      3591.44 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        81.78 |      2244.02 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2243.26 |         1.11 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.32 |       402.94 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       402.94 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.70 |       944.48 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       943.52 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       735.78 |      6269.61 |         3.10 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.76 |       169.15 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       169.15 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       551.75 |      2345.11 |         1.16 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.41 |      1195.16 |         0.59 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1194.20 |         0.59 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        43.11 |       225.57 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       225.57 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       377.70 |       419.58 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.54 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       223.04 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        52.06 |       504.80 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       504.00 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.80 |       168.86 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.86 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       122.90 |      3586.48 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        40.27 |      2232.47 |         1.10 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2231.51 |         1.10 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        29.76 |       402.69 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       402.69 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.93 |       951.33 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       950.56 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       742.22 |      6362.66 |         3.14 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.55 |       168.99 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.99 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       548.79 |      2374.39 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        45.85 |      1216.03 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1215.04 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.00 |       226.56 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.56 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       373.39 |       419.68 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       197.06 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       222.62 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        51.86 |       512.13 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       511.20 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.98 |       167.84 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.84 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       130.90 |      3651.44 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.11 |      2288.09 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2287.29 |         1.13 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        36.38 |       404.03 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       404.03 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.34 |       959.33 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       958.37 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       742.93 |      6336.49 |         3.13 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.28 |       168.41 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.41 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       543.04 |      2387.90 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.18 |      1233.05 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1232.28 |         0.61 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.94 |       227.78 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       227.78 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       367.80 |       418.62 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.96 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       221.66 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        58.92 |       508.45 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       507.68 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        26.06 |       168.48 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.48 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       134.97 |      3611.70 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.27 |      2246.46 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2245.66 |         1.11 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        36.36 |       403.68 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.68 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        40.36 |       961.57 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       960.80 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       753.97 |      6364.59 |         3.14 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.66 |       168.57 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.57 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       565.88 |      2392.31 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.57 |      1237.15 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1236.38 |         0.61 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.72 |       227.74 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       227.74 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       390.69 |       421.73 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.99 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       224.74 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        53.83 |       505.69 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.86 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       504.83 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.29 |       167.87 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.87 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       126.35 |      3635.83 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.92 |      2272.95 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2271.99 |         1.12 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.25 |       403.62 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.62 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.09 |       959.26 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       958.46 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       740.63 |      6356.75 |         3.14 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        22.28 |       168.00 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.00 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       551.90 |      2378.04 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.88 |      1226.04 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1225.12 |         0.61 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.27 |       226.69 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.69 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       380.70 |       420.99 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       197.09 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       223.90 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        52.75 |       504.32 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       503.39 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.89 |       167.78 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.78 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       125.07 |      3642.93 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        40.97 |      2279.74 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2278.97 |         1.13 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.10 |       404.06 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       404.06 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.47 |       959.13 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       958.17 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       775.07 |      6330.03 |         3.13 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        22.02 |       168.51 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.51 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       568.77 |      2380.70 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.44 |      1227.64 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1226.88 |         0.61 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.48 |       226.27 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.27 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       389.37 |       421.28 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.67 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       224.61 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        64.69 |       505.50 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.12 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       504.38 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.64 |       168.13 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.13 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       139.28 |      3612.69 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        51.57 |      2255.35 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2254.58 |         1.11 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.42 |       403.65 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.65 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        40.33 |       953.69 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       952.73 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       760.08 |      6322.67 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        20.77 |       168.54 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.54 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       566.60 |      2373.40 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.44 |      1224.64 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1223.87 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        39.48 |       225.28 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       225.28 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       393.34 |       420.32 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.70 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       223.62 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        54.02 |       503.17 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       502.37 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.50 |       167.13 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.13 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       127.08 |      3613.59 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.20 |      2254.94 |         1.11 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2253.98 |         1.11 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.50 |       403.13 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.13 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.81 |       955.52 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       954.75 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       775.11 |      6317.55 |         3.12 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.69 |       168.29 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.29 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       539.42 |      2360.83 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.21 |      1210.04 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1208.99 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        39.42 |       226.21 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.21 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       357.93 |       417.89 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.25 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       221.63 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        52.02 |       506.69 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       505.73 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        26.10 |       167.29 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.29 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       170.32 |      3621.14 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.20 |      2262.71 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2261.78 |         1.12 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.72 |       403.52 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.52 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        83.50 |       954.91 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       954.14 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       820.02 |      6353.93 |         3.14 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.84 |       168.26 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.26 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       570.00 |      2375.03 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.31 |      1216.35 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1215.58 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.45 |       226.59 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.59 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       390.07 |       421.82 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.93 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       224.90 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        62.30 |       510.27 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       509.31 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.57 |       167.87 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.87 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       183.34 |      3642.77 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        44.00 |      2274.49 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2273.69 |         1.12 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.33 |       403.42 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.42 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        43.00 |       964.86 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       963.87 |         0.48 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       743.24 |      6360.78 |         3.14 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        21.31 |       168.99 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.99 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       551.00 |      2378.84 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.99 |      1220.76 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1219.96 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.29 |       226.85 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.85 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       376.88 |       419.68 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       197.06 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       222.62 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        55.19 |       511.55 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       510.65 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.53 |       167.36 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.36 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       127.99 |      3645.59 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.80 |      2280.79 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2279.86 |         1.13 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.00 |       403.74 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.74 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        39.22 |       961.05 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       960.28 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       740.87 |      6359.40 |         3.14 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.93 |       168.51 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.51 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       535.46 |      2389.18 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.88 |      1228.70 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1227.71 |         0.61 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.31 |       226.46 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.46 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       361.89 |       418.69 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.77 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       221.92 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        56.99 |       515.33 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       514.56 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        25.50 |       167.42 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.42 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       141.55 |      3634.29 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.52 |      2271.99 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2271.22 |         1.12 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.82 |       403.45 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.45 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        50.44 |       958.84 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       958.08 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       756.21 |      6292.84 |         3.11 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        24.07 |       168.96 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.96 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       558.63 |      2373.81 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.15 |      1229.53 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1228.60 |         0.61 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        39.62 |       226.27 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.27 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       384.68 |       420.93 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.54 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       224.38 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        52.90 |       497.09 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       496.06 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.17 |       168.57 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.57 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       131.32 |      3581.49 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.50 |      2216.70 |         1.09 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2215.93 |         1.09 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        33.00 |       403.97 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.97 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        42.12 |       960.83 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       959.87 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       736.84 |      6352.91 |         3.14 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.72 |       168.48 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.48 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       548.82 |      2379.61 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.03 |      1224.95 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1224.19 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        39.03 |       227.62 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       227.62 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       376.44 |       421.79 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       197.18 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       224.61 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        53.49 |       505.25 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       504.45 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.80 |       168.80 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.80 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       127.20 |      3636.02 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.17 |      2275.26 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.06 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2274.20 |         1.12 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.68 |       403.97 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.97 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        40.64 |       956.80 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       956.03 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       758.97 |      6328.94 |         3.13 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.47 |       168.99 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.99 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       563.26 |      2378.87 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.67 |      1221.34 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1220.57 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.07 |       227.17 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       227.17 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       390.69 |       423.01 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.54 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       226.46 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        58.01 |       507.36 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.93 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       506.43 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        26.55 |       169.57 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       169.57 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       131.65 |      3611.51 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.77 |      2257.27 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2256.38 |         1.11 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        32.10 |       403.23 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.23 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        40.28 |       951.00 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       950.24 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       742.91 |      6340.62 |         3.13 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        20.02 |       168.42 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.42 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       547.39 |      2380.79 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        39.25 |      1216.16 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1215.20 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        38.98 |       226.37 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.37 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       375.24 |       420.10 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.99 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       223.10 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        51.81 |       518.17 |         0.26 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       517.18 |         0.26 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        26.34 |       169.15 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       169.15 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       130.94 |      3622.26 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        40.79 |      2263.10 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2262.30 |         1.12 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        36.25 |       403.58 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.58 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.34 |       955.58 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       954.59 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       716.76 |      6370.66 |         3.15 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.26 |       168.25 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.25 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       531.03 |      2387.67 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.47 |      1233.18 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1232.38 |         0.61 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        43.39 |       227.39 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       227.39 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       346.86 |       419.65 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.80 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       222.85 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        52.65 |       507.45 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       506.43 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        23.36 |       170.11 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       170.11 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       126.11 |      3644.63 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        43.35 |      2278.17 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2277.40 |         1.12 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        31.18 |       404.25 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       404.25 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.94 |       962.21 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       961.25 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       765.09 |      6369.87 |         3.15 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        18.42 |       168.03 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.03 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       581.17 |      2394.07 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.01 |      1221.12 |         0.60 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1220.12 |         0.60 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        37.62 |       226.97 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.97 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       371.43 |       420.64 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       196.77 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       223.87 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        57.04 |       525.34 |         0.26 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       524.54 |         0.26 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.78 |       167.46 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       167.46 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       123.74 |      3640.31 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        40.52 |      2273.66 |         1.12 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.90 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2272.76 |         1.12 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.03 |       404.29 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       404.29 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        38.48 |       962.37 |         0.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       961.60 |         0.48 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- LlamaDecoderLayer                                        |       732.66 |      6361.93 |         3.14 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        22.18 |       168.57 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       168.57 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaAttention                                          |       547.64 |      2375.77 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        37.53 |      1227.84 |         0.61 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.99 |         0.00 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      1226.84 |         0.61 | mm(float16[8000, 4096], float16[4096, 12288]) <- matmul(f...
|---- RotaryEmbedding                                        |        36.58 |       226.75 |         0.11 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |       226.75 |         0.11 | _C::rotary_embedding(int64[8000], float16[8000, 4096], fl...
|---- Attention                                              |       382.34 |       418.62 |         0.21 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |       197.18 |         0.10 | _C_cache_ops::reshape_and_cache_flash(float16[8000, 32, 1...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |       221.44 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[8000, 32, 128], flo...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        52.15 |       502.56 |         0.25 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.96 |         0.00 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       501.60 |         0.25 | mm(float16[8000, 4096], float16[4096, 4096]) <- matmul(fl...
|--- RMSNorm(weight=float16[4096])                           |        22.24 |       169.53 |         0.08 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |       169.53 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
|--- LlamaMLP                                                |       122.31 |      3648.05 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.93 |      2282.68 |         1.13 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |      2281.91 |         1.13 | mm(float16[8000, 4096], float16[4096, 22016]) <- matmul(f...
|---- SiluAndMul                                             |        30.31 |       403.81 |         0.20 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |       403.81 |         0.20 | _C::silu_and_mul(float16[8000, 11008], float16[8000, 22016])
|---- RowParallelLinear(weight=float16[4096, 11008])         |        37.66 |       961.56 |         0.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         1.02 |         0.00 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       960.54 |         0.47 | mm(float16[8000, 11008], float16[11008, 4096]) <- matmul(...
|-- RMSNorm(weight=float16[4096])                            |        18.67 |       168.29 |         0.08 |                                                             
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::... |         0.00 |       168.29 |         0.08 | _C::fused_add_rms_norm(float16[8000, 4096], float16[8000,...
LogitsProcessor                                              |       145.97 |       216.00 |         0.11 |                                                             
|- void at::native::(anonymous namespace)::indexSelectLar... |         0.00 |         4.67 |         0.00 | index_select(float16[8000, 4096], 0, int64[32])             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       211.33 |         0.10 | mm(float16[32, 4096], float16[4096, 32000]) <- matmul(flo...
Sampler                                                      |    177256.05 |       397.37 |         0.20 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.37 |         0.00 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.00 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.37 |         0.00 | copy_(int32[32], int32[32], True) <- _to_copy(int32[32], ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.00 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.00 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.00 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.08 |         0.00 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         5.41 |         0.00 | copy_(float32[32, 32000], float16[32, 32000], False) <- _...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         6.75 |         0.00 | div_(float32[32, 32000], float16[32, 1])                    
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |         5.73 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.02 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.73 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         9.76 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.76 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.63 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        29.82 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.60 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        28.54 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.57 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        27.01 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.79 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        27.49 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         0.70 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.38 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         4.93 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.98 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.66 |         0.00 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        17.15 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |        16.00 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         3.78 |         0.00 | copy_(float32[32, 32000], float32[32, 32000], False) <- s...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         2.11 |         0.00 | copy_(int64[32], int32[32], False) <- _to_copy(int32[32],...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.66 |         0.00 | sub(int64[], int64[32], 1) <- rsub(int64[32], 32000, 1)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.37 |         0.00 | gather(float32[32, 32000], 1, int64[32, 1], False)          
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         5.15 |         0.00 | lt(float32[32, 32000], float32[32, 1])                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         3.01 |         0.00 | masked_fill_(float32[32, 32000], bool[32, 32000], -inf)     
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        12.29 |         0.01 | _softmax(float32[32, 32000], -1, False) <- softmax(float3...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        50.27 |         0.02 | cumsum(float32[32, 32000], -1, None)                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.76 |         0.00 | sub(int64[], float16[32, 1], 1) <- rsub(float16[32, 1], 1...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         7.14 |         0.00 | le(float32[32, 32000], float16[32, 1])                      
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.86 |         0.00 | fill_(bool[32], bool[])                                     
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         3.23 |         0.00 | masked_fill_(float32[32, 32000], bool[32, 32000], -inf)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |        28.25 |         0.01 | scatter_(float32[32, 32000], -1, int64[32, 32000], float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        18.59 |         0.01 | _softmax(float32[32, 32000], -1, False) <- softmax(float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.50 |         0.01 | _log_softmax(float32[32, 32000], -1, False) <- log_softma...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         2.14 |         0.00 | copy_(int64[32], int32[32], False) <- _to_copy(int32[32],...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         8.29 |         0.00 | index(float32[32, 32000], None)                             
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         5.21 |         0.00 | exponential_(float32[32, 32000], 1.0, None)                 
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         4.80 |         0.00 | div_(float32[32, 32000], float32[32, 32000])                
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        12.19 |         0.01 | argmax(float32[32, 32000], 1, False)                        
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.56 |         0.00 | copy_(int64[32, 1], int64[32, 1], False) <- _to_copy(int6...

================================================================================
= First Decode Step Model Table (prompt_len=250, batch_size=32)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
CUDAGraphRunner                                              |       683.31 |     13969.79 |        96.14 |                                                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.28 |         0.01 | copy_(int64[32], int64[32], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.22 |         0.01 | copy_(int64[32], int64[32], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.44 |         0.01 | copy_(int64[32], int64[32], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.34 |         0.01 | copy_(int32[32], int32[32], True)                           
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.54 |         0.01 | copy_(int32[32, 256], int32[32, 256], True)                 
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.09 |         0.01 | fill_(int64[1], 0)                                          
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.25 |         0.01 | fill_(int64[1], 12)                                         
|- void at::native::(anonymous namespace)::indexSelectLar... |         0.00 |         3.36 |         0.02 |                                                             
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10:... |         0.00 |         4.25 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         5.95 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.81 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        91.65 |         0.63 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.02 |         0.18 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       127.39 |         0.88 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.20 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        84.06 |         0.58 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.94 |         0.02 |                                                             
LogitsProcessor                                              |       299.30 |       192.19 |         1.32 |                                                             
|- void at::native::(anonymous namespace)::indexSelectLar... |         0.00 |         4.19 |         0.03 | index_select(float16[32, 4096], 0, int64[32])               
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       188.00 |         1.29 | mm(float16[32, 4096], float16[4096, 32000]) <- matmul(flo...
Sampler                                                      |     10303.35 |       369.03 |         2.54 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.05 |         0.01 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.01 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.01 | copy_(int32[32], int32[32], True) <- _to_copy(int32[32], ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.01 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.01 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.01 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.01 | copy_(float16[32], float16[32], True) <- _to_copy(float16...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         4.96 |         0.03 | copy_(float32[32, 32000], float16[32, 32000], False) <- _...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         6.21 |         0.04 | div_(float32[32, 32000], float16[32, 1])                    
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |         5.28 |         0.04 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.31 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.41 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         9.09 |         0.06 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.82 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        27.01 |         0.19 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        26.85 |         0.18 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        25.34 |         0.17 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        26.24 |         0.18 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         0.77 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.28 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         4.90 |         0.03 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.60 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.01 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        15.55 |         0.11 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |        15.52 |         0.11 | sort(float32[32, 32000], False, -1, False) <- sort(float3...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         3.74 |         0.03 | copy_(float32[32, 32000], float32[32, 32000], False) <- s...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.73 |         0.01 | copy_(int64[32], int32[32], False) <- _to_copy(int32[32],...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.63 |         0.01 | sub(int64[], int64[32], 1) <- rsub(int64[32], 32000, 1)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.50 |         0.02 | gather(float32[32, 32000], 1, int64[32, 1], False)          
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.77 |         0.03 | lt(float32[32, 32000], float32[32, 1])                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.88 |         0.02 | masked_fill_(float32[32, 32000], bool[32, 32000], -inf)     
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        11.20 |         0.08 | _softmax(float32[32, 32000], -1, False) <- softmax(float3...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        44.58 |         0.31 | cumsum(float32[32, 32000], -1, None)                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.70 |         0.01 | sub(int64[], float16[32, 1], 1) <- rsub(float16[32, 1], 1...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         6.59 |         0.05 | le(float32[32, 32000], float16[32, 1])                      
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.79 |         0.01 | fill_(bool[32], bool[])                                     
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.98 |         0.02 | masked_fill_(float32[32, 32000], bool[32, 32000], -inf)     
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |        28.19 |         0.19 | scatter_(float32[32, 32000], -1, int64[32, 32000], float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        16.67 |         0.11 | _softmax(float32[32, 32000], -1, False) <- softmax(float3...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |         9.70 |         0.07 | _log_softmax(float32[32, 32000], -1, False) <- log_softma...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.98 |         0.01 | copy_(int64[32], int32[32], False) <- _to_copy(int32[32],...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         7.39 |         0.05 | index(float32[32, 32000], None)                             
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         4.80 |         0.03 | exponential_(float32[32, 32000], 1.0, None)                 
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         4.51 |         0.03 | div_(float32[32, 32000], float32[32, 32000])                
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        10.75 |         0.07 | argmax(float32[32, 32000], 1, False)                        
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.18 |         0.01 | copy_(int64[32, 1], int64[32, 1], False) <- _to_copy(int6...

================================================================================
= Prefill Summary Table (prompt_len=250, batch_size=32)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
LlamaForCausalLM                                                                 |    201826.69 |        99.70 |            1.00
|- LlamaModel                                                                    |    201826.69 |        99.70 |            1.00
|-- VocabParallelEmbedding(weight=float16[32000, 4096])                          |       236.25 |         0.12 |            1.00
|--- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half,... |       236.25 |         0.12 |            1.00
|-- LlamaDecoderLayer                                                            |    201422.15 |        99.50 |           32.00
|--- RMSNorm(weight=float16[4096])                                               |     10741.46 |         5.31 |           64.00
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10... |       135.14 |         0.07 |            1.00
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void... |     10606.33 |         5.24 |           63.00
|--- LlamaAttention                                                              |     75590.27 |        37.34 |           32.00
|---- QKVParallelLinear(weight=float16[12288, 4096])                             |     38628.54 |        19.08 |           32.00
|----- Memset (Device)                                                           |        27.81 |         0.01 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |     38600.73 |        19.07 |           32.00
|---- RotaryEmbedding                                                            |      7222.92 |         3.57 |           32.00
|----- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::... |      7222.92 |         3.57 |           32.00
|---- Attention                                                                  |     13421.46 |         6.63 |           32.00
|----- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned sho... |      6289.80 |         3.11 |           32.00
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128, 128, 64, 4, false, ... |      7131.66 |         3.52 |           32.00
|---- RowParallelLinear(weight=float16[4096, 4096])                              |     16317.35 |         8.06 |           32.00
|----- Memset (Device)                                                           |        29.09 |         0.01 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |     16288.26 |         8.05 |           32.00
|--- LlamaMLP                                                                    |    115090.41 |        56.85 |           32.00
|---- MergedColumnParallelLinear(weight=float16[22016, 4096])                    |     71586.95 |        35.36 |           32.00
|----- Memset (Device)                                                           |        27.29 |         0.01 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |     71559.66 |        35.35 |           32.00
|---- SiluAndMul                                                                 |     12890.80 |         6.37 |           32.00
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel... |     12890.80 |         6.37 |           32.00
|---- RowParallelLinear(weight=float16[4096, 11008])                             |     30612.66 |        15.12 |           32.00
|----- Memset (Device)                                                           |        27.58 |         0.01 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x256x64_warpgroupsize... |     30585.08 |        15.11 |           32.00
|-- RMSNorm(weight=float16[4096])                                                |       168.29 |         0.08 |            1.00
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>... |       168.29 |         0.08 |            1.00
LogitsProcessor                                                                  |       216.00 |         0.11 |            1.00
|- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half, l... |         4.67 |         0.00 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x64_warpgroupsize1x1x1... |       211.33 |         0.10 |            1.00
Sampler                                                                          |       397.37 |         0.20 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.78 |         0.01 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         5.41 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         6.75 |         0.00 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |         5.73 |         0.00 |            1.00
|- Memset (Device)                                                               |        13.09 |         0.01 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         9.76 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.76 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |       112.86 |         0.06 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         4.93 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.98 |         0.00 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        17.15 |         0.01 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |        16.00 |         0.01 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         3.78 |         0.00 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.26 |         0.00 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.66 |         0.00 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.37 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         5.15 |         0.00 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         6.24 |         0.00 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        30.88 |         0.02 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        50.27 |         0.02 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.76 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         7.14 |         0.00 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.86 |         0.00 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |        28.25 |         0.01 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        10.50 |         0.01 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         8.29 |         0.00 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         5.21 |         0.00 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         4.80 |         0.00 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        12.19 |         0.01 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.56 |         0.00 |            1.00

================================================================================
= First Decode Step Summary Table (prompt_len=250, batch_size=32)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
CUDAGraphRunner                                                                  |     13969.79 |        96.14 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         6.82 |         0.05 |            5.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<... |         2.34 |         0.02 |            2.00
|- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half, l... |         3.36 |         0.02 |            1.00
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10::H... |         4.25 |         0.03 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64_warpgroupsize1x1x1_... |      5380.10 |        37.02 |           64.00
|- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::Half... |       190.50 |         1.31 |           32.00
|- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned short, ... |       121.86 |         0.84 |           32.00
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<128, 64, 128, 4, fal... |      2932.74 |        20.18 |           32.00
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x... |       832.51 |         5.73 |           32.00
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>::... |       188.42 |         1.30 |           64.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |      4076.54 |        28.05 |           32.00
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel<c10... |       230.37 |         1.59 |           32.00
LogitsProcessor                                                                  |       192.19 |         1.32 |            1.00
|- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half, l... |         4.19 |         0.03 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x64_warpgroupsize1x1x1... |       188.00 |         1.29 |            1.00
Sampler                                                                          |       369.03 |         2.54 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.02 |         0.10 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.96 |         0.03 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         6.21 |         0.04 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |         5.28 |         0.04 |            1.00
|- Memset (Device)                                                               |        12.38 |         0.09 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         9.09 |         0.06 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.82 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |       105.44 |         0.73 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         4.90 |         0.03 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.60 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        15.55 |         0.11 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |        15.52 |         0.11 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         3.74 |         0.03 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.71 |         0.03 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.63 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.50 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         4.77 |         0.03 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         5.86 |         0.04 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        27.87 |         0.19 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        44.58 |         0.31 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.70 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         6.59 |         0.05 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.79 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |        28.19 |         0.19 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |         9.70 |         0.07 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         7.39 |         0.05 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         4.80 |         0.03 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         4.51 |         0.03 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        10.75 |         0.07 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.18 |         0.01 |            1.00
Traces saved as prefill.json and decode_1.json, etc. in folder benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_32/chrome_traces
