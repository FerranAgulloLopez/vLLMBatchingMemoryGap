Run profile with:
  engine_args = {'model': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'served_model_name': None, 'tokenizer': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'skip_tokenizer_init': False, 'tokenizer_mode': 'auto', 'trust_remote_code': False, 'download_dir': None, 'load_format': 'auto', 'config_format': <ConfigFormat.AUTO: 'auto'>, 'dtype': 'auto', 'kv_cache_dtype': 'auto', 'quantization_param_path': None, 'seed': 0, 'max_model_len': None, 'worker_use_ray': False, 'distributed_executor_backend': None, 'pipeline_parallel_size': 1, 'tensor_parallel_size': 1, 'max_parallel_loading_workers': None, 'block_size': 16, 'enable_prefix_caching': False, 'disable_sliding_window': False, 'use_v2_block_manager': False, 'swap_space': 4, 'cpu_offload_gb': 0, 'gpu_memory_utilization': 0.9, 'max_num_batched_tokens': 256000, 'max_num_seqs': 1024, 'max_logprobs': 20, 'disable_log_stats': False, 'revision': None, 'code_revision': None, 'rope_scaling': None, 'rope_theta': None, 'tokenizer_revision': None, 'quantization': None, 'enforce_eager': False, 'max_context_len_to_capture': None, 'max_seq_len_to_capture': 8192, 'disable_custom_all_reduce': False, 'tokenizer_pool_size': 0, 'tokenizer_pool_type': 'ray', 'tokenizer_pool_extra_config': None, 'limit_mm_per_prompt': None, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'enable_prompt_adapter': False, 'max_prompt_adapters': 1, 'max_prompt_adapter_token': 0, 'fully_sharded_loras': False, 'lora_extra_vocab_size': 256, 'long_lora_scaling_factors': None, 'lora_dtype': 'auto', 'max_cpu_loras': None, 'device': 'auto', 'num_scheduler_steps': 1, 'multi_step_stream_outputs': True, 'ray_workers_use_nsight': False, 'num_gpu_blocks_override': None, 'num_lookahead_slots': 0, 'model_loader_extra_config': None, 'ignore_patterns': [], 'preemption_mode': None, 'scheduler_delay_factor': 0.0, 'enable_chunked_prefill': None, 'guided_decoding_backend': 'outlines', 'speculative_model': None, 'speculative_model_quantization': None, 'speculative_draft_tensor_parallel_size': None, 'num_speculative_tokens': None, 'speculative_disable_mqa_scorer': False, 'speculative_max_model_len': None, 'speculative_disable_by_batch_size': None, 'ngram_prompt_lookup_max': None, 'ngram_prompt_lookup_min': None, 'spec_decoding_acceptance_method': 'rejection_sampler', 'typical_acceptance_sampler_posterior_threshold': None, 'typical_acceptance_sampler_posterior_alpha': None, 'qlora_adapter_name_or_path': None, 'disable_logprobs_during_spec_decoding': None, 'otlp_traces_endpoint': None, 'collect_detailed_traces': None, 'disable_async_output_proc': False, 'override_neuron_config': None, 'mm_processor_kwargs': None, 'scheduling_policy': 'fcfs'}
  prompt_len = 250
  output_len = 230
  batch_size = 1
  result_dir = benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_1
  save_chrome_traces_folder = chrome_traces
WARNING 11-22 16:42:36 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 11-22 16:42:36 llm_engine.py:237] Initializing an LLM engine (v0.5.2.dev1210+g6f078ea3c) with config: model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', speculative_config=None, tokenizer='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 11-22 16:42:36 model_runner.py:1056] Starting to load model /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b...
INFO 11-22 16:42:42 model_runner.py:1067] Loading model weights took 12.5523 GB
INFO 11-22 16:42:49 worker.py:262] Memory profiling results: total_gpu_memory=63.43GiB initial_memory_usage=13.15GiB peak_torch_memory=36.15GiB non_torch_memory=0.10GiB kv_cache_size=20.84GiB gpu_memory_utilization=0.90
INFO 11-22 16:42:49 gpu_executor.py:122] # GPU blocks: 2667, # CPU blocks: 512
INFO 11-22 16:42:49 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 10.42x
INFO 11-22 16:42:50 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 16:42:50 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 16:43:21 model_runner.py:1523] Graph capturing finished in 31 secs.
llm.llm_engine.model_config.max_model_len:  4096
Warm up run ...
Profile run ...
INFO 11-22 16:43:26 metrics.py:349] Avg prompt throughput: 99.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:31 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:36 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:41 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
================================================================================
= Prefill Model Table (prompt_len=250, batch_size=1)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
LlamaForCausalLM                                             |     22445.75 |     15277.91 |        97.84 |                                                             
|- LlamaModel                                                |     22424.42 |     15277.91 |        97.84 |                                                             
|-- VocabParallelEmbedding(weight=float16[32000, 4096])      |        71.75 |         9.47 |         0.06 |                                                             
|--- void at::native::(anonymous namespace)::indexSelectL... |         0.00 |         9.47 |         0.06 | index_select(float16[32000, 4096], 0, int64[250]) <- embe...
|-- LlamaDecoderLayer                                        |      2584.13 |       480.29 |         3.08 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        76.01 |         4.70 |         0.03 |                                                             
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c... |         0.00 |         4.70 |         0.03 | _C::rms_norm(float16[250, 4096], float16[250, 4096], floa...
|--- LlamaAttention                                          |      2295.23 |       149.63 |         0.96 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        96.30 |        83.07 |         0.53 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        83.07 |         0.53 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        58.18 |         8.32 |         0.05 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.32 |         0.05 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |      1994.76 |        21.54 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.61 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.93 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        85.64 |        36.70 |         0.24 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        36.70 |         0.24 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        35.18 |         5.15 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.15 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       147.69 |       320.80 |         2.05 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        52.79 |       234.18 |         1.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       233.41 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        41.26 |        10.56 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.56 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        35.76 |        76.06 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        76.06 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       737.63 |       479.36 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        20.35 |         4.96 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.96 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       563.21 |       147.52 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.56 |        81.22 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.22 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        38.84 |         9.28 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         9.28 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       396.62 |        21.50 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.61 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.90 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        51.25 |        35.52 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.52 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        22.57 |         5.18 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.18 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       111.82 |       321.70 |         2.06 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        40.53 |       233.73 |         1.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       232.96 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        28.09 |        10.43 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.43 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.81 |        77.54 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        77.54 |         0.50 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       637.71 |       479.81 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.40 |         4.99 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.99 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       473.57 |       148.03 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        31.73 |        81.54 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.54 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        33.28 |         9.18 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         9.18 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       326.33 |        21.82 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.58 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.25 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.53 |        35.49 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.49 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.02 |         5.06 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.06 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       106.06 |       321.73 |         2.06 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        38.09 |       233.25 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       232.48 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.64 |        10.78 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.78 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.81 |        77.70 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        77.70 |         0.50 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       624.23 |       476.70 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.75 |         5.09 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.09 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       457.99 |       147.07 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.08 |        80.80 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        80.80 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.40 |         8.61 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.61 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       316.84 |        21.54 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.64 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.90 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.03 |        36.13 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        36.13 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.18 |         4.96 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.96 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       116.78 |       319.58 |         2.05 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        49.09 |       233.09 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       232.32 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.72 |         9.60 |         0.06 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |         9.60 |         0.06 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.40 |        76.90 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        76.90 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       634.76 |       479.01 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.35 |         5.09 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.09 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       464.70 |       147.68 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.26 |        81.92 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.92 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        29.73 |         8.67 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.67 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       321.53 |        21.76 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.70 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.06 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.66 |        35.33 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.33 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.85 |         5.47 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.47 |         0.04 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       108.82 |       320.77 |         2.05 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.66 |       233.98 |         1.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       233.22 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.93 |        10.85 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.85 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.62 |        75.94 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.94 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       677.05 |       479.07 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.95 |         5.02 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.02 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       519.53 |       147.17 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        31.45 |        80.64 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        80.64 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        35.63 |         8.90 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.90 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       334.50 |        21.44 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.64 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.80 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        78.63 |        36.19 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        36.19 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.01 |         5.09 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.09 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       105.60 |       321.79 |         2.06 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.67 |       234.62 |         1.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       233.85 |         1.50 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.36 |        10.11 |         0.06 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.11 |         0.06 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.28 |        77.06 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        77.06 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       615.61 |       478.24 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.85 |         5.02 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.02 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       463.08 |       148.26 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.13 |        82.18 |         0.53 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        82.18 |         0.53 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        39.77 |         8.58 |         0.05 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.58 |         0.05 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       316.91 |        21.79 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.58 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.22 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.05 |        35.71 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.71 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.58 |         4.99 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.99 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       103.56 |       319.97 |         2.05 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.62 |       231.26 |         1.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.50 |         1.48 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        27.25 |        11.49 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        11.49 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        28.68 |        77.22 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        77.22 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       620.11 |       478.78 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.50 |         5.02 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.02 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       452.71 |       148.58 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.18 |        81.31 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.31 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.67 |         8.86 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.86 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       311.96 |        21.86 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.64 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.22 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.27 |        36.54 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        36.54 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.65 |         5.09 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.09 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       108.42 |       320.10 |         2.05 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.55 |       232.10 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       231.33 |         1.48 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.81 |        10.21 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.21 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.84 |        77.79 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        77.79 |         0.50 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       613.63 |       475.36 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.44 |         4.99 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.99 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       458.31 |       148.06 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.04 |        82.02 |         0.53 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        82.02 |         0.53 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        36.72 |         8.99 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.99 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       314.15 |        21.50 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.51 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.99 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.15 |        35.55 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.55 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.13 |         5.05 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.05 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       104.12 |       317.25 |         2.03 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.05 |       229.57 |         1.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       228.80 |         1.47 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.89 |        10.37 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.37 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.37 |        77.31 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        77.31 |         0.50 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       614.68 |       476.16 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.60 |         5.06 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.06 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       463.58 |       147.26 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.04 |        81.15 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.15 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.84 |         8.73 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.73 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       330.42 |        21.63 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.58 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.06 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.67 |        35.74 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.74 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.51 |         5.25 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.25 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       101.82 |       318.59 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.37 |       232.45 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       231.68 |         1.48 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.99 |        10.37 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.37 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        28.45 |        75.78 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.78 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       612.59 |       474.46 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.28 |         4.90 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.90 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       449.99 |       148.00 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        27.97 |        81.47 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.47 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.94 |         9.12 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         9.12 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       315.09 |        21.70 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.51 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.18 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.00 |        35.71 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.71 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.90 |         5.09 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.09 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       110.94 |       316.48 |         2.03 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.52 |       229.28 |         1.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       228.51 |         1.46 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        27.24 |        10.43 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.43 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.24 |        76.77 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        76.77 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       636.28 |       478.05 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.93 |         5.25 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.25 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       481.92 |       148.45 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.21 |        81.92 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.92 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        33.17 |         8.64 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.64 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       321.53 |        21.57 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.61 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.96 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.16 |        36.32 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        36.32 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.03 |         4.99 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.99 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       102.94 |       319.36 |         2.05 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.35 |       232.64 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       231.84 |         1.48 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.18 |        10.75 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.75 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.35 |        75.97 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.97 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       608.15 |       479.11 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.92 |         5.12 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.12 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       444.42 |       148.32 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.14 |        81.89 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.89 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        34.53 |         9.25 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         9.25 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       301.58 |        21.76 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.61 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.15 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.34 |        35.42 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.42 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.85 |         5.38 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.38 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       111.06 |       320.29 |         2.05 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.81 |       233.95 |         1.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       233.18 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.83 |        10.91 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.91 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.34 |        75.42 |         0.48 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.42 |         0.48 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       666.39 |       477.31 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.12 |         5.18 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.18 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       465.75 |       148.03 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.12 |        81.38 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.38 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.86 |         8.87 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.87 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       332.10 |        21.60 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.61 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.99 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.12 |        36.19 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        36.19 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.49 |         5.03 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.03 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       151.14 |       319.07 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        75.89 |       233.98 |         1.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       233.18 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        33.60 |        10.02 |         0.06 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.02 |         0.06 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.34 |        75.07 |         0.48 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.07 |         0.48 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       628.77 |       476.42 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.36 |         5.02 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.02 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       469.13 |       147.65 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        35.34 |        82.02 |         0.53 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        82.02 |         0.53 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.57 |         8.58 |         0.05 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.58 |         0.05 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       321.87 |        21.70 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.67 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.02 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.53 |        35.36 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.36 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.82 |         5.22 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.22 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       106.49 |       318.53 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.06 |       229.66 |         1.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       228.90 |         1.47 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.35 |        10.59 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.59 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.70 |        78.27 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        78.27 |         0.50 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       611.07 |       476.70 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.37 |         5.12 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.12 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       451.45 |       147.10 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.49 |        80.93 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        80.93 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.38 |         8.58 |         0.05 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.58 |         0.05 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       313.42 |        21.60 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.61 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.99 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.27 |        36.00 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        36.00 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.24 |         5.28 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.28 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       109.09 |       319.20 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.69 |       232.67 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       231.90 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.58 |        10.37 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.37 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.44 |        76.16 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        76.16 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       592.53 |       474.53 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.79 |         4.96 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.96 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       443.31 |       147.62 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.18 |        82.37 |         0.53 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        82.37 |         0.53 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.77 |         8.51 |         0.05 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.51 |         0.05 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       314.10 |        21.47 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.51 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.96 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        37.49 |        35.26 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.26 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.07 |         5.41 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.41 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       100.64 |       316.54 |         2.03 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.86 |       230.34 |         1.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       229.57 |         1.47 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.68 |        10.27 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.27 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        28.94 |        75.94 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.94 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       625.15 |       477.82 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.23 |         5.09 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.09 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       462.81 |       147.36 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        28.67 |        81.50 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.50 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.06 |         8.80 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.80 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       325.81 |        21.50 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.61 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.90 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        44.28 |        35.55 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.55 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.70 |         5.09 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.09 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       109.59 |       320.29 |         2.05 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        38.92 |       233.54 |         1.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       232.74 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.56 |        10.34 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.34 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.40 |        76.42 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        76.42 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       600.48 |       477.98 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.32 |         5.06 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.06 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       437.35 |       147.58 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.30 |        81.92 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.92 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.47 |         8.58 |         0.05 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.58 |         0.05 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       302.42 |        21.47 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.61 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.86 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.88 |        35.62 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.62 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.41 |         5.02 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.02 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       113.03 |       320.32 |         2.05 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.84 |       233.54 |         1.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       232.77 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        36.89 |        11.23 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        11.23 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.75 |        75.55 |         0.48 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.55 |         0.48 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       587.15 |       475.94 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.36 |         5.25 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.25 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       438.33 |       146.59 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.97 |        80.77 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        80.77 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        29.41 |         8.83 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.83 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       309.56 |        21.63 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.61 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.02 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        37.31 |        35.36 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.36 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        17.97 |         5.22 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.22 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       101.02 |       318.88 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.90 |       234.14 |         1.50 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       233.38 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.28 |         9.76 |         0.06 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |         9.76 |         0.06 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.36 |        74.98 |         0.48 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        74.98 |         0.48 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       642.93 |       475.62 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.23 |         4.99 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.99 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       483.87 |       147.71 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.23 |        81.89 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.89 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.32 |         8.74 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.74 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       345.81 |        21.47 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.51 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.96 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.34 |        35.62 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.62 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.47 |         5.28 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.28 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       107.24 |       317.63 |         2.03 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        38.15 |       231.17 |         1.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.40 |         1.48 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.21 |        10.75 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.75 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.25 |        75.71 |         0.48 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.71 |         0.48 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       648.88 |       476.03 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.19 |         5.22 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.22 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       451.90 |       146.94 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.95 |        80.93 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        80.93 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.08 |         8.77 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.77 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       313.52 |        21.63 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.67 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.96 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.63 |        35.62 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.62 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        22.35 |         5.15 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.15 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       142.29 |       318.72 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.52 |       232.74 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       231.97 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.01 |        10.24 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.24 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        68.69 |        75.74 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.74 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       665.76 |       479.78 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.41 |         4.96 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.96 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       459.81 |       148.42 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.88 |        82.08 |         0.53 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        82.08 |         0.53 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        33.14 |         9.18 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         9.18 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       320.18 |        21.82 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.58 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.25 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.62 |        35.33 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.33 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.06 |         5.31 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.31 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       146.67 |       321.09 |         2.06 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.78 |       232.90 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       232.10 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.46 |        10.75 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.75 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.92 |        77.44 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        77.44 |         0.50 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       611.44 |       476.67 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.70 |         5.09 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.09 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       451.49 |       147.23 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.94 |        80.90 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        80.90 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.07 |         9.41 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         9.41 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       317.66 |        21.50 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.67 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.83 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.67 |        35.42 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.42 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.18 |         5.06 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.06 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       106.04 |       319.30 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.81 |       233.09 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       232.29 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.39 |        10.18 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.18 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.95 |        76.03 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        76.03 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       600.91 |       476.09 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.92 |         4.99 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.99 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       446.82 |       147.81 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.27 |        81.95 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.95 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.40 |         8.83 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.83 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       313.92 |        21.73 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.61 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.12 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.48 |        35.30 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.30 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.21 |         5.15 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.15 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       102.77 |       318.14 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.96 |       231.33 |         1.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.56 |         1.48 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.06 |        10.88 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.88 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.67 |        75.94 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.94 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       612.08 |       475.01 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.70 |         5.03 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.03 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       457.91 |       146.40 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.43 |        80.45 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        80.45 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.18 |         8.74 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.74 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       310.31 |        21.47 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.54 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.93 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.57 |        35.74 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.74 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.63 |         5.25 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.25 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       105.15 |       318.34 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.64 |       233.28 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       232.51 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.95 |        10.05 |         0.06 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.05 |         0.06 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.74 |        75.01 |         0.48 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.01 |         0.48 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       612.81 |       474.69 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.20 |         4.99 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.99 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       447.60 |       148.03 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.05 |        81.98 |         0.53 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.98 |         0.53 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.99 |         8.67 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.67 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       314.13 |        21.79 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.58 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.22 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.31 |        35.58 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.58 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.24 |         5.12 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.12 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       108.02 |       316.54 |         2.03 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        38.23 |       230.43 |         1.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       229.66 |         1.47 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.24 |        10.62 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.62 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.49 |        75.49 |         0.48 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.49 |         0.48 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       602.47 |       475.23 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.41 |         5.06 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.06 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       447.04 |       147.01 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        30.33 |        80.86 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        80.86 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        34.51 |         8.64 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.64 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       311.41 |        21.60 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.58 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.02 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.09 |        35.90 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.90 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.16 |         4.96 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.96 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       104.23 |       318.21 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.18 |       232.90 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       232.13 |         1.49 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.14 |        10.27 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.27 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.73 |        75.04 |         0.48 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.04 |         0.48 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       604.09 |       473.92 |         3.03 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.87 |         4.93 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.93 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       448.96 |       147.49 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.61 |        81.41 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.41 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        34.84 |         8.96 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.96 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       312.53 |        21.82 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.58 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        17.25 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.18 |        35.30 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.30 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.53 |         5.18 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.18 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       105.63 |       316.32 |         2.03 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.35 |       230.53 |         1.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       229.76 |         1.47 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.49 |        10.21 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.21 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.47 |        75.58 |         0.48 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.58 |         0.48 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       628.55 |       476.13 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.62 |         5.22 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.22 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       473.64 |       147.07 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.99 |        81.28 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.28 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        33.73 |         8.77 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.77 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       326.59 |        21.47 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.54 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.93 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.83 |        35.55 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.55 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.91 |         5.06 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.06 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       104.83 |       318.79 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.21 |       232.39 |         1.49 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       231.62 |         1.48 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.11 |        10.85 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.85 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        33.41 |        75.55 |         0.48 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        75.55 |         0.48 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       643.30 |       477.50 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        21.77 |         4.90 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         4.90 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       485.30 |       148.29 |         0.95 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        29.48 |        81.98 |         0.53 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        81.98 |         0.53 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.05 |         8.86 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.86 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       317.07 |        21.50 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.51 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.99 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.98 |        35.94 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.94 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.15 |         5.06 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.06 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       103.03 |       319.26 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.55 |       230.08 |         1.47 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       229.31 |         1.47 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.42 |        10.69 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.69 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.45 |        78.50 |         0.50 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        78.50 |         0.50 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       597.40 |       475.68 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.13 |         5.22 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.22 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaAttention                                          |       447.70 |       146.59 |         0.94 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        28.59 |        80.83 |         0.52 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        80.83 |         0.52 | mm(float16[250, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        37.21 |         8.80 |         0.06 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |         8.80 |         0.06 | _C::rotary_embedding(int64[250], float16[250, 4096], floa...
|---- Attention                                              |       314.24 |        21.54 |         0.14 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         4.58 |         0.03 | _C_cache_ops::reshape_and_cache_flash(float16[250, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        16.96 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[250, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.42 |        35.42 |         0.23 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        35.42 |         0.23 | mm(float16[250, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.36 |         5.09 |         0.03 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         5.09 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
|--- LlamaMLP                                                |       101.95 |       318.78 |         2.04 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.87 |       231.71 |         1.48 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       230.94 |         1.48 | mm(float16[250, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.06 |        10.37 |         0.07 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        10.37 |         0.07 | _C::silu_and_mul(float16[250, 11008], float16[250, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        28.67 |        76.70 |         0.49 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x1... |         0.00 |        76.70 |         0.49 | mm(float16[250, 11008], float16[11008, 4096]) <- matmul(f...
|-- RMSNorm(weight=float16[4096])                            |        15.87 |         4.99 |         0.03 |                                                             
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::... |         0.00 |         4.99 |         0.03 | _C::fused_add_rms_norm(float16[250, 4096], float16[250, 4...
LogitsProcessor                                              |       103.60 |       190.11 |         1.22 |                                                             
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         2.56 |         0.02 | index_select(float16[250, 4096], 0, int64[1])               
|- Memset (Device)                                           |         0.00 |         0.77 |         0.00 | mm(float16[1, 4096], float16[4096, 32000]) <- matmul(floa...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       186.78 |         1.20 | mm(float16[1, 4096], float16[4096, 32000]) <- matmul(floa...
Sampler                                                      |      2705.32 |       147.93 |         0.95 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.77 |         0.00 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.77 |         0.00 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.80 |         0.01 | copy_(int32[1], int32[1], True) <- _to_copy(int32[1], 3, ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.77 |         0.00 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.77 |         0.00 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.70 |         0.00 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.70 |         0.00 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         3.81 |         0.02 | copy_(float32[1, 32000], float16[1, 32000], False) <- _to...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.81 |         0.02 | div_(float32[1, 32000], float16[1, 1])                      
|- at::native::(anonymous namespace)::fill_reverse_indice... |         0.00 |         1.92 |         0.01 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.74 |         0.00 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.63 |         0.01 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         3.74 |         0.02 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.66 |         0.01 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.01 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         8.48 |         0.05 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         7.87 |         0.05 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.28 |         0.01 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         7.62 |         0.05 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         7.87 |         0.05 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.60 |         0.01 | copy_(float32[1, 32000], float32[1, 32000], False) <- sor...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.76 |         0.01 | copy_(int64[1], int32[1], False) <- _to_copy(int32[1], 4,...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.44 |         0.01 | sub(int64[], int64[1], 1) <- rsub(int64[1], 32000, 1)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.24 |         0.01 | gather(float32[1, 32000], 1, int64[1, 1], False)            
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.23 |         0.02 | lt(float32[1, 32000], float32[1, 1])                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.70 |         0.01 | masked_fill_(float32[1, 32000], bool[1, 32000], -inf)       
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.53 |         0.07 | _softmax(float32[1, 32000], -1, False) <- softmax(float32...
|- void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda... |         0.00 |         1.50 |         0.01 | cumsum(float32[1, 32000], -1, None)                         
|- void at_cuda_detail::cub::DeviceScanKernel<at_cuda_det... |         0.00 |         2.85 |         0.02 | cumsum(float32[1, 32000], -1, None)                         
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.54 |         0.01 | sub(int64[], float16[1, 1], 1) <- rsub(float16[1, 1], 1, 1) 
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.00 |         0.03 | le(float32[1, 32000], float16[1, 1])                        
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.28 |         0.01 | fill_(bool[1], bool[])                                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.66 |         0.01 | masked_fill_(float32[1, 32000], bool[1, 32000], -inf)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         4.16 |         0.03 | scatter_(float32[1, 32000], -1, int64[1, 32000], float32[...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        15.90 |         0.10 | _softmax(float32[1, 32000], -1, False) <- softmax(float32...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |         8.22 |         0.05 | _log_softmax(float32[1, 32000], -1, False) <- log_softmax...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.70 |         0.01 | copy_(int64[1], int32[1], False) <- _to_copy(int32[1], 4,...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         4.61 |         0.03 | index(float32[1, 32000], None)                              
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         2.02 |         0.01 | exponential_(float32[1, 32000], 1.0, None)                  
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.17 |         0.01 | div_(float32[1, 32000], float32[1, 32000])                  
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        11.10 |         0.07 | argmax(float32[1, 32000], 1, False)                         
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.56 |         0.02 | copy_(int64[1, 1], int64[1, 1], False) <- _to_copy(int64[...

================================================================================
= First Decode Step Model Table (prompt_len=250, batch_size=1)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
CUDAGraphRunner                                              |       610.49 |      8612.64 |        96.17 |                                                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.44 |         0.02 | copy_(int64[1], int64[1], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.38 |         0.02 | copy_(int64[1], int64[1], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.60 |         0.02 | copy_(int64[1], int64[1], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.38 |         0.02 | copy_(int32[1], int32[1], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.28 |         0.01 | copy_(int32[1, 256], int32[1, 256], True)                   
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.12 |         0.01 | fill_(int64[1], 0)                                          
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.25 |         0.01 | fill_(int64[1], 12)                                         
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         2.37 |         0.03 |                                                             
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10:... |         0.00 |         4.32 |         0.05 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.29 |         0.05 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.26 |         0.04 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |         8.74 |         0.10 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         4.51 |         0.05 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        26.14 |         0.29 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        72.67 |         0.81 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         6.88 |         0.08 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64... |         0.00 |        63.65 |         0.71 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.91 |         0.03 |                                                             
LogitsProcessor                                              |       229.46 |       185.09 |         2.07 |                                                             
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         3.07 |         0.03 | index_select(float16[1, 4096], 0, int64[1])                 
|- Memset (Device)                                           |         0.00 |         0.77 |         0.01 | mm(float16[1, 4096], float16[4096, 32000]) <- matmul(floa...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       181.25 |         2.02 | mm(float16[1, 4096], float16[4096, 32000]) <- matmul(floa...
Sampler                                                      |      9452.16 |       157.82 |         1.76 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.33 |         0.03 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.02 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.30 |         0.03 | copy_(int32[1], int32[1], True) <- _to_copy(int32[1], 3, ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.02 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.11 |         0.02 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.02 |         0.02 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.02 | copy_(float16[1], float16[1], True) <- _to_copy(float16[1...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         4.07 |         0.05 | copy_(float32[1, 32000], float16[1, 32000], False) <- _to...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.81 |         0.04 | div_(float32[1, 32000], float16[1, 1])                      
|- at::native::(anonymous namespace)::fill_reverse_indice... |         0.00 |         1.89 |         0.02 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.02 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         3.68 |         0.04 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.86 |         0.02 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.02 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         8.26 |         0.09 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.02 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         8.16 |         0.09 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.31 |         0.01 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         8.00 |         0.09 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.02 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         7.52 |         0.08 | sort(float32[1, 32000], False, -1, False) <- sort(float32...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.57 |         0.02 | copy_(float32[1, 32000], float32[1, 32000], False) <- sor...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.70 |         0.02 | copy_(int64[1], int32[1], False) <- _to_copy(int32[1], 4,...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.28 |         0.01 | sub(int64[], int64[1], 1) <- rsub(int64[1], 32000, 1)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.21 |         0.02 | gather(float32[1, 32000], 1, int64[1, 1], False)            
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.17 |         0.04 | lt(float32[1, 32000], float32[1, 1])                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.70 |         0.02 | masked_fill_(float32[1, 32000], bool[1, 32000], -inf)       
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.46 |         0.12 | _softmax(float32[1, 32000], -1, False) <- softmax(float32...
|- void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda... |         0.00 |         1.54 |         0.02 | cumsum(float32[1, 32000], -1, None)                         
|- void at_cuda_detail::cub::DeviceScanKernel<at_cuda_det... |         0.00 |         3.07 |         0.03 | cumsum(float32[1, 32000], -1, None)                         
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.57 |         0.02 | sub(int64[], float16[1, 1], 1) <- rsub(float16[1, 1], 1, 1) 
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.19 |         0.05 | le(float32[1, 32000], float16[1, 1])                        
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.28 |         0.01 | fill_(bool[1], bool[])                                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.63 |         0.02 | masked_fill_(float32[1, 32000], bool[1, 32000], -inf)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         4.09 |         0.05 | scatter_(float32[1, 32000], -1, int64[1, 32000], float32[...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        15.74 |         0.18 | _softmax(float32[1, 32000], -1, False) <- softmax(float32...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |         8.29 |         0.09 | _log_softmax(float32[1, 32000], -1, False) <- log_softmax...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.92 |         0.02 | copy_(int64[1], int32[1], False) <- _to_copy(int32[1], 4,...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         4.61 |         0.05 | index(float32[1, 32000], None)                              
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         2.05 |         0.02 | exponential_(float32[1, 32000], 1.0, None)                  
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.14 |         0.02 | div_(float32[1, 32000], float32[1, 32000])                  
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        11.17 |         0.12 | argmax(float32[1, 32000], 1, False)                         
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.40 |         0.03 | copy_(int64[1, 1], int64[1, 1], False) <- _to_copy(int64[...

================================================================================
= Prefill Summary Table (prompt_len=250, batch_size=1)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
LlamaForCausalLM                                                                 |     15277.91 |        97.84 |            1.00
|- LlamaModel                                                                    |     15277.91 |        97.84 |            1.00
|-- VocabParallelEmbedding(weight=float16[32000, 4096])                          |         9.47 |         0.06 |            1.00
|--- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half,... |         9.47 |         0.06 |            1.00
|-- LlamaDecoderLayer                                                            |     15263.45 |        97.74 |           32.00
|--- RMSNorm(weight=float16[4096])                                               |       326.14 |         2.09 |           64.00
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10... |         4.70 |         0.03 |            1.00
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void... |       321.44 |         2.06 |           63.00
|--- LlamaAttention                                                              |      4724.96 |        30.26 |           32.00
|---- QKVParallelLinear(weight=float16[12288, 4096])                             |      2608.54 |        16.70 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |      2608.54 |        16.70 |           32.00
|---- RotaryEmbedding                                                            |       282.24 |         1.81 |           32.00
|----- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::... |       282.24 |         1.81 |           32.00
|---- Attention                                                                  |       691.74 |         4.43 |           32.00
|----- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned sho... |       146.97 |         0.94 |           32.00
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128, 128, 64, 4, false, ... |       544.77 |         3.49 |           32.00
|---- RowParallelLinear(weight=float16[4096, 4096])                              |      1142.43 |         7.32 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1... |      1142.43 |         7.32 |           32.00
|--- LlamaMLP                                                                    |     10212.35 |        65.40 |           32.00
|---- MergedColumnParallelLinear(weight=float16[22016, 4096])                    |      7434.49 |        47.61 |           32.00
|----- Memset (Device)                                                           |        24.74 |         0.16 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |      7409.76 |        47.45 |           32.00
|---- SiluAndMul                                                                 |       335.29 |         2.15 |           32.00
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel... |       335.29 |         2.15 |           32.00
|---- RowParallelLinear(weight=float16[4096, 11008])                             |      2442.56 |        15.64 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1... |      2442.56 |        15.64 |           32.00
|-- RMSNorm(weight=float16[4096])                                                |         4.99 |         0.03 |            1.00
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>... |         4.99 |         0.03 |            1.00
LogitsProcessor                                                                  |       190.11 |         1.22 |            1.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         2.56 |         0.02 |            1.00
|- Memset (Device)                                                               |         0.77 |         0.00 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |       186.78 |         1.20 |            1.00
Sampler                                                                          |       147.93 |         0.95 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |         5.28 |         0.03 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.81 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         3.81 |         0.02 |            1.00
|- at::native::(anonymous namespace)::fill_reverse_indices_kernel(long*, int,... |         1.92 |         0.01 |            1.00
|- Memset (Device)                                                               |         8.10 |         0.05 |            6.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         3.74 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.66 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        31.84 |         0.20 |            4.00
|- Memcpy DtoD (Device -> Device)                                                |         1.60 |         0.01 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.46 |         0.02 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.44 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.24 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         3.23 |         0.02 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         3.36 |         0.02 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        26.43 |         0.17 |            2.00
|- void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ScanTi... |         1.50 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceScanKernel<at_cuda_detail::cub::DeviceScan... |         2.85 |         0.02 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.54 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.00 |         0.03 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::FillFunctor<bool>... |         1.28 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         4.16 |         0.03 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |         8.22 |         0.05 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         4.61 |         0.03 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         2.02 |         0.01 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         2.17 |         0.01 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        11.10 |         0.07 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.56 |         0.02 |            1.00

================================================================================
= First Decode Step Summary Table (prompt_len=250, batch_size=1)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
CUDAGraphRunner                                                                  |      8612.64 |        96.17 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         7.07 |         0.08 |            5.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<... |         2.37 |         0.03 |            2.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         2.37 |         0.03 |            1.00
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10::H... |         4.32 |         0.05 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |      4651.01 |        51.93 |           64.00
|- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::Half... |       137.22 |         1.53 |           32.00
|- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned short, ... |       104.45 |         1.17 |           32.00
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<128, 64, 128, 4, fal... |       279.55 |         3.12 |           32.00
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel_traits<128, 64, 128... |       144.38 |         1.61 |           32.00
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x... |       836.61 |         9.34 |           32.00
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>::... |       186.37 |         2.08 |           64.00
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel<c10... |       220.16 |         2.46 |           32.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x64x64_warpgroupsize1x1x1_... |      2036.77 |        22.74 |           32.00
LogitsProcessor                                                                  |       185.09 |         2.07 |            1.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         3.07 |         0.03 |            1.00
|- Memset (Device)                                                               |         0.77 |         0.01 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |       181.25 |         2.02 |            1.00
Sampler                                                                          |       157.82 |         1.76 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.78 |         0.17 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.07 |         0.05 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         3.81 |         0.04 |            1.00
|- at::native::(anonymous namespace)::fill_reverse_indices_kernel(long*, int,... |         1.89 |         0.02 |            1.00
|- Memset (Device)                                                               |         8.03 |         0.09 |            6.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         3.68 |         0.04 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.86 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        31.94 |         0.36 |            4.00
|- Memcpy DtoD (Device -> Device)                                                |         1.57 |         0.02 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.62 |         0.04 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.28 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.21 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         3.17 |         0.04 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         3.33 |         0.04 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        26.21 |         0.29 |            2.00
|- void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ScanTi... |         1.54 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceScanKernel<at_cuda_detail::cub::DeviceScan... |         3.07 |         0.03 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.57 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.19 |         0.05 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::FillFunctor<bool>... |         1.28 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         4.09 |         0.05 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |         8.29 |         0.09 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         4.61 |         0.05 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         2.05 |         0.02 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         2.14 |         0.02 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        11.17 |         0.12 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.40 |         0.03 |            1.00
Traces saved as prefill.json and decode_1.json, etc. in folder benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_1/chrome_traces
