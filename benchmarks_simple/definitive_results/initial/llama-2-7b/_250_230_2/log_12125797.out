Run profile with:
  engine_args = {'model': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'served_model_name': None, 'tokenizer': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'skip_tokenizer_init': False, 'tokenizer_mode': 'auto', 'trust_remote_code': False, 'download_dir': None, 'load_format': 'auto', 'config_format': <ConfigFormat.AUTO: 'auto'>, 'dtype': 'auto', 'kv_cache_dtype': 'auto', 'quantization_param_path': None, 'seed': 0, 'max_model_len': None, 'worker_use_ray': False, 'distributed_executor_backend': None, 'pipeline_parallel_size': 1, 'tensor_parallel_size': 1, 'max_parallel_loading_workers': None, 'block_size': 16, 'enable_prefix_caching': False, 'disable_sliding_window': False, 'use_v2_block_manager': False, 'swap_space': 4, 'cpu_offload_gb': 0, 'gpu_memory_utilization': 0.9, 'max_num_batched_tokens': 256000, 'max_num_seqs': 1024, 'max_logprobs': 20, 'disable_log_stats': False, 'revision': None, 'code_revision': None, 'rope_scaling': None, 'rope_theta': None, 'tokenizer_revision': None, 'quantization': None, 'enforce_eager': False, 'max_context_len_to_capture': None, 'max_seq_len_to_capture': 8192, 'disable_custom_all_reduce': False, 'tokenizer_pool_size': 0, 'tokenizer_pool_type': 'ray', 'tokenizer_pool_extra_config': None, 'limit_mm_per_prompt': None, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'enable_prompt_adapter': False, 'max_prompt_adapters': 1, 'max_prompt_adapter_token': 0, 'fully_sharded_loras': False, 'lora_extra_vocab_size': 256, 'long_lora_scaling_factors': None, 'lora_dtype': 'auto', 'max_cpu_loras': None, 'device': 'auto', 'num_scheduler_steps': 1, 'multi_step_stream_outputs': True, 'ray_workers_use_nsight': False, 'num_gpu_blocks_override': None, 'num_lookahead_slots': 0, 'model_loader_extra_config': None, 'ignore_patterns': [], 'preemption_mode': None, 'scheduler_delay_factor': 0.0, 'enable_chunked_prefill': None, 'guided_decoding_backend': 'outlines', 'speculative_model': None, 'speculative_model_quantization': None, 'speculative_draft_tensor_parallel_size': None, 'num_speculative_tokens': None, 'speculative_disable_mqa_scorer': False, 'speculative_max_model_len': None, 'speculative_disable_by_batch_size': None, 'ngram_prompt_lookup_max': None, 'ngram_prompt_lookup_min': None, 'spec_decoding_acceptance_method': 'rejection_sampler', 'typical_acceptance_sampler_posterior_threshold': None, 'typical_acceptance_sampler_posterior_alpha': None, 'qlora_adapter_name_or_path': None, 'disable_logprobs_during_spec_decoding': None, 'otlp_traces_endpoint': None, 'collect_detailed_traces': None, 'disable_async_output_proc': False, 'override_neuron_config': None, 'mm_processor_kwargs': None, 'scheduling_policy': 'fcfs'}
  prompt_len = 250
  output_len = 230
  batch_size = 2
  result_dir = benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_2
  save_chrome_traces_folder = chrome_traces
WARNING 11-22 16:42:36 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 11-22 16:42:36 llm_engine.py:237] Initializing an LLM engine (v0.5.2.dev1210+g6f078ea3c) with config: model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', speculative_config=None, tokenizer='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 11-22 16:42:36 model_runner.py:1056] Starting to load model /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b...
INFO 11-22 16:42:54 model_runner.py:1067] Loading model weights took 12.5523 GB
INFO 11-22 16:43:01 worker.py:262] Memory profiling results: total_gpu_memory=63.43GiB initial_memory_usage=13.15GiB peak_torch_memory=36.15GiB non_torch_memory=0.10GiB kv_cache_size=20.84GiB gpu_memory_utilization=0.90
INFO 11-22 16:43:02 gpu_executor.py:122] # GPU blocks: 2667, # CPU blocks: 512
INFO 11-22 16:43:02 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 10.42x
INFO 11-22 16:43:03 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 16:43:03 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 16:43:33 model_runner.py:1523] Graph capturing finished in 31 secs.
llm.llm_engine.model_config.max_model_len:  4096
Warm up run ...
Profile run ...
INFO 11-22 16:43:39 metrics.py:349] Avg prompt throughput: 196.4 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:44 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:49 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:54 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 11-22 16:43:59 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
================================================================================
= Prefill Model Table (prompt_len=250, batch_size=2)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
LlamaForCausalLM                                             |     22476.83 |     15459.32 |        97.48 |                                                             
|- LlamaModel                                                |     22453.71 |     15459.32 |        97.48 |                                                             
|-- VocabParallelEmbedding(weight=float16[32000, 4096])      |        73.30 |        16.07 |         0.10 |                                                             
|--- void at::native::(anonymous namespace)::indexSelectL... |         0.00 |        16.07 |         0.10 | index_select(float16[32000, 4096], 0, int64[500]) <- embe...
|-- LlamaDecoderLayer                                        |      2632.20 |       506.17 |         3.19 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        73.15 |         7.97 |         0.05 |                                                             
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c... |         0.00 |         7.97 |         0.05 | _C::rms_norm(float16[500, 4096], float16[500, 4096], floa...
|--- LlamaAttention                                          |      2337.90 |       209.50 |         1.32 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        99.61 |       125.70 |         0.79 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       124.93 |         0.79 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        61.47 |        12.61 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.61 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |      2020.75 |        27.62 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         9.09 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.53 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        90.30 |        43.58 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        43.58 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        37.41 |         7.10 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.10 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       151.98 |       281.60 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        49.61 |       163.17 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       162.40 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        44.16 |        23.68 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.68 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        36.81 |        94.75 |         0.60 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        94.75 |         0.60 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       721.58 |       482.56 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        19.58 |         6.82 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         6.82 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       547.14 |       187.84 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.97 |       105.67 |         0.67 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.90 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        41.99 |        12.80 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.80 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       378.21 |        26.62 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.51 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.11 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        46.13 |        42.75 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.75 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        23.25 |         7.14 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.14 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       110.82 |       280.77 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        39.13 |       162.18 |         1.02 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       161.38 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.83 |        23.55 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.55 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.47 |        95.04 |         0.60 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        95.04 |         0.60 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       665.84 |       483.14 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        22.02 |         7.36 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.36 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       485.42 |       186.21 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        35.32 |       104.22 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.42 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        33.28 |        12.38 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.38 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       340.58 |        27.39 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.67 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.72 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.21 |        42.21 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.21 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.46 |         6.78 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         6.78 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       109.05 |       282.78 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.34 |       162.59 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       161.82 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.02 |        23.52 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.52 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.22 |        96.67 |         0.61 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        96.67 |         0.61 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       622.72 |       474.95 |         2.99 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.96 |         7.07 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.07 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       457.32 |       188.42 |         1.19 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.43 |       105.57 |         0.67 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.80 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.44 |        12.45 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.45 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       318.85 |        26.79 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.38 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.40 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.59 |        43.62 |         0.28 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        43.62 |         0.28 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.51 |         7.14 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.14 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       112.96 |       272.32 |         1.72 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        41.37 |       161.79 |         1.02 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       160.99 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.68 |        23.68 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.68 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.18 |        86.85 |         0.55 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        86.85 |         0.55 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       636.45 |       487.46 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.36 |         7.26 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.26 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       459.52 |       187.55 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.74 |       105.79 |         0.67 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       105.03 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        33.66 |        12.93 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.93 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       314.68 |        26.94 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.61 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.34 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.85 |        41.89 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.89 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.91 |         7.17 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.17 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       122.33 |       285.47 |         1.80 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        38.00 |       164.26 |         1.04 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       163.46 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        31.13 |        23.87 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.87 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.45 |        97.34 |         0.61 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        97.34 |         0.61 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       692.59 |       483.75 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.18 |         7.65 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.65 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       532.58 |       188.13 |         1.19 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.69 |       105.41 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.64 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.56 |        12.58 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.58 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       344.58 |        26.98 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.67 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.30 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        77.97 |        43.17 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        43.17 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.94 |         7.17 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.17 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       105.78 |       280.80 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.70 |       163.97 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       163.20 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.96 |        23.71 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.71 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.07 |        93.12 |         0.59 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        93.12 |         0.59 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       629.35 |       485.60 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.49 |         7.46 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.46 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       469.88 |       186.53 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        35.80 |       104.38 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.58 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.09 |        12.64 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.64 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       328.38 |        27.71 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.61 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        19.10 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.55 |        41.79 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.79 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.10 |         7.14 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.14 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       109.71 |       284.48 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.40 |       163.71 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       162.91 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        31.78 |        24.16 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        24.16 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.34 |        96.61 |         0.61 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        96.61 |         0.61 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       622.07 |       482.75 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.12 |         7.10 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.10 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       462.01 |       189.95 |         1.20 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        35.12 |       106.02 |         0.67 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       105.22 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.86 |        12.26 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.26 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       319.95 |        27.33 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.77 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.56 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.66 |        44.35 |         0.28 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        44.35 |         0.28 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        22.01 |         7.14 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.14 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       104.63 |       278.56 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.68 |       162.98 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       162.18 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.97 |        24.00 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        24.00 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.82 |        91.58 |         0.58 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        91.58 |         0.58 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       608.80 |       483.62 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.61 |         7.49 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.49 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       449.54 |       185.86 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.12 |       103.90 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.14 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        34.96 |        12.61 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.61 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       308.42 |        27.81 |         0.18 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.96 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.85 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.92 |        41.54 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.54 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.66 |         7.14 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.14 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       106.89 |       283.14 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.84 |       161.82 |         1.02 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       161.02 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.06 |        23.94 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.94 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.09 |        97.38 |         0.61 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        97.38 |         0.61 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       649.25 |       484.06 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.77 |         7.20 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.20 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       493.08 |       189.89 |         1.20 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.97 |       106.66 |         0.67 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       105.86 |         0.67 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.86 |        12.96 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.96 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       348.54 |        27.30 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.99 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.30 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        42.04 |        42.98 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.98 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.33 |         7.17 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.17 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       104.70 |       279.81 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.15 |       163.33 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       162.56 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.41 |        23.33 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.33 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.27 |        93.15 |         0.59 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        93.15 |         0.59 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       604.45 |       484.03 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.33 |         7.36 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.36 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       453.56 |       186.27 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.73 |       105.25 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.48 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.74 |        12.38 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.38 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       317.04 |        27.14 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.64 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.50 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.13 |        41.50 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.50 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.84 |         7.26 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.26 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       101.45 |       283.14 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.42 |       162.37 |         1.02 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       161.60 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.55 |        24.03 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        24.03 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.50 |        96.74 |         0.61 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        96.74 |         0.61 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       650.87 |       480.42 |         3.03 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.85 |         7.13 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.13 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       485.86 |       188.45 |         1.19 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        35.33 |       105.38 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.58 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.12 |        12.51 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.51 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       320.85 |        26.82 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.22 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.59 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.94 |        43.74 |         0.28 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        43.74 |         0.28 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        25.68 |         7.10 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.10 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       107.06 |       277.73 |         1.75 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.43 |       163.90 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       163.14 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.44 |        23.33 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.33 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.87 |        90.50 |         0.57 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        90.50 |         0.57 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       607.62 |       483.26 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.68 |         7.52 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.52 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       443.51 |       186.27 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.41 |       104.16 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.39 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.82 |        12.51 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.51 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       309.94 |        27.23 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.83 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.40 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.16 |        42.37 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.37 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.40 |         7.33 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.33 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       110.35 |       282.14 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        42.85 |       162.40 |         1.02 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       161.60 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.09 |        23.49 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.49 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.15 |        96.26 |         0.61 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        96.26 |         0.61 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       641.03 |       481.09 |         3.03 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.99 |         7.36 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.36 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       447.45 |       188.16 |         1.19 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.32 |       105.66 |         0.67 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.90 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.67 |        12.48 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.48 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       313.57 |        26.95 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.58 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.37 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.53 |        43.07 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        43.07 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.70 |         7.20 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.20 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       143.81 |       278.37 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        74.54 |       162.08 |         1.02 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       161.31 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.63 |        23.26 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.26 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.54 |        93.03 |         0.59 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        93.03 |         0.59 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       612.32 |       478.66 |         3.02 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.54 |         7.30 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.30 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       445.76 |       186.56 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.65 |       104.29 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.49 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.00 |        12.73 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.73 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       305.32 |        27.36 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.67 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.69 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        45.09 |        42.18 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.18 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.67 |         7.14 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.14 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       111.27 |       277.66 |         1.75 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.05 |       164.42 |         1.04 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       163.65 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        29.91 |        23.97 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.97 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.01 |        89.28 |         0.56 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        89.28 |         0.56 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       601.18 |       480.00 |         3.03 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.42 |         7.20 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.20 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       445.16 |       189.06 |         1.19 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.36 |       105.44 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.67 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.59 |        12.61 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.61 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       301.17 |        27.49 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.80 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.69 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.76 |        43.52 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        43.52 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.60 |         7.07 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.07 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       103.07 |       276.67 |         1.74 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.80 |       163.81 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       163.04 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.44 |        23.65 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.65 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.20 |        89.22 |         0.56 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        89.22 |         0.56 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       589.78 |       481.18 |         3.03 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.14 |         7.39 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.39 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       438.30 |       186.43 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.57 |       104.64 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.87 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.88 |        12.22 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.22 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       296.88 |        27.39 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.58 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.82 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.21 |        42.18 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.18 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        17.74 |         7.33 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.33 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       102.58 |       280.03 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.81 |       162.21 |         1.02 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       161.44 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.88 |        23.84 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.84 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.37 |        93.98 |         0.59 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        93.98 |         0.59 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       629.99 |       474.78 |         2.99 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.68 |         7.04 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.04 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       466.02 |       186.37 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.41 |       103.87 |         0.65 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.11 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.37 |        12.70 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.70 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       320.84 |        26.94 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.67 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.27 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.40 |        42.85 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.85 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.77 |         7.01 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.01 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       107.54 |       274.37 |         1.73 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.44 |       162.66 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       161.89 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.27 |        23.30 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.30 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.98 |        88.42 |         0.56 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        88.42 |         0.56 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       596.81 |       484.80 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.50 |         7.42 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.42 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       443.84 |       186.75 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.87 |       104.06 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.27 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        36.10 |        12.74 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.74 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       299.81 |        27.17 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.22 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.94 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.51 |        42.78 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.78 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.93 |         7.04 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.04 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       101.03 |       283.58 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.82 |       164.00 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       163.23 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.60 |        23.68 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.68 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.43 |        95.90 |         0.60 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        95.90 |         0.60 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       582.54 |       481.54 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.33 |         7.33 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.33 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       431.14 |       187.81 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        31.82 |       105.60 |         0.67 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.80 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.45 |        12.67 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.67 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       298.20 |        27.55 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.93 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.62 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.19 |        41.98 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.98 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        17.60 |         7.07 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.07 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       102.74 |       279.33 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.38 |       164.10 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       163.33 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.46 |        23.65 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.65 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.30 |        91.58 |         0.58 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        91.58 |         0.58 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       646.71 |       478.27 |         3.02 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.59 |         7.30 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.30 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       484.27 |       186.05 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.23 |       104.19 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.42 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.69 |        12.64 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.64 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       345.01 |        27.59 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.96 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.62 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.04 |        41.63 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.63 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.57 |         7.46 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.46 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       110.59 |       277.47 |         1.75 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.49 |       164.64 |         1.04 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       163.87 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        29.42 |        23.46 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.46 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        30.73 |        89.38 |         0.56 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        89.38 |         0.56 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       637.04 |       483.68 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        20.21 |         7.14 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.14 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       441.99 |       189.66 |         1.20 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.20 |       106.53 |         0.67 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       105.76 |         0.67 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.94 |        12.86 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.86 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       300.06 |        27.52 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.70 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.82 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.48 |        42.75 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.75 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.57 |         7.11 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.11 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       137.21 |       279.78 |         1.76 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.69 |       164.74 |         1.04 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       163.97 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.38 |        23.20 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.20 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        65.30 |        91.84 |         0.58 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        91.84 |         0.58 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       637.10 |       484.71 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        14.94 |         7.33 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.33 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       437.99 |       186.43 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.29 |       105.22 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.45 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        29.96 |        12.58 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.58 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       301.79 |        26.82 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.19 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.62 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        41.94 |        41.83 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.83 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.90 |         7.26 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.26 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       148.88 |       283.68 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.87 |       165.44 |         1.04 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       164.64 |         1.04 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.69 |        23.97 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.97 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.52 |        94.27 |         0.59 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        94.27 |         0.59 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       599.83 |       475.77 |         3.00 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.94 |         7.04 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.04 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       447.25 |       186.43 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.49 |       105.38 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.61 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.26 |        12.06 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.06 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       309.03 |        27.07 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.86 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.21 |         0.11 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.37 |        41.92 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.92 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.19 |         7.17 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.17 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       102.02 |       275.14 |         1.73 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.29 |       163.49 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       162.69 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.68 |        23.52 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.52 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.26 |        88.13 |         0.56 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        88.13 |         0.56 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       614.80 |       483.20 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.29 |         7.46 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.46 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       465.38 |       188.22 |         1.19 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        38.32 |       104.77 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.00 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        31.96 |        13.06 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        13.06 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       324.50 |        27.23 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.61 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.62 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.80 |        43.17 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        43.17 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.67 |         7.04 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.04 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |        99.41 |       280.48 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.35 |       163.46 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       162.69 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.48 |        23.55 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.55 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        28.98 |        93.47 |         0.59 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        93.47 |         0.59 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       608.92 |       473.31 |         2.98 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        14.93 |         7.36 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.36 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       444.34 |       186.78 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.37 |       105.06 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.29 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.81 |        12.74 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.74 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       299.72 |        27.07 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.64 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.43 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        48.97 |        41.92 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.92 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        22.71 |         7.07 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.07 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       109.79 |       272.10 |         1.72 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        37.47 |       162.91 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       162.11 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.12 |        23.39 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.39 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        32.31 |        85.79 |         0.54 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        85.79 |         0.54 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       606.69 |       485.12 |         3.06 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.96 |         7.39 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.39 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       449.79 |       186.56 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.74 |       104.70 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.90 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.91 |        12.90 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.90 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       310.18 |        27.33 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.80 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.53 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.82 |        41.63 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.63 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.36 |         7.13 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.13 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       103.17 |       284.03 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.38 |       162.59 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       161.79 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.98 |        24.03 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        24.03 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.28 |        97.41 |         0.61 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        97.41 |         0.61 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       593.97 |       482.05 |         3.04 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.04 |         7.42 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.42 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       444.08 |       186.24 |         1.17 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.76 |       104.19 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.42 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        33.98 |        12.67 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.67 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       302.10 |        27.23 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.70 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.53 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        38.89 |        42.15 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.15 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        18.56 |         7.10 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.10 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |        98.94 |       281.28 |         1.77 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.41 |       164.83 |         1.04 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       164.06 |         1.03 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        24.00 |        23.10 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.10 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        28.99 |        93.34 |         0.59 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        93.34 |         0.59 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       608.20 |       483.78 |         3.05 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.68 |         7.39 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.39 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       442.47 |       186.50 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        32.56 |       104.70 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.94 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        30.59 |        12.29 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.29 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       303.18 |        27.62 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.58 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        19.04 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        43.34 |        41.89 |         0.26 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        41.89 |         0.26 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.88 |         7.17 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.17 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       111.35 |       282.72 |         1.78 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.67 |       163.30 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       162.50 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.60 |        23.78 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.78 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        35.66 |        95.65 |         0.60 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        95.65 |         0.60 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       604.51 |       475.87 |         3.00 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        16.64 |         7.07 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.07 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       448.94 |       188.13 |         1.19 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.87 |       105.25 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       104.45 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.59 |        12.90 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.90 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       300.02 |        27.33 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.77 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.56 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        39.10 |        42.66 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.66 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        20.63 |         7.20 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.20 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       101.96 |       273.47 |         1.72 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        34.46 |       163.07 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       162.30 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.52 |        23.68 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.68 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        29.16 |        86.72 |         0.55 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        86.72 |         0.55 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       650.99 |       487.14 |         3.07 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.81 |         7.39 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.39 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       488.53 |       188.35 |         1.19 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        33.72 |       106.18 |         0.67 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       105.41 |         0.66 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        29.82 |        12.42 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.42 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       308.67 |        26.91 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         8.35 |         0.05 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.56 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        48.32 |        42.85 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.85 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        21.08 |         6.98 |         0.04 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         6.98 |         0.04 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       108.55 |       284.42 |         1.79 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        36.80 |       163.10 |         1.03 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       162.34 |         1.02 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        26.54 |        24.16 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        24.16 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        31.67 |        97.15 |         0.61 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        97.15 |         0.61 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- LlamaDecoderLayer                                        |       622.06 |       479.26 |         3.02 |                                                             
|--- RMSNorm(weight=float16[4096])                           |        15.90 |         7.17 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.17 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaAttention                                          |       467.10 |       186.97 |         1.18 |                                                             
|---- QKVParallelLinear(weight=float16[12288, 4096])         |        34.41 |       104.16 |         0.66 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.80 |         0.01 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       103.36 |         0.65 | mm(float16[500, 4096], float16[4096, 12288]) <- matmul(fl...
|---- RotaryEmbedding                                        |        32.42 |        12.58 |         0.08 |                                                             
|----- void vllm::rotary_embedding_kernel<c10::Half, true... |         0.00 |        12.58 |         0.08 | _C::rotary_embedding(int64[500], float16[500, 4096], floa...
|---- Attention                                              |       326.85 |        27.49 |         0.17 |                                                             
|----- void vllm::reshape_and_cache_flash_kernel<unsigned... |         0.00 |         9.02 |         0.06 | _C_cache_ops::reshape_and_cache_flash(float16[500, 32, 12...
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128,... |         0.00 |        18.46 |         0.12 | vllm_flash_attn_c::varlen_fwd(float16[500, 32, 128], floa...
|---- RowParallelLinear(weight=float16[4096, 4096])          |        40.29 |        42.75 |         0.27 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        42.75 |         0.27 | mm(float16[500, 4096], float16[4096, 4096]) <- matmul(flo...
|--- RMSNorm(weight=float16[4096])                           |        19.28 |         7.26 |         0.05 |                                                             
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10:... |         0.00 |         7.26 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
|--- LlamaMLP                                                |       103.76 |       277.86 |         1.75 |                                                             
|---- MergedColumnParallelLinear(weight=float16[22016, 40... |        35.60 |       161.41 |         1.02 |                                                             
|----- Memset (Device)                                       |         0.00 |         0.77 |         0.00 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |       160.64 |         1.01 | mm(float16[500, 4096], float16[4096, 22016]) <- matmul(fl...
|---- SiluAndMul                                             |        25.73 |        23.62 |         0.15 |                                                             
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Ha... |         0.00 |        23.62 |         0.15 | _C::silu_and_mul(float16[500, 11008], float16[500, 22016])  
|---- RowParallelLinear(weight=float16[4096, 11008])         |        28.69 |        92.83 |         0.59 |                                                             
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x... |         0.00 |        92.83 |         0.59 | mm(float16[500, 11008], float16[11008, 4096]) <- matmul(f...
|-- RMSNorm(weight=float16[4096])                            |        15.74 |         7.26 |         0.05 |                                                             
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::... |         0.00 |         7.26 |         0.05 | _C::fused_add_rms_norm(float16[500, 4096], float16[500, 4...
LogitsProcessor                                              |       115.73 |       184.42 |         1.16 |                                                             
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         3.10 |         0.02 | index_select(float16[500, 4096], 0, int64[2])               
|- Memset (Device)                                           |         0.00 |         0.77 |         0.00 | mm(float16[2, 4096], float16[4096, 32000]) <- matmul(floa...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       180.54 |         1.14 | mm(float16[2, 4096], float16[4096, 32000]) <- matmul(floa...
Sampler                                                      |      2708.63 |       214.75 |         1.35 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.77 |         0.00 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.77 |         0.00 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.77 |         0.00 | copy_(int32[2], int32[2], True) <- _to_copy(int32[2], 3, ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.77 |         0.00 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.80 |         0.01 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.77 |         0.00 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         0.77 |         0.00 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         4.06 |         0.03 | copy_(float32[2, 32000], float16[2, 32000], False) <- _to...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.26 |         0.03 | div_(float32[2, 32000], float16[2, 1])                      
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |         1.92 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.70 |         0.00 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.60 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         3.68 |         0.02 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.70 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |        10.02 |         0.06 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         9.06 |         0.06 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.31 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         8.77 |         0.06 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         8.93 |         0.06 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.77 |         0.00 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.22 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         2.66 |         0.02 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.73 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.54 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         7.14 |         0.04 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |         2.85 |         0.02 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.79 |         0.01 | copy_(float32[2, 32000], float32[2, 32000], False) <- sor...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.89 |         0.01 | copy_(int64[2], int32[2], False) <- _to_copy(int32[2], 4,...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.50 |         0.01 | sub(int64[], int64[2], 1) <- rsub(int64[2], 32000, 1)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.59 |         0.02 | gather(float32[2, 32000], 1, int64[2, 1], False)            
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.10 |         0.02 | lt(float32[2, 32000], float32[2, 1])                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.57 |         0.01 | masked_fill_(float32[2, 32000], bool[2, 32000], -inf)       
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.66 |         0.07 | _softmax(float32[2, 32000], -1, False) <- softmax(float32...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        44.26 |         0.28 | cumsum(float32[2, 32000], -1, None)                         
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.63 |         0.01 | sub(int64[], float16[2, 1], 1) <- rsub(float16[2, 1], 1, 1) 
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.42 |         0.03 | le(float32[2, 32000], float16[2, 1])                        
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.98 |         0.01 | fill_(bool[2], bool[])                                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.76 |         0.01 | masked_fill_(float32[2, 32000], bool[2, 32000], -inf)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         4.35 |         0.03 | scatter_(float32[2, 32000], -1, int64[2, 32000], float32[...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        16.26 |         0.10 | _softmax(float32[2, 32000], -1, False) <- softmax(float32...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |         8.54 |         0.05 | _log_softmax(float32[2, 32000], -1, False) <- log_softmax...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.76 |         0.01 | copy_(int64[2], int32[2], False) <- _to_copy(int32[2], 4,...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         4.80 |         0.03 | index(float32[2, 32000], None)                              
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         2.14 |         0.01 | exponential_(float32[2, 32000], 1.0, None)                  
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.27 |         0.01 | div_(float32[2, 32000], float32[2, 32000])                  
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        11.04 |         0.07 | argmax(float32[2, 32000], 1, False)                         
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.62 |         0.02 | copy_(int64[2, 1], int64[2, 1], False) <- _to_copy(int64[...

================================================================================
= First Decode Step Model Table (prompt_len=250, batch_size=2)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
CUDAGraphRunner                                              |       688.29 |     11831.94 |        96.71 |                                                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.47 |         0.01 | copy_(int64[2], int64[2], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.47 |         0.01 | copy_(int64[2], int64[2], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.47 |         0.01 | copy_(int64[2], int64[2], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.38 |         0.01 | copy_(int32[2], int32[2], True)                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.31 |         0.01 | copy_(int32[2, 256], int32[2, 256], True)                   
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.12 |         0.01 | fill_(int64[1], 0)                                          
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.28 |         0.01 | fill_(int64[1], 12)                                         
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         3.10 |         0.03 |                                                             
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10:... |         0.00 |         4.42 |         0.04 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         4.67 |         0.04 |                                                             
|- void vllm::reshape_and_cache_flash_kernel<unsigned sho... |         0.00 |         3.36 |         0.03 |                                                             
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<... |         0.00 |        10.59 |         0.09 |                                                             
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel... |         0.00 |         3.78 |         0.03 |                                                             
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64... |         0.00 |        73.66 |         0.60 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       121.38 |         0.99 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         7.07 |         0.06 |                                                             
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f1... |         0.00 |        63.23 |         0.52 |                                                             
|- void splitKreduce_kernel<32, 16, int, float, __half, f... |         0.00 |         1.86 |         0.02 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         2.98 |         0.02 |                                                             
LogitsProcessor                                              |       235.65 |       177.98 |         1.45 |                                                             
|- void at::native::(anonymous namespace)::indexSelectSma... |         0.00 |         2.98 |         0.02 | index_select(float16[2, 4096], 0, int64[2])                 
|- Memset (Device)                                           |         0.00 |         0.96 |         0.01 | mm(float16[2, 4096], float16[4096, 32000]) <- matmul(floa...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |       174.05 |         1.42 | mm(float16[2, 4096], float16[4096, 32000]) <- matmul(floa...
Sampler                                                      |      9575.90 |       224.03 |         1.83 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.30 |         0.02 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.02 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.34 |         0.02 | copy_(int32[2], int32[2], True) <- _to_copy(int32[2], 3, ...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.02 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.02 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.02 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         1.98 |         0.02 | copy_(float16[2], float16[2], True) <- _to_copy(float16[2...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         4.10 |         0.03 | copy_(float32[2, 32000], float16[2, 32000], False) <- _to...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.16 |         0.03 | div_(float32[2, 32000], float16[2, 1])                      
|- at::native::(anonymous namespace)::fill_index_and_segm... |         0.00 |         1.95 |         0.02 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.99 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.44 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         3.68 |         0.03 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         2.08 |         0.02 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         9.79 |         0.08 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.50 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         8.99 |         0.07 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.31 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         8.99 |         0.07 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         8.77 |         0.07 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         0.74 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKern... |         0.00 |         2.62 |         0.02 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumK... |         0.00 |         1.47 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memset (Device)                                           |         0.00 |         1.47 |         0.01 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKerne... |         0.00 |         7.04 |         0.06 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- void at::native::(anonymous namespace)::sort_postproce... |         0.00 |         2.94 |         0.02 | sort(float32[2, 32000], False, -1, False) <- sort(float32...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.66 |         0.01 | copy_(float32[2, 32000], float32[2, 32000], False) <- sor...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.86 |         0.02 | copy_(int64[2], int32[2], False) <- _to_copy(int32[2], 4,...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.47 |         0.01 | sub(int64[], int64[2], 1) <- rsub(int64[2], 32000, 1)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         2.69 |         0.02 | gather(float32[2, 32000], 1, int64[2, 1], False)            
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         3.07 |         0.03 | lt(float32[2, 32000], float32[2, 1])                        
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.60 |         0.01 | masked_fill_(float32[2, 32000], bool[2, 32000], -inf)       
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        10.59 |         0.09 | _softmax(float32[2, 32000], -1, False) <- softmax(float32...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |        44.55 |         0.36 | cumsum(float32[2, 32000], -1, None)                         
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.54 |         0.01 | sub(int64[], float16[2, 1], 1) <- rsub(float16[2, 1], 1, 1) 
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         4.42 |         0.04 | le(float32[2, 32000], float16[2, 1])                        
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         1.82 |         0.01 | fill_(bool[2], bool[])                                      
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.63 |         0.01 | masked_fill_(float32[2, 32000], bool[2, 32000], -inf)       
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         4.22 |         0.03 | scatter_(float32[2, 32000], -1, int64[2, 32000], float32[...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        16.16 |         0.13 | _softmax(float32[2, 32000], -1, False) <- softmax(float32...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |         8.51 |         0.07 | _log_softmax(float32[2, 32000], -1, False) <- log_softmax...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         1.82 |         0.01 | copy_(int64[2], int32[2], False) <- _to_copy(int32[2], 4,...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |         4.77 |         0.04 | index(float32[2, 32000], None)                              
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |         2.21 |         0.02 | exponential_(float32[2, 32000], 1.0, None)                  
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         2.27 |         0.02 | div_(float32[2, 32000], float32[2, 32000])                  
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        11.94 |         0.10 | argmax(float32[2, 32000], 1, False)                         
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.21 |         0.02 | copy_(int64[2, 1], int64[2, 1], False) <- _to_copy(int64[...

================================================================================
= Prefill Summary Table (prompt_len=250, batch_size=2)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
LlamaForCausalLM                                                                 |     15459.32 |        97.48 |            1.00
|- LlamaModel                                                                    |     15459.32 |        97.48 |            1.00
|-- VocabParallelEmbedding(weight=float16[32000, 4096])                          |        16.07 |         0.10 |            1.00
|--- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half,... |        16.07 |         0.10 |            1.00
|-- LlamaDecoderLayer                                                            |     15435.99 |        97.34 |           32.00
|--- RMSNorm(weight=float16[4096])                                               |       462.46 |         2.92 |           64.00
|---- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10... |         7.97 |         0.05 |            1.00
|---- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void... |       454.50 |         2.87 |           63.00
|--- LlamaAttention                                                              |      6018.35 |        37.95 |           32.00
|---- QKVParallelLinear(weight=float16[12288, 4096])                             |      3381.99 |        21.33 |           32.00
|----- Memset (Device)                                                           |        24.93 |         0.16 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |      3357.06 |        21.17 |           32.00
|---- RotaryEmbedding                                                            |       403.46 |         2.54 |           32.00
|----- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::... |       403.46 |         2.54 |           32.00
|---- Attention                                                                  |       871.72 |         5.50 |           32.00
|----- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned sho... |       277.60 |         1.75 |           32.00
|----- void flash_fwd_kernel<Flash_fwd_kernel_traits<128, 128, 64, 4, false, ... |       594.11 |         3.75 |           32.00
|---- RowParallelLinear(weight=float16[4096, 4096])                              |      1361.18 |         8.58 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |      1361.18 |         8.58 |           32.00
|--- LlamaMLP                                                                    |      8955.18 |        56.47 |           32.00
|---- MergedColumnParallelLinear(weight=float16[22016, 4096])                    |      5224.71 |        32.95 |           32.00
|----- Memset (Device)                                                           |        24.96 |         0.16 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |      5199.75 |        32.79 |           32.00
|---- SiluAndMul                                                                 |       757.09 |         4.77 |           32.00
|----- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel... |       757.09 |         4.77 |           32.00
|---- RowParallelLinear(weight=float16[4096, 11008])                             |      2973.38 |        18.75 |           32.00
|----- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize... |      2973.38 |        18.75 |           32.00
|-- RMSNorm(weight=float16[4096])                                                |         7.26 |         0.05 |            1.00
|--- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>... |         7.26 |         0.05 |            1.00
LogitsProcessor                                                                  |       184.42 |         1.16 |            1.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         3.10 |         0.02 |            1.00
|- Memset (Device)                                                               |         0.77 |         0.00 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |       180.54 |         1.14 |            1.00
Sampler                                                                          |       214.75 |         1.35 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |         5.41 |         0.03 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.06 |         0.03 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.26 |         0.03 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |         1.92 |         0.01 |            1.00
|- Memset (Device)                                                               |        11.65 |         0.07 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         3.68 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.70 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        36.77 |         0.23 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         2.66 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.73 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |         7.14 |         0.04 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |         2.85 |         0.02 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         1.79 |         0.01 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.65 |         0.02 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.50 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.59 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         3.10 |         0.02 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         3.33 |         0.02 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        26.91 |         0.17 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        44.26 |         0.28 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.63 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.42 |         0.03 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.98 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         4.35 |         0.03 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |         8.54 |         0.05 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         4.80 |         0.03 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         2.14 |         0.01 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         2.27 |         0.01 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        11.04 |         0.07 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.62 |         0.02 |            1.00

================================================================================
= First Decode Step Summary Table (prompt_len=250, batch_size=2)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
CUDAGraphRunner                                                                  |     11831.94 |        96.71 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         7.10 |         0.06 |            5.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<... |         2.40 |         0.02 |            2.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         3.10 |         0.03 |            1.00
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10::H... |         4.42 |         0.04 |            1.00
|- sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x... |      4714.50 |        38.54 |           64.00
|- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::Half... |       149.50 |         1.22 |           32.00
|- void vllm::reshape_and_cache_flash_kernel<unsigned short, unsigned short, ... |       107.52 |         0.88 |           32.00
|- void flash_fwd_splitkv_kernel<Flash_fwd_kernel_traits<128, 64, 128, 4, fal... |       338.94 |         2.77 |           32.00
|- void flash_fwd_splitkv_combine_kernel<Flash_fwd_kernel_traits<128, 64, 128... |       120.83 |         0.99 |           32.00
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>::... |       190.46 |         1.56 |           64.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |      3884.03 |        31.75 |           32.00
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel<c10... |       226.30 |         1.85 |           32.00
|- void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f16_128x64_64x6_tn_ali... |      2023.42 |        16.54 |           32.00
|- void splitKreduce_kernel<32, 16, int, float, __half, float, __half, true, ... |        59.39 |         0.49 |           32.00
LogitsProcessor                                                                  |       177.98 |         1.45 |            1.00
|- void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, l... |         2.98 |         0.02 |            1.00
|- Memset (Device)                                                               |         0.96 |         0.01 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |       174.05 |         1.42 |            1.00
Sampler                                                                          |       224.03 |         1.83 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        14.56 |         0.12 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.10 |         0.03 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.16 |         0.03 |            1.00
|- at::native::(anonymous namespace)::fill_index_and_segment_kernel(int2*, in... |         1.95 |         0.02 |            1.00
|- Memset (Device)                                                               |        11.87 |         0.10 |            9.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         3.68 |         0.03 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         2.08 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |        36.54 |         0.30 |            4.00
|- void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::c... |         2.62 |         0.02 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail... |         1.47 |         0.01 |            1.00
|- void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cu... |         7.04 |         0.06 |            1.00
|- void at::native::(anonymous namespace)::sort_postprocess_kernel<float>(flo... |         2.94 |         0.02 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         1.66 |         0.01 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         3.68 |         0.03 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.47 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         2.69 |         0.02 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         3.07 |         0.03 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |         3.23 |         0.03 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        26.75 |         0.22 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |        44.55 |         0.36 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.54 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |         4.42 |         0.04 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         1.82 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         4.22 |         0.03 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |         8.51 |         0.07 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |         4.77 |         0.04 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |         2.21 |         0.02 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |         2.27 |         0.02 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        11.94 |         0.10 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.21 |         0.02 |            1.00
Traces saved as prefill.json and decode_1.json, etc. in folder benchmarks_simple/definitive_results/initial/llama-2-7b/_250_230_2/chrome_traces
