Run profile with:
  engine_args = {'model': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'served_model_name': None, 'tokenizer': '/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', 'skip_tokenizer_init': False, 'tokenizer_mode': 'auto', 'trust_remote_code': False, 'download_dir': None, 'load_format': 'auto', 'config_format': <ConfigFormat.AUTO: 'auto'>, 'dtype': 'auto', 'kv_cache_dtype': 'auto', 'quantization_param_path': None, 'seed': 0, 'max_model_len': None, 'worker_use_ray': False, 'distributed_executor_backend': None, 'pipeline_parallel_size': 1, 'tensor_parallel_size': 1, 'max_parallel_loading_workers': None, 'block_size': 16, 'enable_prefix_caching': False, 'disable_sliding_window': False, 'use_v2_block_manager': False, 'swap_space': 4, 'cpu_offload_gb': 0, 'gpu_memory_utilization': 0.9, 'max_num_batched_tokens': 4096, 'max_num_seqs': 1024, 'max_logprobs': 20, 'disable_log_stats': False, 'revision': None, 'code_revision': None, 'rope_scaling': None, 'rope_theta': None, 'tokenizer_revision': None, 'quantization': None, 'enforce_eager': False, 'max_context_len_to_capture': None, 'max_seq_len_to_capture': 8192, 'disable_custom_all_reduce': False, 'tokenizer_pool_size': 0, 'tokenizer_pool_type': 'ray', 'tokenizer_pool_extra_config': None, 'limit_mm_per_prompt': None, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'enable_prompt_adapter': False, 'max_prompt_adapters': 1, 'max_prompt_adapter_token': 0, 'fully_sharded_loras': False, 'lora_extra_vocab_size': 256, 'long_lora_scaling_factors': None, 'lora_dtype': 'auto', 'max_cpu_loras': None, 'device': 'auto', 'num_scheduler_steps': 1, 'multi_step_stream_outputs': True, 'ray_workers_use_nsight': False, 'num_gpu_blocks_override': None, 'num_lookahead_slots': 0, 'model_loader_extra_config': None, 'ignore_patterns': [], 'preemption_mode': None, 'scheduler_delay_factor': 0.0, 'enable_chunked_prefill': None, 'guided_decoding_backend': 'outlines', 'speculative_model': None, 'speculative_model_quantization': None, 'speculative_draft_tensor_parallel_size': None, 'num_speculative_tokens': None, 'speculative_disable_mqa_scorer': False, 'speculative_max_model_len': None, 'speculative_disable_by_batch_size': None, 'ngram_prompt_lookup_max': None, 'ngram_prompt_lookup_min': None, 'spec_decoding_acceptance_method': 'rejection_sampler', 'typical_acceptance_sampler_posterior_threshold': None, 'typical_acceptance_sampler_posterior_alpha': None, 'qlora_adapter_name_or_path': None, 'disable_logprobs_during_spec_decoding': None, 'otlp_traces_endpoint': None, 'collect_detailed_traces': None, 'disable_async_output_proc': False, 'override_neuron_config': None, 'mm_processor_kwargs': None, 'scheduling_policy': 'fcfs'}
  prompt_len = 250
  output_len = 230
  batch_size = 160
  result_dir = benchmarks_simple/definitive_results/initial_xformers/llama-2-7b/_250_230_160
  save_chrome_traces_folder = None
WARNING 12-09 16:00:24 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 12-09 16:00:24 llm_engine.py:237] Initializing an LLM engine (v0.5.2.dev1210+g6f078ea3c) with config: model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', speculative_config=None, tokenizer='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 12-09 16:00:25 selector.py:115] Using XFormers backend.
INFO 12-09 16:00:25 model_runner.py:1056] Starting to load model /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b...
INFO 12-09 16:00:26 selector.py:115] Using XFormers backend.
INFO 12-09 16:00:55 model_runner.py:1067] Loading model weights took 12.5523 GB
INFO 12-09 16:00:56 worker.py:262] Memory profiling results: total_gpu_memory=63.43GiB initial_memory_usage=13.15GiB peak_torch_memory=13.78GiB non_torch_memory=0.10GiB kv_cache_size=43.21GiB gpu_memory_utilization=0.90
INFO 12-09 16:00:57 gpu_executor.py:122] # GPU blocks: 5531, # CPU blocks: 512
INFO 12-09 16:00:57 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 21.61x
INFO 12-09 16:00:58 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 12-09 16:00:58 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 12-09 16:01:30 model_runner.py:1523] Graph capturing finished in 33 secs.
llm.llm_engine.model_config.max_model_len:  4096
Warm up run ...
Profile run ...

PREFILL PHASES
PREFILL - 0 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 0. RUNNING: 0. WAITING: 160
PREFILL - 1 -> PREVIOUSLY RUN PREFILLS: 16. PREVIOUSLY RUN DECODES: 0. RUNNING: 16. WAITING: 144
PREFILL - 2 -> PREVIOUSLY RUN PREFILLS: 16. PREVIOUSLY RUN DECODES: 0. RUNNING: 32. WAITING: 128
PREFILL - 3 -> PREVIOUSLY RUN PREFILLS: 16. PREVIOUSLY RUN DECODES: 0. RUNNING: 48. WAITING: 112
PREFILL - 4 -> PREVIOUSLY RUN PREFILLS: 16. PREVIOUSLY RUN DECODES: 0. RUNNING: 64. WAITING: 96
PREFILL - 5 -> PREVIOUSLY RUN PREFILLS: 16. PREVIOUSLY RUN DECODES: 0. RUNNING: 80. WAITING: 80
PREFILL - 6 -> PREVIOUSLY RUN PREFILLS: 16. PREVIOUSLY RUN DECODES: 0. RUNNING: 96. WAITING: 64
PREFILL - 7 -> PREVIOUSLY RUN PREFILLS: 16. PREVIOUSLY RUN DECODES: 0. RUNNING: 112. WAITING: 48
PREFILL - 8 -> PREVIOUSLY RUN PREFILLS: 16. PREVIOUSLY RUN DECODES: 0. RUNNING: 128. WAITING: 32
PREFILL - 9 -> PREVIOUSLY RUN PREFILLS: 16. PREVIOUSLY RUN DECODES: 0. RUNNING: 144. WAITING: 16
PREFILL - 10 -> PREVIOUSLY RUN PREFILLS: 16. PREVIOUSLY RUN DECODES: 0. RUNNING: 160. WAITING: 0

DECODE PHASES
DECODE - 0 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 1 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 2 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 3 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 4 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:01:37 metrics.py:360] Avg prompt throughput: 7159.7 tokens/s, Avg generation throughput: 807.1 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 46.3%, CPU KV cache usage: 0.0%.
DECODE - 5 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 6 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 7 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 8 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 9 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:01:43 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.5 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 49.2%, CPU KV cache usage: 0.0%.
DECODE - 10 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 11 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 12 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 13 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:01:48 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.0 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 49.2%, CPU KV cache usage: 0.0%.
DECODE - 14 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 15 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 16 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 17 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:01:53 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.2 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 49.2%, CPU KV cache usage: 0.0%.
DECODE - 18 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 19 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 20 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 21 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 22 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:00 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 126.7 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 52.1%, CPU KV cache usage: 0.0%.
DECODE - 23 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 24 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 25 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 26 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:05 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.7 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 52.1%, CPU KV cache usage: 0.0%.
DECODE - 27 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 28 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 29 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 30 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 31 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:11 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.0 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 52.1%, CPU KV cache usage: 0.0%.
DECODE - 32 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 33 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 34 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 35 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:17 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.6 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 52.1%, CPU KV cache usage: 0.0%.
DECODE - 36 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 37 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 38 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 39 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:22 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 55.0%, CPU KV cache usage: 0.0%.
DECODE - 40 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 41 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 42 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 43 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:27 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.2 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 55.0%, CPU KV cache usage: 0.0%.
DECODE - 44 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 45 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 46 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 47 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:33 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.1 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 55.0%, CPU KV cache usage: 0.0%.
DECODE - 48 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 49 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 50 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 51 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:38 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 55.0%, CPU KV cache usage: 0.0%.
DECODE - 52 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 53 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 54 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 55 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:43 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.1 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 57.9%, CPU KV cache usage: 0.0%.
DECODE - 56 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 57 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 58 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 59 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:49 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.7 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 57.9%, CPU KV cache usage: 0.0%.
DECODE - 60 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 61 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 62 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 63 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 64 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:02:55 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.5 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 57.9%, CPU KV cache usage: 0.0%.
DECODE - 65 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 66 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 67 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 68 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:00 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.3 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 57.9%, CPU KV cache usage: 0.0%.
DECODE - 69 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 70 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 71 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 72 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:05 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 121.5 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 60.7%, CPU KV cache usage: 0.0%.
DECODE - 73 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 74 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 75 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 76 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:11 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 60.7%, CPU KV cache usage: 0.0%.
DECODE - 77 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 78 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 79 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 80 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:16 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.9 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 60.7%, CPU KV cache usage: 0.0%.
DECODE - 81 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 82 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 83 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 84 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 85 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:22 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 131.2 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 63.6%, CPU KV cache usage: 0.0%.
DECODE - 86 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 87 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 88 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 89 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:27 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.0 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 63.6%, CPU KV cache usage: 0.0%.
DECODE - 90 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 91 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 92 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 93 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:33 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.6 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 63.6%, CPU KV cache usage: 0.0%.
DECODE - 94 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 95 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 96 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 97 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:38 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.8 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 63.6%, CPU KV cache usage: 0.0%.
DECODE - 98 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 99 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 100 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 101 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:44 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 66.5%, CPU KV cache usage: 0.0%.
DECODE - 102 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 103 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 104 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 105 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:49 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.3 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 66.5%, CPU KV cache usage: 0.0%.
DECODE - 106 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 107 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 108 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 109 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:54 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.4 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 66.5%, CPU KV cache usage: 0.0%.
DECODE - 110 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 111 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 112 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 113 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:03:59 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.6 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 66.5%, CPU KV cache usage: 0.0%.
DECODE - 114 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 115 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 116 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 117 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:05 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.6 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 69.4%, CPU KV cache usage: 0.0%.
DECODE - 118 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 119 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 120 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 121 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:10 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.0 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 69.4%, CPU KV cache usage: 0.0%.
DECODE - 122 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 123 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 124 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 125 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:16 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.1 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 69.4%, CPU KV cache usage: 0.0%.
DECODE - 126 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 127 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 128 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 129 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:22 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.9 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 69.4%, CPU KV cache usage: 0.0%.
DECODE - 130 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 131 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 132 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 133 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:27 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.1 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 72.3%, CPU KV cache usage: 0.0%.
DECODE - 134 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 135 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 136 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 137 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:32 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 119.7 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 72.3%, CPU KV cache usage: 0.0%.
DECODE - 138 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 139 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 140 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 141 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:37 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.7 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 72.3%, CPU KV cache usage: 0.0%.
DECODE - 142 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 143 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 144 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 145 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:43 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.3 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 72.3%, CPU KV cache usage: 0.0%.
DECODE - 146 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 147 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 148 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 149 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:48 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.4 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 75.2%, CPU KV cache usage: 0.0%.
DECODE - 150 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 151 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 152 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 153 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:53 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.6 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 75.2%, CPU KV cache usage: 0.0%.
DECODE - 154 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 155 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 156 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 157 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:04:59 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 75.2%, CPU KV cache usage: 0.0%.
DECODE - 158 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 159 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 160 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 161 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:05 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.0 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 75.2%, CPU KV cache usage: 0.0%.
DECODE - 162 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 163 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 164 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 165 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:10 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 126.1 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 78.1%, CPU KV cache usage: 0.0%.
DECODE - 166 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 167 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 168 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 169 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:15 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.9 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 78.1%, CPU KV cache usage: 0.0%.
DECODE - 170 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 171 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 172 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 173 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:20 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.0 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 78.1%, CPU KV cache usage: 0.0%.
DECODE - 174 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 175 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 176 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 177 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:26 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.1 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 78.1%, CPU KV cache usage: 0.0%.
DECODE - 178 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 179 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 180 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 181 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:31 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.6 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 81.0%, CPU KV cache usage: 0.0%.
DECODE - 182 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 183 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 184 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 185 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:36 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 121.0 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 81.0%, CPU KV cache usage: 0.0%.
DECODE - 186 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 187 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 188 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 189 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:41 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.1 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 81.0%, CPU KV cache usage: 0.0%.
DECODE - 190 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 191 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 192 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 193 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:47 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.0 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 81.0%, CPU KV cache usage: 0.0%.
DECODE - 194 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 195 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 196 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 197 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:53 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.8 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 83.9%, CPU KV cache usage: 0.0%.
DECODE - 198 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 199 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 200 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 201 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:05:59 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.9 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 83.9%, CPU KV cache usage: 0.0%.
DECODE - 202 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 203 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 204 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 205 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:06:04 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 121.8 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 83.9%, CPU KV cache usage: 0.0%.
DECODE - 206 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 207 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 208 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 209 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:06:09 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 121.7 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 83.9%, CPU KV cache usage: 0.0%.
DECODE - 210 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 211 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 212 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 213 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:06:15 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 86.8%, CPU KV cache usage: 0.0%.
DECODE - 214 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 215 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 216 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 217 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:06:20 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.7 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 86.8%, CPU KV cache usage: 0.0%.
DECODE - 218 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 219 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 220 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 221 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:06:25 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.5 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 86.8%, CPU KV cache usage: 0.0%.
DECODE - 222 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 223 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 224 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 225 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
INFO 12-09 16:06:30 metrics.py:360] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.8 tokens/s, Running: 160 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 86.8%, CPU KV cache usage: 0.0%.
DECODE - 226 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 227 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0
DECODE - 228 -> PREVIOUSLY RUN PREFILLS: 0. PREVIOUSLY RUN DECODES: 160. RUNNING: 160. WAITING: 0

================================================================================
= First Decode Step Model Table (prompt_len=250, batch_size=160)
================================================================================

name                                                         | cpu_time_us  | cuda_time_us | pct_cuda_... | trace                                                       
========================================================================================================================================================================
CUDAGraphRunner                                              |       781.89 |     25582.37 |        94.13 |                                                             
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.38 |         0.01 | copy_(int64[160], int64[160], True)                         
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.44 |         0.01 | copy_(int64[160], int64[160], True)                         
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.18 |         0.00 | copy_(int64[160], int64[160], True)                         
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.18 |         0.00 | copy_(int32[160], int32[160], True)                         
|- Memcpy DtoD (Device -> Device)                            |         0.00 |         1.79 |         0.01 | copy_(int32[160, 256], int32[160, 256], True)               
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.38 |         0.01 | fill_(int64[1], 0)                                          
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.09 |         0.00 | fill_(int64[1], 68)                                         
|- void at::native::(anonymous namespace)::indexSelectLar... |         0.00 |         7.01 |         0.03 |                                                             
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10:... |         0.00 |         4.96 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x... |         0.00 |        87.36 |         0.32 |                                                             
|- void vllm::rotary_embedding_kernel<c10::Half, true>(lo... |         0.00 |         7.42 |         0.03 |                                                             
|- void vllm::reshape_and_cache_kernel<unsigned short, un... |         0.00 |        30.91 |         0.11 |                                                             
|- void vllm::paged_attention_v1_kernel<unsigned short, u... |         0.00 |       450.43 |         1.66 |                                                             
|- memcpy128                                                 |         0.00 |         3.39 |         0.01 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
|- Memset (Unknown)                                          |         0.00 |         0.74 |         0.00 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       144.25 |         0.53 |                                                             
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half v... |         0.00 |         9.06 |         0.03 |                                                             
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x6... |         0.00 |        28.48 |         0.10 |                                                             
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Ha... |         0.00 |         4.13 |         0.02 |                                                             
LogitsProcessor                                              |       403.05 |       214.46 |         0.79 |                                                             
|- void at::native::(anonymous namespace)::indexSelectLar... |         0.00 |         6.91 |         0.03 | index_select(float16[160, 4096], 0, int64[160])             
|- Memset (Device)                                           |         0.00 |         0.96 |         0.00 | mm(float16[160, 4096], float16[4096, 32000]) <- matmul(fl...
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x6... |         0.00 |       206.59 |         0.76 | mm(float16[160, 4096], float16[4096, 32000]) <- matmul(fl...
Sampler                                                      |     20521.88 |      1380.12 |         5.08 |                                                             
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.37 |         0.01 | copy_(float16[160], float16[160], True) <- _to_copy(float...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.08 |         0.01 | copy_(float16[160], float16[160], True) <- _to_copy(float...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.18 |         0.01 | copy_(int32[160], int32[160], True) <- _to_copy(int32[160...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.18 |         0.01 | copy_(float16[160], float16[160], True) <- _to_copy(float...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.21 |         0.01 | copy_(float16[160], float16[160], True) <- _to_copy(float...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.18 |         0.01 | copy_(float16[160], float16[160], True) <- _to_copy(float...
|- Memcpy HtoD (Pinned -> Device)                            |         0.00 |         2.18 |         0.01 | copy_(float16[160], float16[160], True) <- _to_copy(float...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |        17.12 |         0.06 | copy_(float32[160, 32000], float16[160, 32000], False) <-...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |        22.40 |         0.08 | div_(float32[160, 32000], float16[160, 1])                  
|- at::native::(anonymous namespace)::fill_reverse_indice... |         0.00 |        26.11 |         0.10 | sort(float32[160, 32000], False, -1, False) <- sort(float...
|- void at_cuda_detail::cub::DeviceSegmentedRadixSortKern... |         0.00 |       131.39 |         0.48 | sort(float32[160, 32000], False, -1, False) <- sort(float...
|- void at_cuda_detail::cub::DeviceSegmentedRadixSortKern... |         0.00 |       128.29 |         0.47 | sort(float32[160, 32000], False, -1, False) <- sort(float...
|- void at_cuda_detail::cub::DeviceSegmentedRadixSortKern... |         0.00 |       125.60 |         0.46 | sort(float32[160, 32000], False, -1, False) <- sort(float...
|- void at_cuda_detail::cub::DeviceSegmentedRadixSortKern... |         0.00 |       125.92 |         0.46 | sort(float32[160, 32000], False, -1, False) <- sort(float...
|- void at_cuda_detail::cub::DeviceSegmentedRadixSortKern... |         0.00 |       118.11 |         0.43 | sort(float32[160, 32000], False, -1, False) <- sort(float...
|- void at_cuda_detail::cub::DeviceSegmentedRadixSortKern... |         0.00 |       108.58 |         0.40 | sort(float32[160, 32000], False, -1, False) <- sort(float...
|- Memcpy DtoD (Device -> Device)                            |         0.00 |        21.54 |         0.08 | copy_(float32[160, 32000], float32[160, 32000], False) <-...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         2.40 |         0.01 | copy_(int64[160], int32[160], False) <- _to_copy(int32[16...
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.66 |         0.01 | sub(int64[], int64[160], 1) <- rsub(int64[160], 32000, 1)   
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |         3.52 |         0.01 | gather(float32[160, 32000], 1, int64[160, 1], False)        
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |        24.19 |         0.09 | lt(float32[160, 32000], float32[160, 1])                    
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |        12.99 |         0.05 | masked_fill_(float32[160, 32000], bool[160, 32000], -inf)   
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        32.99 |         0.12 | _softmax(float32[160, 32000], -1, False) <- softmax(float...
|- void at::native::tensor_kernel_scan_innermost_dim<floa... |         0.00 |       105.44 |         0.39 | cumsum(float32[160, 32000], -1, None)                       
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |         1.66 |         0.01 | sub(int64[], float16[160, 1], 1) <- rsub(float16[160, 1],...
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |        24.00 |         0.09 | le(float32[160, 32000], float16[160, 1])                    
|- void at::native::elementwise_kernel<128, 4, at::native... |         0.00 |         2.14 |         0.01 | fill_(bool[160], bool[])                                    
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |        24.26 |         0.09 | masked_fill_(float32[160, 32000], bool[160, 32000], -inf)   
|- void at::native::_scatter_gather_elementwise_kernel<12... |         0.00 |       119.42 |         0.44 | scatter_(float32[160, 32000], -1, int64[160, 32000], floa...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        40.29 |         0.15 | _softmax(float32[160, 32000], -1, False) <- softmax(float...
|- void at::native::(anonymous namespace)::cunn_SoftMaxFo... |         0.00 |        37.18 |         0.14 | _log_softmax(float32[160, 32000], -1, False) <- log_softm...
|- void at::native::unrolled_elementwise_kernel<at::nativ... |         0.00 |         2.40 |         0.01 | copy_(int64[160], int32[160], False) <- _to_copy(int32[16...
|- void at::native::index_elementwise_kernel<128, 4, at::... |         0.00 |        37.98 |         0.14 | index(float32[160, 32000], None)                            
|- void at::native::(anonymous namespace)::distribution_e... |         0.00 |        12.90 |         0.05 | exponential_(float32[160, 32000], 1.0, None)                
|- void at::native::vectorized_elementwise_kernel<4, at::... |         0.00 |        35.36 |         0.13 | div_(float32[160, 32000], float32[160, 32000])              
|- void at::native::reduce_kernel<512, 1, at::native::Red... |         0.00 |        16.29 |         0.06 | argmax(float32[160, 32000], 1, False)                       
|- Memcpy DtoH (Device -> Pageable)                          |         0.00 |         2.62 |         0.01 | copy_(int64[160, 1], int64[160, 1], False) <- _to_copy(in...

================================================================================
= First Decode Step Summary Table (prompt_len=250, batch_size=160)
================================================================================

name                                                                             | cuda_time_us | pct_cuda_... | invocations    
================================================================================================================================
CUDAGraphRunner                                                                  |     25582.37 |        94.13 |            1.00
|- Memcpy DtoD (Device -> Device)                                                |         6.98 |         0.03 |            5.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<... |         2.46 |         0.01 |            2.00
|- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half, l... |         7.01 |         0.03 |            1.00
|- void vllm::rms_norm_kernel<c10::Half>(c10::Half*, c10::Half const*, c10::H... |         4.96 |         0.02 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x... |      2795.52 |        10.29 |           32.00
|- void vllm::rotary_embedding_kernel<c10::Half, true>(long const*, c10::Half... |       237.57 |         0.87 |           32.00
|- void vllm::reshape_and_cache_kernel<unsigned short, unsigned short, (vllm:... |       989.15 |         3.64 |           32.00
|- void vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 128, ... |     14413.76 |        53.04 |           32.00
|- memcpy128                                                                     |       108.54 |         0.40 |           32.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1... |      1822.72 |         6.71 |           64.00
|- std::enable_if<(((8)>(0)))&&vllm::_typeConvert<c10::Half>::exists, void>::... |       264.19 |         0.97 |           64.00
|- Memset (Unknown)                                                              |        23.55 |         0.09 |           32.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x64_warpgroupsize1x1x1... |      4616.16 |        16.99 |           32.00
|- void vllm::act_and_mul_kernel<c10::Half, &(c10::Half vllm::silu_kernel<c10... |       289.79 |         1.07 |           32.00
LogitsProcessor                                                                  |       214.46 |         0.79 |            1.00
|- void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::Half, l... |         6.91 |         0.03 |            1.00
|- Memset (Device)                                                               |         0.96 |         0.00 |            1.00
|- sm90_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x256x64_warpgroupsize1x1x1... |       206.59 |         0.76 |            1.00
Sampler                                                                          |      1380.12 |         5.08 |            1.00
|- Memcpy HtoD (Pinned -> Device)                                                |        15.36 |         0.06 |            7.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |        17.12 |         0.06 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |        22.40 |         0.08 |            1.00
|- at::native::(anonymous namespace)::fill_reverse_indices_kernel(long*, int,... |        26.11 |         0.10 |            1.00
|- void at_cuda_detail::cub::DeviceSegmentedRadixSortKernel<at_cuda_detail::c... |       511.20 |         1.88 |            4.00
|- void at_cuda_detail::cub::DeviceSegmentedRadixSortKernel<at_cuda_detail::c... |       226.69 |         0.83 |            2.00
|- Memcpy DtoD (Device -> Device)                                                |        21.54 |         0.08 |            1.00
|- void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kerne... |         4.80 |         0.02 |            2.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.66 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |         3.52 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |        24.19 |         0.09 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous n... |        37.25 |         0.14 |            2.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        73.28 |         0.27 |            2.00
|- void at::native::tensor_kernel_scan_innermost_dim<float, std::plus<float> ... |       105.44 |         0.39 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorO... |         1.66 |         0.01 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at... |        24.00 |         0.09 |            1.00
|- void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_no... |         2.14 |         0.01 |            1.00
|- void at::native::_scatter_gather_elementwise_kernel<128, 4, at::native::_c... |       119.42 |         0.44 |            1.00
|- void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, floa... |        37.18 |         0.14 |            1.00
|- void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_ke... |        37.98 |         0.14 |            1.00
|- void at::native::(anonymous namespace)::distribution_elementwise_grid_stri... |        12.90 |         0.05 |            1.00
|- void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFuncto... |        35.36 |         0.13 |            1.00
|- void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::nat... |        16.29 |         0.06 |            1.00
|- Memcpy DtoH (Device -> Pageable)                                              |         2.62 |         0.01 |            1.00
